---
title: "Examining Social Inequities in Traffic-Related Air Pollution Disparities in the Seattle Metropolitan Area Using Predictive Modeling"
author: "Mariana Cortes Espinoza, Cecilia Martindale, Stephanie Mitchell, Jorge Rivera-Gonzalez, Katelin Teigen"
output: 
  html_document:
    df_print: "paged"
    fig_caption: yes
    toc: true
    toc_float: true
    toc_collapsed: true
toc_depth: 3
date: "2024-12-10"
execute:
  echo: true
  warning: false
  cache: false
  message: false
editor_options: 
  chunk_output_type: console
---

# Background

Insert intro here

```{r setup}
# Clear workspace of all objects and unload non-base packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
    suppressWarnings(
        lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
               detach, character.only=TRUE, unload=TRUE, force=TRUE)
    )
}

# Load or install 'pacman' for package management
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {
    install.packages("pacman", repos = my_repo)
}

# **SPH server**: need to install rnaturalearthhires like so on the SPH server
if (!require("rnaturalearthhires")) {
    install.packages("rnaturalearthhires", repos = "https://ropensci.r-universe.dev", type = "source")
}

pacman::p_load(
    tidyverse,                 # Data manipulation and visualization
    # takes a while to install on SPH
    biscale,
    broom,
    ggspatial,                 # Geospatial extensions for ggplot.  
    maptiles, # maptiles and tmap libraries can be used instead of or in combination with ggplot + ggspatial. maptiles offers more tile-based map flexibility; ggspatial provides the ability to annotate maps easily; tmap offers both static and interactive maps that we won't review in this course. 
    terra, # alternative mapping with raster files
    
    # need for SPH server?
    prettymapr,
    
    rnaturalearth,             # Land features for map layers (remove water locations)
    rnaturalearthhires,        # High-resolution land features 
    sf,                        # Handling spatial objects (modern replacement for 'sp')
    knitr,                     # Formatting tables with kable()
    gstat,                     # Geostatistical methods (e.g., kriging)
    Hmisc,                     # Data description functions like describe()
    scales,                    # Color scale customization for ggplot
    akima,                     # Bivariate interpolation for irregular data
    downloader,                 # Downloading files over HTTP/HTTPS
    tigris,
    RColorBrewer,
    scales,
    kableExtra, patchwork
)

```

```{r load.data}

annual_data <- read_csv("annual_data_and_predictions.csv")

census_shapefile <- st_read("cb_2019_53_tract_500k.shp")

grid_covariates <- read_csv("dr0311_grid_covariates.csv")

mobile_covariates <- read_csv("dr0311_mobile_covariates.csv")

stop_data <- read_csv("stop_data.csv")

acs_race <- read_csv("ACS19_race.csv")

acs_ethnicity <- read_csv("ACS19_ethnicity.csv")

acs_language <- read_csv("ACS19_language.csv")

acs_median_income <- read_csv("ACS19_median_income.csv")

acs_poverty <- read_csv("ACS19_poverty.csv")

census_blocks_covar <- read_csv("wa_block_covar_clean.csv")
```

```{r data.cleaning}
# annual data
#glimpse(annual_data)
table(annual_data$variable)

#create variable to differentiate between stop site and collocation site
annual_data <- annual_data %>% 
  mutate(SiteType = ifelse(grepl("MS", location), "stop site",
                       ifelse(grepl("MC", location), "collocation site", NA)))

# keep only NO2 and PM, get rid of annual variable since they are all mean_of_win_medians
annual_clean <- annual_data %>% 
  filter(variable == "no2" | variable == "pm2.5_ug_m3") %>%
  filter(SiteType == "stop site") %>% 
  select(-c(annual, SiteType))


# stop data
#create variable to differentiate between stop site and collocation site
stop_data <- stop_data %>% 
  mutate(SiteType = ifelse(grepl("MS", location), "stop site",
                       ifelse(grepl("MC", location), "collocation site", NA)))

stop_clean <- stop_data %>% 
  filter(variable == "no2" | variable == "neph_bscat") %>%
  filter(SiteType == "stop site") %>% 
  select(c(runname, time, location, stop_id, instrument_id, variable, mean_value, median_value))   

# Get location coordinates
stop_coords <- annual_clean %>% select(c("location", "longitude", "latitude")) %>%
  unique()

stop_clean <- stop_clean %>% 
  filter(median_value > 0) %>% # what are our thoughts on this? Thought: should we report how many this got rid of?
  mutate(log_mean = log(mean_value)) %>% 
  mutate(log_median = log(median_value)) %>% 
  filter(log_median > 0) %>% # and on this? 
  left_join(stop_coords)

# Generate NO2 specific dataset
no2_stop_clean <- stop_clean %>%
  filter(variable == "no2")

# Generate PM2.5 specific dataset
pm25_stop_clean <- stop_clean %>% 
  filter(variable == "neph_bscat")


# census data
#glimpse(census_shapefile)
table(census_shapefile$COUNTYFP) # there are 39 counties, we don't need them all 
# only keep King (033), Snohomish (061), Pierce (053), Kitsap (035)
# get rid of LSAD and STATEFP since they are all the same
census_clean <- census_shapefile %>%
  filter(COUNTYFP %in% c("033", "061", "053", "035")) %>%
  select(-c(LSAD,STATEFP)) %>% 
  st_as_sf()

# grid covar
#glimpse(grid_covariates)
# create 6 digit TRACTCE var that matches census shapefile var
# log-transform distance covariates

grid_covar_clean <- grid_covariates %>%
  mutate(TRACTCE = substr(as.character(tract_key), 
            nchar(as.character(tract_key)) - 5, nchar(as.character(tract_key)))) %>%
  select(c(location_id, native_id, tract_key, m_to_a1, m_to_a2, m_to_a3, pop10_s05000, m_to_coast, m_to_l_airp, m_to_comm,lu_industrial_p03000, longitude, latitude)) %>%
  mutate(log_m_to_a1 = log(m_to_a1 + 1), log_m_to_a2=log(m_to_a2 +1),
         log_m_to_a3=log(m_to_a3+1),
         log_m_to_coast=log(m_to_coast+1), log_m_to_l_airp=log(m_to_l_airp+1), 
         log_m_to_comm=log(m_to_comm+1)) %>% 
  select(-c(m_to_a1,m_to_a2,m_to_a3,m_to_coast,m_to_l_airp,m_to_comm))

# mobile covar
#glimpse(mobile_covariates)
# create 6 digit TRACTCE var that matches census shapefile var
mobile_covar_clean <- mobile_covariates %>%
  mutate(TRACTCE = substr(as.character(tract_key), 
            nchar(as.character(tract_key)) - 5, nchar(as.character(tract_key)))) %>%
  select(c(location_id, native_id, tract_key, TRACTCE, m_to_a1, m_to_a2, m_to_a3, pop10_s05000, m_to_coast, m_to_l_airp, m_to_comm,lu_industrial_p03000)) %>%
  mutate(log_m_to_a1 = log(m_to_a1+1), log_m_to_a2=log(m_to_a2+1), log_m_to_a3=log(m_to_a3+1),
         log_m_to_coast=log(m_to_coast+1), log_m_to_l_airp=log(m_to_l_airp+1), 
         log_m_to_comm=log(m_to_comm+1)) %>% 
  select(-c(m_to_a1,m_to_a2,m_to_a3,m_to_coast,m_to_l_airp,m_to_comm))

# create dataset with stop and mobile data combined
stop_and_mobile <- merge(stop_clean, mobile_covar_clean,
                       by.x = "location", by.y = "native_id", 
                       all = FALSE)

# Join with census_clean
stop_mobile_census <- census_clean %>%
  inner_join(stop_and_mobile, by = "TRACTCE") %>% 
  select(c(TRACTCE, location, time, variable, mean_value, median_value, pop10_s05000, lu_industrial_p03000, log_m_to_a1, log_m_to_a2, log_m_to_a3, log_m_to_coast, log_m_to_l_airp, log_m_to_comm, geometry))


# acs race data
#glimpse(acs_race)
# create 6 digit TRACTCE var that matches census shapefile var
acs_race_clean <- acs_race %>%
  mutate(TRACTCE = substr(as.character(GEO_ID), 
            nchar(as.character(GEO_ID)) - 5, nchar(as.character(GEO_ID))))
#colnames(acs_race_clean)
# acs ethnicity data
#glimpse(acs_ethnicity)
# create 6 digit TRACTCE var that matches census shapefile var
acs_ethnicity_clean <- acs_ethnicity %>%
  mutate(TRACTCE = substr(as.character(GEO_ID), 
            nchar(as.character(GEO_ID)) - 5, nchar(as.character(GEO_ID))))
#colnames(acs_ethnicity_clean)
# acs language data
#glimpse(acs_language)
# create 6 digit TRACTCE var that matches census shapefile var
acs_language_clean <- acs_language %>%
  mutate(TRACTCE = substr(as.character(GEO_ID), 
            nchar(as.character(GEO_ID)) - 5, nchar(as.character(GEO_ID))))
#colnames(acs_language_clean)
# acs median income data
#glimpse(acs_median_income)
# create 6 digit TRACTCE var that matches census shapefile var

```

```{r data.cleaning.census.blocks}
###how I got the unique tracts for cleaning the wa blocks
# unique_tracts <- as.data.frame(unique(grid_covar_clean$tract_key))
# write_csv(unique_tracts, "unique_tracts.csv")

# filter census block centroids so that it only contains centroids for the tracts in the grid

census_blocks_covar_clean <- census_blocks_covar %>%
  mutate(log_m_to_a1 = log(m_to_a1+1), log_m_to_a2=log(m_to_a2+1), log_m_to_a3=log(m_to_a3+1),
         log_m_to_coast=log(m_to_coast+1), log_m_to_l_airp=log(m_to_l_airp+1), 
         log_m_to_comm=log(m_to_comm+1)) %>% 
  select(-c(m_to_a1,m_to_a2,m_to_a3,m_to_coast,m_to_l_airp,m_to_comm)) %>%
  filter(tract_key %in% mobile_covar_clean$tract_key)

```

```{r merge.acs.files}
#--------------Steph & Jorge edits Nov 30
# merge and clean language, race, ethnicity and poverty datasets
# filter out the water census tracts, cleaned up NAs
acs_combined <- reduce(
  list(acs_language_clean, acs_race_clean, acs_ethnicity_clean, acs_poverty),
  function(x, y) merge(x, y, by = c("GEO_ID", "NAME"), all = TRUE)
) %>% filter(Total_Pop_5yrsplus!=0)
colnames(acs_combined)


#NOTE: Variables dropped from this merge: dropped 2ormore column due to complications with code running and to simplify demographics, dropped non-hispanic or latino since it was redundant and we have a "yes" column for hispanic or latino; also removed total_pop_5yearsplus since it is a variable for children over age 5 and did not seem relevant to our questions
acs_simplified <- acs_combined %>%
  select(
    GEO_ID, NAME, TRACTCE, English_only, Language_other_than_English,
    Spanish, Indo_European, Asian_PI, Other, White, Black_AfricanAmerican,
    AIAN, Asian, Hawaiian_PacificIslander, Other_alone, HispanicorLatino,
    Total_Pop, Total_Pop_ME, Percent_Below_Poverty, Percent_Below_Poverty_ME
  ) %>%
  mutate(
    Percent_Below_Poverty = as.numeric(Percent_Below_Poverty),
    Percent_Below_Poverty_ME = as.numeric(Percent_Below_Poverty_ME),
    tract_key = as.numeric(substr(as.character(GEO_ID), 
                                  nchar(as.character(GEO_ID)) - 10, 
                                  nchar(as.character(GEO_ID))))
  ) %>% #fixing incorrect Total_Pop for Pierce County tract 729.06
  mutate(Total_Pop=case_when(NAME=="Census Tract 729.06, Pierce County, Washington" ~ 1995,
                             .default=Total_Pop))

#-----------------------# Data Dictionary for ACS Combined Dataset # 
# GEO_ID : Unique identifier for the geographic area (e.g., census tract, block group) # NAME : Name of the geographic area (e.g., "Census Tract 1, County, State") # TRACTCE : Census tract code, uniquely identifying the geographic unit within a county # # Household Demographics: # White_Households : Number of households with a White population # Black_Households : Number of households with a Black or African American population # AIAN_Households : Number of households with an American Indian or Alaska Native (AIAN) population # Asian_Households : Number of households with an Asian population # Hawaiian_PI_Households : Number of households with a Native Hawaiian or Pacific Islander (Hawaiian_PI) population # Other_alone_Households : Number of households with people from other racial/ethnic groups (alone) # HispanicorLatino_Households : Number of households with a Hispanic or Latino population #

```

```{r merge.stop.and.acs.data}
# create stop data with land use vars and ACS (for descriptive tables)
# merge locations on location var variable (stop_clean) and native_id variable (mobile_covar)
# pre-merge stop_clean dim 17533 x 8, mobile_covar_clean dim 311 x 12
# want to make sure we end up with 17533 rows
# removed extraneous geographic/census vars
stop_w_acs <- left_join(stop_clean, mobile_covar_clean, join_by(location==native_id)) %>%
  left_join(acs_simplified, join_by(tract_key==tract_key)) %>%
  select(-c(GEO_ID))
#dim(stop_w_acs) 
# dimensions are correct (17533 rows)

# created stop data for use in regression for prediction
stop_for_preds <- stop_w_acs %>% select(c(time, location, instrument_id, variable,
                                          log_mean,
                                          mean_value, median_value, pop10_s05000,
                                          lu_industrial_p03000, log_m_to_a1, 
                                          log_m_to_a2, log_m_to_a3, log_m_to_coast,
                                          log_m_to_l_airp, log_m_to_comm, longitude, latitude))

# filter stop_for_preds into two different data sets, one for PM2.5 and one for NO2
pm_for_preds <- stop_for_preds %>% filter(variable == "neph_bscat")

no2_for_preds <- stop_for_preds %>% filter(variable == "no2")

annual_w_acs <- left_join(annual_clean, mobile_covar_clean, join_by(location==native_id)) %>%
  left_join(acs_simplified, join_by(tract_key==tract_key)) %>%
  select(-c(GEO_ID))

# created stop data for use in regression for prediction
annual_for_preds <- annual_w_acs %>% 
  mutate(log_value=log(value)) %>%
  select(c(location, variable, log_value, value, pop10_s05000, lu_industrial_p03000,
           log_m_to_a1, log_m_to_a2, log_m_to_a3, log_m_to_coast, log_m_to_l_airp,
           log_m_to_comm, longitude, latitude))

# create grid data with ACS (for predicting onto and answering Q2/3)
# should end with 5040 rows
 
acs_combined<- acs_combined %>% mutate(tract_key=as.numeric(substr(as.character(GEO_ID), 
            nchar(as.character(GEO_ID)) - 10, nchar(as.character(GEO_ID)))))

grid_w_acs <- left_join(grid_covar_clean, acs_combined, join_by(tract_key==tract_key)) %>%
  select(-c(GEO_ID, TRACTCE))

```

# Methods

## Data Description

## Data Cleaning and Exploration

## Statistical Analysis

### Summary Statistics

### Prediction Models

### Association Between Demographic Variables and Predicted Pollutants

# Results

## Summary Statistics

```{r var.distributions}

#---- Means ----
no2_native <- ggplot(data = no2_stop_clean, aes(mean_value)) +
    geom_histogram(colour = "black", 
                   fill = "white") 

# NO2 plot on the log scale
no2_log <- ggplot(data = no2_stop_clean, aes(log_mean)) +
    geom_histogram(colour = "black", 
                   fill = "white") 

# PM2.5 plot on the native scale
pm25_native <- ggplot(data = pm25_stop_clean, aes(mean_value)) +
    geom_histogram(colour = "black", 
                   fill = "white") 

# PM2.5 plot on the log scale
pm25_log <- ggplot(data = pm25_stop_clean, aes(log_mean)) +
    geom_histogram(colour = "black", 
                   fill = "white") 

# Add plot type and scale information to the datasets
no2_native_data <- no2_stop_clean %>%
  mutate(scale = "Native Scale", pollutant = "NO2", value = mean_value)

no2_log_data <- no2_stop_clean %>%
  mutate(scale = "Log Scale", pollutant = "NO2", value = log_mean)

pm25_native_data <- pm25_stop_clean %>%
  mutate(scale = "Native Scale", pollutant = "PM2.5", value = mean_value)

pm25_log_data <- pm25_stop_clean %>%
  mutate(scale = "Log Scale", pollutant = "PM2.5", value = log_mean)

# Combine all datasets
combined_data <- bind_rows(no2_native_data, no2_log_data, pm25_native_data, pm25_log_data)

# Adjusting the scale levels to reorder the columns
combined_data <- combined_data %>%
  mutate(scale = factor(scale, levels = c("Native Scale", "Log Scale"))) 

# Create faceted plot
faceted_histogram <- ggplot(data = combined_data, aes(value)) +
  geom_histogram(colour = "black", fill = "white", bins = 30) +
  facet_grid(pollutant ~ scale, scales = "free_x") +
  labs(x = "Mean Values", y = "Frequency", title="Figure 1. Distribution of Mean Values",
       caption="Distribution of native and log-transformed mean values of NO2 (above)\n and PM2.5 (below). Both variables are log-normally distributed.") + 
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0.5)) +
  theme(panel.grid.minor = element_blank()) 

# Display the plot
print(faceted_histogram)
```

```{r desc.stats.annual.data}
annual_clean %>% 
  mutate(variable = case_when(
    variable == "pm2.5_ug_m3" ~ "PM2.5 (ug/m3)",
    variable == "no2" ~ "NO2 (ppb)",
    TRUE ~ variable
  )) %>%
  group_by(variable) %>% 
  summarise(
  #  N = n(),
    "Arithmetic Mean" = mean(value),
    "SD" = sd(value),
    "Geometric Mean" = exp(mean(log(value), na.rm = TRUE)),
    "Geometric SD" = exp(sd(log(value), na.rm = TRUE)),
    Max = max(value),
    Min = min(value),
    Median = median(value)
  ) %>%
  rename(Pollutant = variable) %>% 
  kable(caption = "Table 1: Descriptive statistics for Annual NO2 and PM2.5 Measurements Across Study Area Monitoring Locations (N = 304)", digits = 2) %>% 
  kable_styling(full_width = FALSE, latex_options = c("hold_position")) %>% 
  add_footnote("Note: The mean of winsorized medians was used to summarize site annual averages.", # pulled wording from data dictionary 
               notation = "none", 
               threeparttable = TRUE)

```

```{r set.up.sf.data, include=FALSE}

latlong_proj <- 4326  

lambert_proj <- "+proj=lcc +lat_1=33 +lat_2=45 +lat_0=39 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs"

#transform stop data to Lambert for distances 
stop_for_preds <- st_as_sf(stop_for_preds, coords = c('longitude', 'latitude'), crs = 4326) %>%
  # # convert to a meters CRS
  st_transform(lambert_proj)

annual_for_preds <- st_as_sf(annual_for_preds, coords = c('longitude', 'latitude'), crs = 4326) %>%
  # # convert to a meters CRS
  st_transform(lambert_proj)

# Convert grid to sf
grid_covar_clean <- grid_covar_clean %>% 
  st_as_sf(coords = c('longitude', 'latitude'), crs = latlong_proj)

# Download the land polygon data as an sf multipolygon
# the CRS for this is in lat/long degrees
land <- ne_download(scale = "large", type = "land", category = "physical", returnclass = "sf")

# Crop the land area to the bounding box of the LA grid to reduce processing time
# have to convert the la_grid to the same degrees as LA
land <- suppressWarnings(st_crop(land, st_bbox(grid_covar_clean))) 

# Visualize cropped land area
ggplot(land) + geom_sf()

# Filter la_grid to keep only points that intersect with land
grid_covar_clean <- grid_covar_clean[st_within(grid_covar_clean, land) %>% lengths() > 0,]

# Visualize grid land locations (zoom in)
ggplot(grid_covar_clean) + geom_sf(size=0.001)

```

```{r study.area.map.setup}
# ---- Study Area Map Setup ----
# Define a bounding box (min & max X and Y) with a 10,000m buffer around `grid_covar_clean`
map_bbox <- grid_covar_clean %>%
  # convert from degrees to meters
  st_transform(crs = lambert_proj) %>%
  # add a buffer around the area for visualization purposes
  st_buffer(dist = 10000) %>%
  # convert back to original CRS
  st_transform(crs = latlong_proj) %>%
  # take the min/max X/Y
  st_bbox()

#map_bbox

# Base map setup with ggplot2 and ggspatial using OSM tiles
g <- ggplot() +
  ggspatial::annotation_map_tile(type = "osm", zoom = 10, ) +
  theme_minimal()

# Plot background map and the Seattle grid (zoom in)
#g + geom_sf(data = grid_covar_clean, size=0.001)

#---- Annual data map
# Add NO2 data with additional map elements
no2_map_annual <- g + 
  geom_sf(data = annual_for_preds %>% filter(variable == "no2"), aes(color = value)) + 
  scale_color_viridis_c() + 
  theme_void() +  
  theme(
    panel.border = element_rect(color = "black", fill = NA, size = 1),
     plot.title = element_text(hjust = 0.5)
  ) + 
  ggspatial::annotation_scale(location = "bl", width_hint = 0.3, unit_category = "imperial") +
  ggspatial::annotation_north_arrow(location = "tr", which_north = "true", style = north_arrow_fancy_orienteering) +  
  labs(col="NO2 (ppb)", title="NO2"
       )

# Add PM2.5 data with additional map elements, restricting scale to 40 and below
pm25_map_annual <- g + 
  geom_sf(data = annual_for_preds %>% filter(variable == "pm2.5_ug_m3"), aes(color = value)) + 
  scale_color_viridis_c() +
  theme_void() +  
  theme(
    panel.border = element_rect(color = "black", fill = NA, size = 1),
    plot.title = element_text(hjust = 0.5)
  ) + 
  ggspatial::annotation_scale(location = "bl", width_hint = 0.3, unit_category = "imperial") +
  ggspatial::annotation_north_arrow(location = "tr", which_north = "true", style = north_arrow_fancy_orienteering) +  
  labs(col="PM2.5 (ug/m3)", title="PM2.5"
       )

no2_map_annual + pm25_map_annual + plot_annotation(title = 'Figure 2: Map of Study Area Stop Concentrations',
  #subtitle = 'NO2 (left) and PM2.5 (right)',
  caption = 'Both pollutants are presented on the native scale. \n NO2 is in ppb and PM2.5 is in ug/m3. This map captures the annual average concentrations\n stops at 304 monitoring locations.',
  theme = theme(plot.title = element_text(face = "bold", hjust = 0.5),
                plot.subtitle=element_text(hjust=0.5),
                plot.caption=element_text(hjust=0.5)))

```

```{r demographics.table}

# Demographics table for study area
demo_summary <- acs_simplified %>% 
  filter(tract_key %in% unique(stop_w_acs$tract_key)) %>% 
  summarise(
    `Total Pop. (N)` = sum(Total_Pop),
    `Census Tracts`=length(unique(tract_key)),
    White = mean(White / Total_Pop) * 100,
    `Black or African-American` = mean(Black_AfricanAmerican / Total_Pop) * 100,
    `American Indian or Alaska Native` = mean(AIAN / Total_Pop) * 100,
    Asian = mean(Asian / Total_Pop) * 100,
    `Hawaiian or Pacific Islander` = mean(Hawaiian_PacificIslander / Total_Pop) * 100,
    `Hispanic or Latino` = mean(HispanicorLatino / Total_Pop) * 100,
    `Percent Below Poverty` = mean(Percent_Below_Poverty),
    `SD Percent Below Poverty` = sd(Percent_Below_Poverty),
    `English only` = mean(English_only / Total_Pop) * 100,
    `Language other than English` = mean(Language_other_than_English / Total_Pop) * 100,
    Spanish = mean(Spanish / Total_Pop) * 100,
    `Other language` = mean(Other / Total_Pop) * 100
  )

# Generate table
demo_summary %>% 
  kable(
    caption = "Table 2: Demographics All Census Tracts in Study Area. All demographics are presented as percents besides total population and number of census tracts.",
    digits = 2
  ) %>%
  kable_styling()

```

## Prediction model results

```{r}
# Convert to a meter CRS for distance calculations
stop_for_preds <- st_transform(stop_for_preds, crs = lambert_proj) 

annual_for_preds <- st_transform(annual_for_preds, crs = lambert_proj) 
```

```{r no2.variograms}
# ---- Empirical Variogram Plots ----

# Variogram Cloud with stop_for_preds --> 24956646 observations
#vgm_no2_stop <- variogram(log_mean ~ pop10_s05000 + lu_industrial_p03000 + log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm, stop_for_preds %>% filter(variable == "no2"), cloud = TRUE)

# Variogram Cloud with annual_for_preds --> 32135
vgm_no2_annual <- variogram(log_value ~ pop10_s05000 + lu_industrial_p03000 + log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm, stop_for_preds, annual_for_preds %>% filter(variable == "no2"), cloud = TRUE)

# deleted code for the stop data because it runs forever

# pretty - moved the actual plot to the Appendix!
# ggplot(vgm_no2_annual, aes(x = dist, y=gamma)) + 
#   geom_point(alpha=0.1) + 
#   labs(x="Distance (meters)",
#        title="Variogram Cloud Using Annual Mean Concentration: NO2")


# Binned Variogram with annual data --> prettier! -- moved actual plot to appendix -cm
#plot(variogram(log_value ~ pop10_s05000 + lu_industrial_p03000 + log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm, annual_for_preds %>% filter(variable == "no2")), main="Binned Variogram with Annual Concentration Data: NO2")

# Binned Variogram with Point Counts
#plot(variogram(log_value ~ pop10_s05000 + lu_industrial_p03000 + log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm, annual_for_preds %>% filter(variable == "no2")), pl = TRUE, main="Binned Variogram with Annual Concentration Data with Point Counts: NO2")

# moved smoothed cloud variogram to Appendix
# Smoothed Cloud Variogram using ggplot2

```

```{r est.no2.var}
# Estimate the empirical variogram
##  By default, the variogram() function limits the maximum lag distance. Increasing the cutoff parameter will allow it to calculate semivariance values over a larger distance, which might help the semivariogram level off if it's naturally reaching a sill

v_annual <- variogram(log_value ~ pop10_s05000 + lu_industrial_p03000 + log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm, data=annual_for_preds %>% filter(variable == "no2"))

# Fit a model to the variogram, trying exponential, spherical, and Matern options
v_annual.fit <- fit.variogram(v_annual, vgm(c("Exp", "Sph", "Mat")))

# Display the selected variogram model and its parameters (sill, range, nugget)
# Note: what model is selected? Spherical for both
v_annual.fit

# Plot the empirical variogram with the fitted model overlaid
# consider expanding the x-axis here - moved to Appendix
#plot(v_annual, v_annual.fit, main="Empirical Variogram with Fitted Model Overlaid: NO2")

```

```{r fitting variogram for universal kriging no2}
# Estimate the variogram with a covariate predictor
# 100000 is way too high a cutoff

v_annual.uk <- variogram(log_value ~ log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm + pop10_s05000 + lu_industrial_p03000, data=annual_for_preds %>% filter(variable=="no2"))
plot(v_annual.uk)


# Fit the variogram model with multiple options (Exponential, Spherical, Matern)
# note that this may have convergence issues. If so, you can try selecting one variogram model instead. You can also give it initial values for range, nugget etc. based on looking at the variogram cloud.

# sometimes has convergence issues 
m_annual.uk <- fit.variogram(v_annual.uk, vgm(c("Exp", "Sph", "Mat")))

# Alternatively, you could fit the variogram with modified initial values. This still has convergence issues.
# m.uk <- fit.variogram(v.uk, vgm("Exp", nugget = 0.01, psill = 0.03, range = 15000))


# Display the selected variogram model parameters
m_annual.uk

```

```{r universal.kriging.prediction.no2.grid.annual, warning=FALSE}
#transform stop data to Lambert for distances 
grid_for_preds <- st_as_sf(grid_covar_clean, coords = c('longitude', 'latitude'), crs = 4326) %>%
  # # convert to a meters CRS
  st_transform(lambert_proj)

# ---- Universal Kriging Prediction Code from Lab 6 ----

# Fit the universal kriging model for NO2 annual data and predict on the grid 
ln.no2.kr <- krige(log_value ~ log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm + pop10_s05000 + lu_industrial_p03000, 
                 annual_for_preds %>% filter(variable=="no2"), # CRS in meters
                 newdata = grid_for_preds, # CRS in meters
                 model = m_annual.uk)

# Calculate standard errors to assess prediction uncertainty across locations
ln.no2.kr$se <- sqrt(ln.no2.kr$var1.var)  # Standard error is the square root of variance

#plotting these in Appendix
```

```{r universal.kriging.prediction.no2.BLOCK, warning=FALSE}
#transform stop data to Lambert for distances 
blocks_for_preds <- st_as_sf(census_blocks_covar_clean, coords = c('longitude', 'latitude'), crs = 4326) %>%
  # # convert to a meters CRS
  st_transform(lambert_proj)

# ---- Universal Kriging Prediction Code from Lab 6 ----

# Fit the universal kriging model for PM2.5 and predict on the grid
ln.NO2.block.kr <- krige(log_value ~ log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm + pop10_s05000 + lu_industrial_p03000, 
                 annual_for_preds %>% filter(variable=="no2"), # CRS in meters
                 newdata = blocks_for_preds, # CRS in meters
                 model = m_annual.uk)

# Calculate standard errors to assess prediction uncertainty across locations
ln.NO2.block.kr$se <- sqrt(ln.NO2.block.kr$var1.var)  # Standard error is the square root of variance

```

```{r generate.pm25.variogram}
#Generate variograms for pm25

# Estimate the variogram with a covariate predictor
v.uk.pm25 <- variogram(log_value ~ log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm + pop10_s05000 + lu_industrial_p03000, data=annual_for_preds %>% filter(variable=="pm2.5_ug_m3"))
                  #,cutoff=5000)

# Fit the variogram model with multiple options (Exponential, Spherical, Matern)
# note that this may have convergence issues. If so, you can try selecting one variogram model instead. You can also give it initial values for range, nugget etc. based on looking at the variogram cloud.

m.uk.pm25 <- fit.variogram(v.uk.pm25, vgm(c("Exp", "Sph", "Mat")))

# Display the selected variogram model parameters
m.uk.pm25

# Plot the empirical variogram with the fitted model
#plot(v.uk.pm25, model = m.uk.pm25)
```

```{r universal.kriging.prediction.pm25.grid, warning=FALSE}
# ---- Universal Kriging Prediction Code from Lab 6 ----

# Fit the universal kriging model for PM2.5 and predict on the grid
ln.PM25.kr.grid <- krige(log_value ~ log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm + pop10_s05000 + lu_industrial_p03000, 
                 annual_for_preds %>% filter(variable=="pm2.5_ug_m3"), # CRS in meters
                 newdata = grid_for_preds, # CRS in meters
                 model = m.uk.pm25)

# Calculate standard errors to assess prediction uncertainty across locations
ln.PM25.kr.grid$se <- sqrt(ln.PM25.kr.grid$var1.var)  # Standard error is the square root of variance

```

```{r universal.kriging.prediction.pm25.BLOCK, warning=FALSE}
# ---- Universal Kriging Prediction Code from Lab 6 ----

# Fit the universal kriging model for PM2.5 and predict on the grid
ln.PM25.kr <- krige(log_value ~ log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm + pop10_s05000 + lu_industrial_p03000, 
                 annual_for_preds %>% filter(variable=="pm2.5_ug_m3"), # CRS in meters
                 newdata = blocks_for_preds, # CRS in meters
                 model = m.uk.pm25)

# Calculate standard errors to assess prediction uncertainty across locations
ln.PM25.kr$se <- sqrt(ln.PM25.kr$var1.var)  # Standard error is the square root of variance

# moved plots to Appendix
# Plot UK predictions
#pl3 <- plot(ln.PM25.kr["var1.pred"], main = "UK Prediction of Log(PM2.5)")

# Plot UK prediction standard errors
#pl4 <- plot(ln.PM25.kr["se"], main = "UK Prediction Error PM2.5 (Standard Error)")

```

```{r define CV functions}
# ---- Define Cross-Validation Functions ----

# Wrapper function krige.cv2() to retain the projection of the sf object.
# This fixes a known bug in krige.cv() where projection information is lost.
# (Bug reported and fixed on GitHub, but this wrapper may be required for now.)
krige.cv2 <- function(formula, locations, model = NULL, ..., beta = NULL, 
                      nmax = Inf, nmin = 0, maxdist = Inf, 
                      nfold = nrow(locations),  # default is leave-one-out
                      verbose = interactive(), 
                      debug.level = 0) {
  
  # Perform cross-validation and retain projection if it's missing
  krige.cv1 <- krige.cv(formula = formula, locations = locations, model = model, ..., 
                        beta = beta, nmax = nmax, nmin = nmin, maxdist = maxdist, 
                        nfold = nfold, verbose = verbose, debug.level = debug.level)
  
  # Set projection from input data if krige.cv output lacks it
  if (is.na(st_crs(krige.cv1))) {
    st_crs(krige.cv1) <- st_crs(locations)
  }
  return(krige.cv1)
}

# Function to create a bubble plot for kriging residuals
krige.cv.bubble <- function(cv.out, plot_title) {
  ggplot(data = cv.out) +
    geom_sf(aes(size = abs(residual), color = factor(residual > 0)), alpha = 0.5) +
    scale_color_discrete(name = 'Residual > 0', direction = -1) +
    scale_size_continuous(name = '|Residual|') +
    ggtitle(plot_title) +
    theme_bw()
}

# Function to calculate performance metrics: RMSE and R²
krige.cv.stats <- function(krige.cv.output, description) {
  d <- krige.cv.output
  
  # Calculate Mean Squared Error (MSE) and R²
  mean_observed <- mean(d$observed)
  MSE_pred <- mean((d$observed - d$var1.pred)^2)
  MSE_obs <- mean((d$observed - mean_observed)^2)
  
  # Create a summary table with rounded RMSE and MSE-based R²
  tibble(
    Description = description, 
    RMSE = round(sqrt(MSE_pred), 4), 
    MSE_based_R2 = round(max(1 - MSE_pred / MSE_obs, 0), 4)
  )
}

```

```{r cross-validation-no2, warning=FALSE}
# ---- Cross-Validation for log(no2)----

# Perform Universal Kriging (UK) with 5-fold Cross-Validation
cv5uk_no2 <- krige.cv2(log_value ~ log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm + pop10_s05000 + lu_industrial_p03000, 
                 annual_for_preds %>% filter(variable=="no2"), 
                 model = m_annual.uk,
                 nfold = 5)

# Perform Universal Kriging (UK) with 10-fold Cross-Validation
cv10uk_no2 <- krige.cv2(log_value ~ log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm + pop10_s05000 + lu_industrial_p03000, 
                 annual_for_preds %>% filter(variable=="no2"), 
                 model = m_annual.uk,
                 nfold = 10)

# Calculate and compare performance statistics across cross-validation methods
# Compile results into a summary table
bind_rows(
  krige.cv.stats(cv5uk_no2, "UK: 5-Fold CV NO2"),
  krige.cv.stats(cv10uk_no2, "UK: 10-Fold CV NO2")
) %>% 
  kable(caption = "Summary of Kriging Cross-Validation Results for log(NO2)") %>% kable_styling()

```

```{r cross-validation-pm25, warning=FALSE}
# ---- Cross-Validation for log(PM2.5)----

# Perform Universal Kriging (UK) with 5-fold Cross-Validation
cv5uk_pm25 <- krige.cv2(log_value ~ log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm + pop10_s05000 + lu_industrial_p03000, 
                 annual_for_preds %>% filter(variable=="pm2.5_ug_m3"), 
                 model = m.uk.pm25,
                 nfold = 5)

# Perform Universal Kriging (UK) with 10-fold Cross-Validation (LOOCV)
cv10uk_pm25 <- krige.cv2(log_value ~ log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm + pop10_s05000 + lu_industrial_p03000, 
                 annual_for_preds %>% filter(variable=="pm2.5_ug_m3"), 
                 model = m.uk.pm25,
                 nfold = 10)

# Calculate and compare performance statistics across cross-validation methods
# Compile results into a summary table
bind_rows(
  krige.cv.stats(cv5uk_pm25, "UK: 5-Fold CV PM2.5"),
  krige.cv.stats(cv10uk_pm25, "UK: 10-Fold CV PM2.5")
) %>% 
  kable(caption = "Summary of Kriging Cross-Validation Results for log(PM2.5)") %>% kable_styling()

```

```{r map.native.scale}

# exponentiate log vars
ln.no2.kr$var1.pred.native <- exp(ln.no2.kr$var1.pred)
ln.PM25.kr$var1.pred.native <- exp(ln.PM25.kr$var1.pred)

# re-project back to long-lat proj
blocks_for_preds <- st_transform(blocks_for_preds, latlong_proj)
ln.no2.kr <- st_transform(ln.no2.kr, latlong_proj)
ln.PM25.kr <- st_transform(ln.PM25.kr, latlong_proj)

# check that coordinates are nearly identical
all.equal(st_coordinates(blocks_for_preds), st_coordinates(ln.no2.kr))
all.equal(st_coordinates(blocks_for_preds), st_coordinates(ln.PM25.kr))

# Join grid to predictions; do so by nearest feature to avoid precision merging issues
new_blocks_no2 <- st_join(blocks_for_preds, ln.no2.kr, join = st_nearest_feature) %>% 
  rename(`NO2 (ppb)` = var1.pred.native, 
         `ln(NO2) ln(ppb)`=var1.pred,
         `ln(NO2) var`=var1.var,
         `ln(NO2) se`=se)
new_blocks_pm25 <- st_join(blocks_for_preds, ln.PM25.kr, join = st_nearest_feature) %>% 
  rename(`PM2.5 (ug/m^3)` = var1.pred.native, 
         `ln(PM2.5) ln(ug/m^3)`=var1.pred,
         `ln(PM2.5) var`=var1.var,
         `ln(PM2.5) se`=se)

# Verify the join was successful (no NAs in predictions)
all(!is.na(new_blocks_pm25$`ln(PM2.5) ln(ug/m^3)`)) 
all(!is.na(new_blocks_no2$`ln(NO2) ln(ppb)`)) 
#yay!
```

```{r plot.points}
#NO2 - native scale
ggplot() + ggspatial::annotation_map_tile(type = "osm", zoom = 10, ) + 
  theme_minimal() +
  geom_sf(data = new_blocks_no2, aes(color = `NO2 (ppb)`), alpha = 0.05) +
  # color friendly color scale
  scale_color_viridis_c(option = "plasma") + 
  ggtitle("Map of Seattle Area with UK NO2\n Predictions Overlaid as Points") +
  ggspatial::annotation_scale(location = "bl", width_hint = 0.3, unit_category = "imperial") +  # Scale in miles
  ggspatial::annotation_north_arrow(location = "tr", which_north = "true", style = north_arrow_fancy_orienteering) +


#PM2.5 - native scale
ggplot() + ggspatial::annotation_map_tile(type = "osm", zoom = 10, ) + 
  theme_minimal() +
  geom_sf(data = new_blocks_pm25, aes(color = `PM2.5 (ug/m^3)`), alpha = 0.05) +
  # color friendly color scale
  scale_color_viridis_c(option = "plasma") + 
  ggtitle("Map of Seattle Area with UK PM2.5\n Predictions Overlaid as Points") + ggspatial::annotation_scale(location = "bl", width_hint = 0.3, unit_category = "imperial") +  # Scale in miles
  ggspatial::annotation_north_arrow(location = "tr", which_north = "true", style = north_arrow_fancy_orienteering)
```

```{r merge.newblocks.acs}
# had to average the predictions over each tract based on the predicted blocks
predicted_to_tracts_no2 <- new_blocks_no2 %>% group_by(tract_key) %>%
  summarise("ln(NO2) ln(ppb)"=mean(`ln(NO2) ln(ppb)`),
            "ln(NO2) var" = mean(`ln(NO2) var`), "ln(NO2) se" = mean(`ln(NO2) se`),
            "NO2 (ppb)" = mean(`NO2 (ppb)`))

predicted_to_tracts_pm25 <- new_blocks_pm25 %>% group_by(tract_key) %>%
  summarise("ln(PM2.5) ln(ug/m^3)"=mean(`ln(PM2.5) ln(ug/m^3)`),
            "ln(PM2.5) var" = mean(`ln(PM2.5) var`), "ln(PM2.5) se" = mean(`ln(PM2.5) se`),
            "PM2.5 (ug/m^3)" = mean(`PM2.5 (ug/m^3)`))

full_data_w_preds <- acs_simplified %>% left_join(predicted_to_tracts_no2, join_by(tract_key==tract_key)) %>% left_join(predicted_to_tracts_pm25, join_by(tract_key==tract_key)) %>% select(-c(geometry.x, geometry.y)) %>% 
  filter(!is.na(`ln(NO2) ln(ppb)`) | !is.na(`ln(PM2.5) ln(ug/m^3)`))
```

```{r desc.stats.pred.data}

full_data_w_preds %>% 
  pivot_longer(cols=contains(c("NO2", "PM2.5")),
               names_to = "variable") %>%
  filter(variable == "NO2 (ppb)" | variable == "PM2.5 (ug/m^3)") %>%
  group_by(variable) %>% 
  summarise(
    #"Census tracts" = n(),
    "Arithmetic Mean" = mean(value),
    "SD" = sd(value),
    "Geometric Mean" = exp(mean(log(value), na.rm = TRUE)),
    "Geometric SD" = exp(sd(log(value), na.rm = TRUE)),
    Max = max(value),
    Min = min(value),
    Median = median(value)
  ) %>%
  kable(caption = "Table X: Descriptive statistics for predicted NO2 and PM2.5 annual means in study area (n=452 census tracts).", digits = 2) %>% 
  kable_styling(full_width = FALSE, latex_options = c("hold_position")) %>% 
  add_footnote("Note: The mean of winsorized medians was used to summarize site annual averages.", # pulled wording from data dictionary 
               notation = "none", 
               threeparttable = TRUE)

```

## Important note about the full data

**IMPORTANT**: FULL DATA containing the predicted NO2, predicted PM2.5, and ACS variables is located in the dataframe called "full_data_w_preds". This is the dataframe we should use to do our association tests. If there are variables in other acs sets you can sub that in in the line of code above as long as they still have "tract_key" -- otherwise you'll need to do a little more.

## Associations Between Pollutant Concentrations & Demographics

```{r q3.univariate.reg.assoc.pm25}
# Regression between annual PM2.5 concentrations and various demographics

# Created new columns for percent of population for each race (same as those included in demographic statistics summary table (except for "Other Langugage"))

full_data_w_preds_perc <- full_data_w_preds %>% 
    mutate(
           p_White = (White / Total_Pop)*100,
           p_BlackAA = (Black_AfricanAmerican/ Total_Pop)*100,
           p_AIAN = (AIAN / Total_Pop)*100,
           p_Asian = (Asian / Total_Pop)*100,
           p_HPI = (Hawaiian_PacificIslander / Total_Pop)*100,
           p_Hispanic = (HispanicorLatino / Total_Pop)*100,
           p_engl_only = (English_only / Total_Pop)*100,
           p_lang_other = (Language_other_than_English / Total_Pop)*100,
           p_spanish = (Spanish / Total_Pop)*100
           )


## I think all of these need to be edited (the predictor and response variables are switched) -kt
## Edited - cm
#PM2.5 x poverty
pm_poverty <- lm(`ln(PM2.5) ln(ug/m^3)` ~ Percent_Below_Poverty, 
                 data = full_data_w_preds)
pm_poverty_res <- tidy(pm_poverty, conf.int=TRUE, exponentiate=TRUE) %>%
  mutate(intercept=case_when(term=="Percent_Below_Poverty" ~ 4.21,
                             .default=NA))
           
# PM2.5 x White
pm_white <- lm(`ln(PM2.5) ln(ug/m^3)` ~ p_White, data = full_data_w_preds_perc)
pm_white_res <- tidy(pm_white, conf.int=TRUE, exponentiate=TRUE) %>%
  mutate(intercept=case_when(term=="p_White" ~ 4.40,
                             .default=NA))

# PM2.5 x Black African American
pm_BlackAA <- lm(`ln(PM2.5) ln(ug/m^3)` ~ p_BlackAA, data = full_data_w_preds_perc)
pm_BlackAA_res <- tidy(pm_BlackAA, conf.int=TRUE, exponentiate=TRUE) %>%
  mutate(intercept=case_when(term=="p_BlackAA" ~ 4.31,
                             .default=NA))

# PM2.5 x AIAN
pm_AIAN <- lm(`ln(PM2.5) ln(ug/m^3)` ~ p_AIAN, data = full_data_w_preds_perc)
pm_AIAN_res <- tidy(pm_AIAN, conf.int=TRUE, exponentiate=TRUE) %>%
  mutate(intercept=case_when(term=="p_AIAN" ~ 4.40,
                             .default=NA))

# PM2.5 x Asian
pm_Asian <- lm(`ln(PM2.5) ln(ug/m^3)` ~ p_Asian, data = full_data_w_preds_perc)
pm_Asian_res <- tidy(pm_Asian, conf.int=TRUE, exponentiate=TRUE) %>%
  mutate(intercept=case_when(term=="p_Asian" ~ 4.39,
                             .default=NA))

# PM2.5 x HPI
pm_HPI <- lm(`ln(PM2.5) ln(ug/m^3)` ~ p_HPI, data = full_data_w_preds_perc)
pm_HPI_res <- tidy(pm_HPI, conf.int=TRUE, exponentiate=TRUE) %>%
  mutate(intercept=case_when(term=="p_HPI" ~ 4.38,
                             .default=NA))

# PM2.5 x Hispanic
pm_Hispanic <- lm(`ln(PM2.5) ln(ug/m^3)` ~ p_Hispanic, data = full_data_w_preds_perc)
pm_Hispanic_res <- tidy(pm_Hispanic, conf.int=TRUE, exponentiate=TRUE) %>%
  mutate(intercept=case_when(term=="p_Hispanic" ~ 4.36,
                             .default=NA))

# PM2.5 x language
pm_engl_only <- lm(`ln(PM2.5) ln(ug/m^3)` ~ p_engl_only, data = full_data_w_preds_perc)
pm_engl_only_res <- tidy(pm_engl_only, conf.int=TRUE, exponentiate=TRUE) %>%
  mutate(intercept=case_when(term=="p_engl_only" ~ 4.38,
                             .default=NA))

pm_lang_other <- lm(`ln(PM2.5) ln(ug/m^3)` ~ p_lang_other, data = full_data_w_preds_perc)
pm_lang_other_res <- tidy(pm_lang_other, conf.int=TRUE, exponentiate=TRUE) %>%
  mutate(intercept=case_when(term=="p_lang_other" ~ 4.38,
                             .default=NA))

pm_spanish <- lm(`ln(PM2.5) ln(ug/m^3)` ~ p_spanish, data = full_data_w_preds_perc)
pm_spanish_res <- tidy(pm_spanish, conf.int=TRUE, exponentiate=TRUE) %>%
  mutate(intercept=case_when(term=="p_spanish" ~ 4.36,
                             .default=NA))
  
pm_univar_res <- rbind(pm_poverty_res,pm_white_res,pm_BlackAA_res,pm_AIAN_res,
                       pm_Asian_res, pm_HPI_res, pm_Hispanic_res, pm_engl_only_res,
                       pm_lang_other_res,pm_spanish_res) %>% 
  filter(term!="(Intercept)") %>%
  relocate(intercept, .before=estimate)

```

```{r q3.multivariate.reg.assoc.pm25}
# Multiple linear regression model for PM2.5
pm_full <- lm(`ln(PM2.5) ln(ug/m^3)` ~ Percent_Below_Poverty + p_engl_only + p_lang_other + p_spanish + p_White + p_BlackAA + p_AIAN + p_Asian + p_HPI + p_Hispanic, data = full_data_w_preds_perc)

# Extract regression results for PM2.5 model
pm_full_results <- tidy(pm_full, conf.int=TRUE, exponentiate=TRUE)

# Formatted table for regression results
pm_full_results %>%
  select(term, estimate, std.error, statistic, p.value, conf.low, conf.high) %>% 
  mutate(
    Term = case_when(
      term == "(Intercept)" ~ "Intercept",
      term == "Percent_Below_Poverty" ~ "% Below Poverty",
      term == "p_engl_only" ~ "% English Language Only",
      term == "p_lang_other" ~ "% Other Language",
      term == "p_spanish" ~ "% Spanish Language",
      term == "p_White" ~ "% White",
      term == "p_BlackAA" ~ "% Black or African American",
      term == "p_AIAN" ~ "% American Indian or Alaska Native",
      term == "p_Asian" ~ "% Asian",
      term == "p_HPI" ~ "% Hawaiian or Pacific Islander",
      term == "p_Hispanic" ~ "% Hispanic or Latino",
      TRUE ~ term 
    )
  ) %>%
  select(Term, estimate, std.error, statistic, p.value, conf.low, conf.high) %>%
  rename(
    Estimate = estimate,
    `Std. Error` = std.error,
    `t-value` = statistic,
    `P-value` = p.value,
    `95%CI Low` = conf.low,
    `95%CI High` = conf.high
  ) %>% 
  kable(
    caption = "Table: Regression for Association Results for PM2.5 Model. Response variable was log(PM2.5), and all results have been exponentiated.",
    digits = 3, 
    col.names = c("Term", "Estimate", "Std. Error", "t-value", "P-value", "95% CI Low", "95% CI High")
  ) %>%
  kable_styling(
    full_width = FALSE, 
    bootstrap_options = c("striped", "hover", "condensed")
  )

```

```{r q3.univariate.reg.assoc.no2}
# Regression between annual PM2.5 concentrations and various demographics

# Created new columns for percent of population for each race (same as those included in demographic statistics summary table (except for "Other Langugage"))

## I think all of these need to be edited (the predictor and response variables are switched) -kt
## Edited - cm
#NO2 x poverty
no2_poverty <- lm(`ln(NO2) ln(ppb)` ~ Percent_Below_Poverty, 
                 data = full_data_w_preds)
no2_poverty_res <- tidy(no2_poverty, conf.int=TRUE, exponentiate=TRUE) %>%
  mutate(intercept=case_when(term=="Percent_Below_Poverty" ~ 6.86,
                             .default=NA))
           
# PM2.5 x White
no2_white <- lm(`ln(NO2) ln(ppb)` ~ p_White, data = full_data_w_preds_perc)
no2_white_res <- tidy(no2_white, conf.int=TRUE, exponentiate=TRUE) %>%
  mutate(intercept=case_when(term=="p_White" ~ 7.95,
                             .default=NA))

# PM2.5 x Black African American
no2_BlackAA <- lm(`ln(NO2) ln(ppb)` ~ p_BlackAA, data = full_data_w_preds_perc)
no2_BlackAA_res <- tidy(no2_BlackAA, conf.int=TRUE, exponentiate=TRUE) %>%
  mutate(intercept=case_when(term=="p_BlackAA" ~ 7.40,
                             .default=NA))

# PM2.5 x AIAN
no2_AIAN <- lm(`ln(NO2) ln(ppb)` ~ p_AIAN, data = full_data_w_preds_perc)
no2_AIAN_res <- tidy(no2_AIAN, conf.int=TRUE, exponentiate=TRUE) %>%
  mutate(intercept=case_when(term=="p_AIAN" ~ 7.81,
                             .default=NA))

# PM2.5 x Asian
no2_Asian <- lm(`ln(NO2) ln(ppb)` ~ p_Asian, data = full_data_w_preds_perc)
no2_Asian_res <- tidy(no2_Asian, conf.int=TRUE, exponentiate=TRUE) %>%
  mutate(intercept=case_when(term=="p_Asian" ~ 7.77,
                             .default=NA))

# PM2.5 x HPI
no2_HPI <- lm(`ln(NO2) ln(ppb)` ~ p_HPI, data = full_data_w_preds_perc)
no2_HPI_res <- tidy(no2_HPI, conf.int=TRUE, exponentiate=TRUE) %>%
  mutate(intercept=case_when(term=="p_HPI" ~ 7.74,
                             .default=NA))

# PM2.5 x Hispanic
no2_Hispanic <- lm(`ln(NO2) ln(ppb)` ~ p_Hispanic, data = full_data_w_preds_perc)
no2_Hispanic_res <- tidy(no2_Hispanic, conf.int=TRUE, exponentiate=TRUE) %>%
  mutate(intercept=case_when(term=="p_Hispanic" ~ 7.62,
                             .default=NA))

# PM2.5 x language
no2_engl_only <- lm(`ln(NO2) ln(ppb)` ~ p_engl_only, data = full_data_w_preds_perc)
no2_engl_only_res <- tidy(no2_engl_only, conf.int=TRUE, exponentiate=TRUE) %>%
  mutate(intercept=case_when(term=="p_engl_only" ~ 7.79,
                             .default=NA))

no2_lang_other <- lm(`ln(NO2) ln(ppb)` ~ p_lang_other, data = full_data_w_preds_perc)
no2_lang_other_res <- tidy(no2_lang_other, conf.int=TRUE, exponentiate=TRUE) %>%
  mutate(intercept=case_when(term=="p_lang_other" ~ 7.74,
                             .default=NA))

no2_spanish <- lm(`ln(NO2) ln(ppb)` ~ p_spanish, data = full_data_w_preds_perc)
no2_spanish_res <- tidy(no2_spanish, conf.int=TRUE, exponentiate=TRUE) %>%
  mutate(intercept=case_when(term=="p_spanish" ~ 7.63,
                             .default=NA))
  
no2_univar_res <- rbind(no2_poverty_res, no2_white_res, no2_BlackAA_res, no2_AIAN_res,
                       no2_Asian_res, no2_HPI_res, no2_Hispanic_res, no2_engl_only_res,
                       no2_lang_other_res, no2_spanish_res) %>% 
  filter(term!="(Intercept)") %>%
  relocate(intercept, .before=estimate)
```

```{r q3.multivariate.reg.assoc.no2}
# Multiple linear regression model for NO2
no2_full <- lm(`ln(NO2) ln(ppb)` ~ Percent_Below_Poverty + p_engl_only + p_lang_other + p_spanish + p_White + p_BlackAA + p_AIAN + p_Asian + p_HPI + p_Hispanic, data = full_data_w_preds_perc)

# Extract regression results for NO2 model
no2_full_results <- tidy(no2_full, conf.int=TRUE, exponentiate=TRUE)

# Formatted table for regression results
no2_full_results %>%
  select(term, estimate, std.error, statistic, p.value, conf.low, conf.high) %>% 
  mutate(
    Term = case_when(
      term == "(Intercept)" ~ "Intercept",
      term == "Percent_Below_Poverty" ~ "% Below Poverty",
      term == "p_engl_only" ~ "% English Language Only",
      term == "p_lang_other" ~ "% Other Language",
      term == "p_spanish" ~ "% Spanish Language",
      term == "p_White" ~ "% White",
      term == "p_BlackAA" ~ "% Black or African American",
      term == "p_AIAN" ~ "% American Indian or Alaska Native",
      term == "p_Asian" ~ "% Asian",
      term == "p_HPI" ~ "% Hawaiian or Pacific Islander",
      term == "p_Hispanic" ~ "% Hispanic or Latino",
      TRUE ~ term 
    )
  ) %>%
  select(Term, estimate, std.error, statistic, p.value, conf.low, conf.high) %>%
  rename(
    Estimate = estimate,
    `Std. Error` = std.error,
    `t-value` = statistic,
    `P-value` = p.value,
    `95%CI Low` = conf.low,
    `95%CI High` = conf.high
  ) %>% 
  kable(
    caption = "Table: Regression for Association Results for NO2 Model. Response variable was log(NO2), and all results have been exponentiated.",
    digits = 3, 
    col.names = c("Term", "Estimate", "Std. Error", "t-value", "P-value", "95% CI Low", "95% CI High")
  ) %>%
  kable_styling(
    full_width = FALSE, 
    bootstrap_options = c("striped", "hover", "condensed")
  )
```

## Choropleth visualization of predicted pollutants with demographic variables

```{r pm25.choropleth.map}
## The below code to merge the full dataset with the census geometries is kinda trash. Would love your expertise in cleaning it up Cecilia :-D

# Ensure `census_clean` is an sf object
census_clean <- st_as_sf(census_clean)

# Select only the `AFFGEOID` and `geometry` columns
census_geometry <- census_clean %>%
  select(AFFGEOID, geometry) %>% 
  rename(GEO_ID = AFFGEOID)

# Drop geometry temporarily
census_geometry_nonspatial <- st_drop_geometry(census_geometry)

# Perform join
full_data_w_preds_perc_spatial <- full_data_w_preds_perc %>%
  left_join(census_geometry_nonspatial, by = "GEO_ID") 

# Reattach geometry to the joined data
full_data_w_preds_perc_spatial <- st_as_sf(
  left_join(census_clean, full_data_w_preds_perc_spatial, by = c("AFFGEOID" = "GEO_ID"))
) %>% 
  filter(!is.na(NAME.y))


# generating plots for significant predictors from regression results

# PM2.5 and % Below Poverty Map

# Classify data for bivariate mapping
pm_poverty_bivariate <- bi_class(
  full_data_w_preds_perc_spatial,
  x = `PM2.5 (ug/m^3)`, 
  y = Percent_Below_Poverty, 
  style = "quantile", 
  dim = 3 
)

# Define the bounding box for the study area
map_bbox <- st_bbox(pm_poverty_bivariate)

# Generate the bivariate map
pm_poverty_map <- ggplot() +
  ggspatial::annotation_map_tile(type = "osm", zoom = 10) +  
  geom_sf(data = pm_poverty_bivariate, aes(fill = bi_class), color = NA) +
  bi_scale_fill(pal = "PurpleGrn", dim = 3, name = NULL) +
  labs(
    title = "% Below Poverty"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(size = 10),
    legend.position = "none"
  )

# Create legend as a matrix of colors
# legend <- bi_legend(
#   pal = "PurpleGrn",
#   dim = 3,
#   xlab = "Higher PM2.5",
#   ylab = "Higher Poverty",
#   size = 12
# )

# Combine the map and the legend
# pm_poverty_map <- pm_poverty_map + inset_element(
#   legend,
#   left = 1.05,
#   bottom = 0.1,
#   right = 1.35,
#   top = 0.4     
# )


# PM2.5 and % English Language Only Map

# Classify data for bivariate mapping
pm_english_bivariate <- bi_class(
  full_data_w_preds_perc_spatial,
  x = `PM2.5 (ug/m^3)`, 
  y = 'p_engl_only', 
  style = "quantile", 
  dim = 3 
)

# Generate the bivariate map
pm_english_map <- ggplot() +
  ggspatial::annotation_map_tile(type = "osm", zoom = 10) +  
  geom_sf(data = pm_english_bivariate, aes(fill = bi_class), color = NA) +
  bi_scale_fill(pal = "PurpleGrn", dim = 3, name = NULL) +
  labs(
    title = "% Only English Language Speaking"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(size = 10),
    legend.position = "none"
  )

# Create legend as a matrix of colors
# legend <- bi_legend(
#   pal = "PurpleGrn",
#   dim = 3,
#   xlab = "Higher PM2.5",
#   ylab = "Higher English Spoken as the Only Language",
#   size = 12
# )
# 
# # Combine the map and the legend
# pm_english_map <- pm_english_map + inset_element(
#   legend,
#   left = 1.05,
#   bottom = 0.1,
#   right = 1.35,
#   top = 0.4     
# )


# PM2.5 and % White Map
# Classify data for bivariate mapping
pm_white_bivariate <- bi_class(
  full_data_w_preds_perc_spatial,
  x = `PM2.5 (ug/m^3)`, 
  y = 'p_White', 
  style = "quantile", 
  dim = 3 
)

# Generate the bivariate map
pm_white_map <- ggplot() +
  ggspatial::annotation_map_tile(type = "osm", zoom = 10) +  
  geom_sf(data = pm_white_bivariate, aes(fill = bi_class), color = NA) +
  bi_scale_fill(pal = "PurpleGrn", dim = 3, name = NULL) +
  labs(
    title = "% White"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(size = 10),
    legend.position = "none"
  )

# Create legend as a matrix of colors
# legend <- bi_legend(
#   pal = "PurpleGrn",
#   dim = 3,
#   xlab = "Higher PM2.5",
#   ylab = "Higher % White",
#   size = 12
# )
# 
# # Combine the map and the legend
# pm_white_map <- pm_white_map + inset_element(
#   legend,
#   left = 1.05,
#   bottom = 0.1,
#   right = 1.35,
#   top = 0.4     
# )


# PM2.5 and % Hispanic or Latino Map
# Classify data for bivariate mapping
pm_hispanic_bivariate <- bi_class(
  full_data_w_preds_perc_spatial,
  x = `PM2.5 (ug/m^3)`, 
  y = 'p_Hispanic', 
  style = "quantile", 
  dim = 3 
)

# Generate the bivariate map
pm_hispanic_map <- ggplot() +
  ggspatial::annotation_map_tile(type = "osm", zoom = 10) +  
  geom_sf(data = pm_hispanic_bivariate, aes(fill = bi_class), color = NA) +
  bi_scale_fill(pal = "PurpleGrn", dim = 3, name = NULL) +
  labs(
    title = "% Hispanic or Latino"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(size = 10),
    legend.position = "none"
  )

# Create legend as a matrix of colors
# legend <- bi_legend(
#   pal = "PurpleGrn",
#   dim = 3,
#   xlab = "Higher PM2.5",
#   ylab = "Higher % Hispanic or Latino",
#   size = 12
# )
# 
# # Combine the map and the legend
# pm_hispanic_map <- pm_hispanic_map + inset_element(
#   legend,
#   left = 1.05,
#   bottom = 0.1,
#   right = 1.35,
#   top = 0.4     
# )


# Combine 4 maps together
# Create a single legend
common_legend <- bi_legend(
  pal = "PurpleGrn",
  dim = 3,
  xlab = "Higher PM2.5",
  ylab = "Higher % of Demographic Variable (e.g., Poverty, White, etc.)",
  size = 8
) +
  theme(
    legend.key.width = unit(0.5, "cm"), 
    legend.key.height = unit(0.5, "cm") 
  )

# Combine the maps without individual legends
combined_pm_choropleth <- (pm_poverty_map + theme(legend.position = "none") |
                  pm_english_map + theme(legend.position = "none")) /
                 (pm_white_map + theme(legend.position = "none") |
                  pm_hispanic_map + theme(legend.position = "none"))


# Add title and legend in the correct layout
combined_pm_choropleth <- (combined_pm_choropleth | wrap_elements(common_legend)) +
  plot_annotation(
    title = "Figure: Relationship between Predicted PM2.5 (ug/m^3)\nand Significant Demographic Variables from Regression Results",
    theme = theme(
      plot.title = element_text(size = 18, face = "bold", hjust = 0.5, margin = margin(b = 20))
    )
  ) +
  plot_layout(widths = c(4, 1))

# Display the final plot
print(combined_pm_choropleth)
```

```{r no2.choropleth.map}
## generating plots for significant predictors from regression results

# NO2 and % Below Poverty Map
# Classify data for bivariate mapping
no2_poverty_bivariate <- bi_class(
  full_data_w_preds_perc_spatial,
  x = `NO2 (ppb)`, 
  y = Percent_Below_Poverty, 
  style = "quantile", 
  dim = 3 
)

# Define the bounding box for the study area
map_bbox <- st_bbox(no2_poverty_bivariate)

# Generate the bivariate map
no2_poverty_map <- ggplot() +
  ggspatial::annotation_map_tile(type = "osm", zoom = 10) +  
  geom_sf(data = no2_poverty_bivariate, aes(fill = bi_class), color = NA) +
  bi_scale_fill(pal = "PurpleGrn", dim = 3, name = NULL) +
  labs(
    title = "% Below Poverty"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(size = 10),
    legend.position = "none"
  )


# NO2 and % English Language Only Map


# Classify data for bivariate mapping
no2_english_bivariate <- bi_class(
  full_data_w_preds_perc_spatial,
  x = `NO2 (ppb)`, 
  y = 'p_engl_only', 
  style = "quantile", 
  dim = 3 
)

# Generate the bivariate map
no2_english_map <- ggplot() +
  ggspatial::annotation_map_tile(type = "osm", zoom = 10) +  
  geom_sf(data = no2_english_bivariate, aes(fill = bi_class), color = NA) +
  bi_scale_fill(pal = "PurpleGrn", dim = 3, name = NULL) +
  labs(
    title = "% Only English Language Speaking"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(size = 10),
    legend.position = "none"
  )


# NO2 and % White Map
# Classify data for bivariate mapping
no2_white_bivariate <- bi_class(
  full_data_w_preds_perc_spatial,
  x = `NO2 (ppb)`, 
  y = 'p_White', 
  style = "quantile", 
  dim = 3 
)

# Generate the bivariate map
no2_white_map <- ggplot() +
  ggspatial::annotation_map_tile(type = "osm", zoom = 10) +  
  geom_sf(data = no2_white_bivariate, aes(fill = bi_class), color = NA) +
  bi_scale_fill(pal = "PurpleGrn", dim = 3, name = NULL) +
  labs(
    title = "% White"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(size = 10),
    legend.position = "none"
  )


# # including this code if we want to be consistent with PM2.5 map, but % Hispanic wasn't a significant predictor in regression model for NO2 - need to note in write-up

# NO2 and % Hispanic or Latino Map
# Classify data for bivariate mapping
no2_hispanic_bivariate <- bi_class(
  full_data_w_preds_perc_spatial,
  x = `NO2 (ppb)`,
  y = 'p_Hispanic',
  style = "quantile",
  dim = 3
)

# Generate the bivariate map
no2_hispanic_map <- ggplot() +
  ggspatial::annotation_map_tile(type = "osm", zoom = 10) +
  geom_sf(data = no2_hispanic_bivariate, aes(fill = bi_class), color = NA) +
  bi_scale_fill(pal = "PurpleGrn", dim = 3, name = NULL) +
  labs(
    title = "% Hispanic or Latino"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(size = 10),
    legend.position = "none"
  )


# Combine 4 maps together
# Create a single legend
common_legend <- bi_legend(
  pal = "PurpleGrn",
  dim = 3,
  xlab = "Higher NO2",
  ylab = "Higher % of Demographic Variable (e.g., Poverty, White, etc.)",
  size = 8
) +
  theme(
    legend.key.width = unit(0.5, "cm"), 
    legend.key.height = unit(0.5, "cm") 
  )

# Combine the maps without individual legends
combined_no2_choropleth <- (no2_poverty_map + theme(legend.position = "none") |
                  no2_english_map + theme(legend.position = "none")) /
                 (no2_white_map + theme(legend.position = "none") |
                  no2_hispanic_map + theme(legend.position = "none"))


# Add title and legend in the correct layout
combined_no2_choropleth <- (combined_no2_choropleth | wrap_elements(common_legend)) +
  plot_annotation(
    title = "Figure: Relationship between Predicted NO2 (ppb)\nand Significant Demographic Variables from Regression Results",
    theme = theme(
      plot.title = element_text(size = 18, face = "bold", hjust = 0.5, margin = margin(b = 20))
    )
  ) +
  plot_layout(widths = c(4, 1))

# Display the final plot
print(combined_no2_choropleth)

```

```{r}
# Ensure Percent_Below_Poverty in grid_w_acs is numeric
grid_w_acs <- grid_w_acs %>%
  mutate(
    Percent_Below_Poverty = as.numeric(Percent_Below_Poverty),
    White = as.numeric(White),
    Language_other_than_English = as.numeric(Language_other_than_English),
    log_m_to_a1 = as.numeric(log_m_to_a1),
    log_m_to_comm = as.numeric(log_m_to_comm)
  )

# Check for any NA values in the predictors and handle them
grid_w_acs <- grid_w_acs %>%
  filter(
    !is.na(Percent_Below_Poverty) &
    !is.na(White) &
    !is.na(Language_other_than_English) &
    !is.na(log_m_to_a1) &
    !is.na(log_m_to_comm)
  )

## These do not account for spatial structure since they do not have UK, commenting them out (Cecilia)
# Predict PM2.5 concentrations
# grid_w_acs$predicted_pm25 <- predict(pm25_model, newdata = grid_w_acs)

# Display the first few rows of the predictions
# head(grid_w_acs$predicted_pm25)

# Predict PM2.5 concentrations
# rid_w_acs$predicted_no2 <- predict(no2_model, newdata = grid_w_acs)

# Display the first few rows of the predictions
# head(grid_w_acs$predicted_no2)

# Assuming `grid_w_acs` has a column `tract_key`

newgrid_w_acs <- grid_w_acs %>%    mutate(TRACTCE = substr(as.character(tract_key), 
            nchar(as.character(tract_key)) - 5, nchar(as.character(tract_key))))
newgrid_w_acs2 <- left_join(newgrid_w_acs, census_clean, by = "TRACTCE")

```

#improved map: DO NOT DELETE THIS MAP
```{r, eval=F}
#Define the correct bivariate color palette
bivariate_colors_corrected <- c(
  "low_low" = "#f0f0f0",  # Low PM2.5, Low Poverty (light grey, almost transparent)
  "low_high" = "#9ecae1",  # Low PM2.5, High Poverty (light blue)
  "high_low" = "#fc9272",  # High PM2.5, Low Poverty (light red)
  "high_high" = "#67000d"  # High PM2.5, High Poverty (dark red)
)

# Filter and ensure CRS is consistent
newgrid_king_county_filtered <- st_transform(newgrid_king_county, crs = 4326)

# Plot with the corrected color scheme and appropriate transparency
ggplot() +
  annotation_map_tile(type = "osm", zoom = 10) +  # OpenStreetMap basemap
  geom_sf(data = newgrid_king_county_filtered, aes(fill = bivar_category), color = NA, alpha = 0.8) +  # Increased transparency for subtle areas
  scale_fill_manual(
    values = bivariate_colors_corrected,
    name = "PM2.5 & Poverty Levels",
    labels = c(
      "High PM2.5, High Poverty",
      "High PM2.5, Low Poverty",
      "Low PM2.5, High Poverty",
      "Low PM2.5, Low Poverty")
  ) +
  labs(
    title = "PM2.5 and Poverty Levels in King County, WA",
    subtitle = "High PM2.5 and High Poverty areas are highlighted in dark red",
    caption = "Data Sources: ACS, Mobile Monitoring, OpenStreetMap",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 14, hjust = 0.5),
    plot.caption = element_text(size = 10, hjust = 1),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "right",  # Legend on the right-hand side
    legend.box = "vertical",
    panel.background = element_rect(fill = "transparent", color = NA)
  )

```

```{r, eval=FALSE}
colnames(newgrid_king_county)

# Load the dataset
data <- newgrid_king_county

# Clean the data: Ensure columns are numeric and remove invalid values
data <- data %>%
  mutate(
    predicted_no2 = as.numeric(as.character(predicted_no2)),
    Percent_Below_Poverty = as.numeric(as.character(Percent_Below_Poverty))  # Replace with the actual column name for below poverty
  ) %>%
  filter(
    !is.na(predicted_no2),
    !is.na(Percent_Below_Poverty),
    is.finite(predicted_no2),
    is.finite(Percent_Below_Poverty)
  )

# Fit the linear regression model
model_poverty <- lm(predicted_no2 ~ Percent_Below_Poverty, data = data)

# Summarize the model
summary(model_poverty)

# Optional: Plot the relationship
ggplot(data, aes(x = Percent_Below_Poverty, y = predicted_no2)) +
  geom_point() +
  geom_smooth(method = "lm", color = "blue") +
  labs(
    title = "Linear Regression: Predicted NO2 vs Below Poverty",
    x = "Percent Below Poverty",
    y = "Predicted NO2"
  )

```

#Map of Predicted NO2 and poverty
```{r, eval=FALSE}
# Normalize NO2 and Poverty levels to a 0–1 range
newgrid_king_county <- newgrid_king_county %>%
  mutate(
    no2_scaled = rescale(predicted_no2, to = c(0, 1)),  # Assuming `predicted_no2` is the column for NO2
    poverty_scaled = rescale(Percent_Below_Poverty, to = c(0, 1)),
    bivar_category = case_when(
      no2_scaled <= 0.5 & poverty_scaled <= 0.5 ~ "low_low",
      no2_scaled <= 0.5 & poverty_scaled > 0.5 ~ "low_high",
      no2_scaled > 0.5 & poverty_scaled <= 0.5 ~ "high_low",
      no2_scaled > 0.5 & poverty_scaled > 0.5 ~ "high_high"
    )
  )

# Define a bivariate color palette
bivariate_colors_no2 <- c(
  "low_low" = "#f0f0f0",  # Low NO2, Low Poverty (light grey)
  "low_high" = "#9ecae1",  # Low NO2, High Poverty (light blue)
  "high_low" = "#fc9272",  # High NO2, Low Poverty (light red)
  "high_high" = "#67000d"  # High NO2, High Poverty (dark red)
)

# Ensure CRS is consistent for mapping
newgrid_king_county_filtered <- st_transform(newgrid_king_county, crs = 4326)

# Plot the NO2 and Poverty map
ggplot() +
  annotation_map_tile(type = "osm", zoom = 10) +  # OpenStreetMap basemap
  geom_sf(
    data = newgrid_king_county_filtered,
    aes(fill = bivar_category),
    color = NA,
    alpha = 0.8
  ) +
  scale_fill_manual(
    values = bivariate_colors_no2,
    name = "NO2 & Poverty Levels",
    breaks = c("low_low", "low_high", "high_low", "high_high"),  # Ensures the order matches the color definitions
    labels = c(
      "Low NO2, Low Poverty",   # Corresponds to "low_low"
      "Low NO2, High Poverty",  # Corresponds to "low_high"
      "High NO2, Low Poverty",  # Corresponds to "high_low"
      "High NO2, High Poverty"  # Corresponds to "high_high"
    )
  ) +
  labs(
    title = "NO2 and Poverty Levels in King County, WA",
    subtitle = "High NO2 and High Poverty areas are highlighted in dark red",
    caption = "Data Sources: ACS, Mobile Monitoring, OpenStreetMap",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 14, hjust = 0.5),
    plot.caption = element_text(size = 10, hjust = 1),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "right",  # Legend on the right-hand side
    legend.box = "vertical",
    panel.background = element_rect(fill = "transparent", color = NA)
  )

```

#Linear regression of predicted NO2 and language diversity
```{r, eval=FALSE}

# Load the dataset
data <- newgrid_king_county

# Clean the data: Ensure columns are numeric and remove invalid values
data <- data %>%
  mutate(
    predicted_no2 = as.numeric(as.character(predicted_no2)),
    Language_other_than_English = as.numeric(as.character(Language_other_than_English))
  ) %>%
  filter(
    !is.na(predicted_no2), 
    !is.na(Language_other_than_English),
    is.finite(predicted_no2), 
    is.finite(Language_other_than_English)
  )

# Fit the linear regression model
model <- lm(predicted_no2 ~ Language_other_than_English, data = data)

# Summarize the model
summary(model)

# Optional: Plot the relationship
ggplot(data, aes(x = Language_other_than_English, y = predicted_no2)) +
  geom_point() +
  geom_smooth(method = "lm", color = "blue") +
  labs(
    title = "Linear Regression: Predicted NO2 vs Language Other Than English",
    x = "Language Other Than English",
    y = "Predicted NO2"
  )

```

```{r, eval=FALSE}
# Ensure columns are numeric and remove invalid values
newgrid_king_county <- newgrid_king_county %>%
  mutate(
    predicted_no2 = as.numeric(as.character(predicted_no2)),
    Language_other_than_English = as.numeric(as.character(Language_other_than_English))
  ) %>%
  filter(
    !is.na(predicted_no2), 
    !is.na(Language_other_than_English),
    is.finite(predicted_no2), 
    is.finite(Language_other_than_English)
  ) %>%
  mutate(
    no2_scaled = rescale(predicted_no2, to = c(0, 1)),
    language_scaled = rescale(Language_other_than_English, to = c(0, 1)),
    bivar_category = case_when(
      no2_scaled <= 0.5 & language_scaled <= 0.5 ~ "low_low",
      no2_scaled <= 0.5 & language_scaled > 0.5 ~ "low_high",
      no2_scaled > 0.5 & language_scaled <= 0.5 ~ "high_low",
      no2_scaled > 0.5 & language_scaled > 0.5 ~ "high_high"
    )
  )

# Ensure it's an sf object and transform CRS
if (!inherits(newgrid_king_county, "sf")) {
  newgrid_king_county <- st_as_sf(newgrid_king_county, coords = c("longitude", "latitude"), crs = 4326)
}
newgrid_king_county_filtered <- st_transform(newgrid_king_county, crs = 4326)

# Define the bivariate color palette
bivariate_colors_no2 <- c(
  "low_low" = "#f0f0f0", 
  "low_high" = "#9ecae1", 
  "high_low" = "#fc9272", 
  "high_high" = "#67000d"
)

# Plot the map
ggplot() +
  annotation_map_tile(type = "osm", zoom = 10) +
  geom_sf(
    data = newgrid_king_county_filtered,
    aes(fill = bivar_category),
    color = NA,
    alpha = 0.8
  ) +
  scale_fill_manual(
    values = bivariate_colors_no2,
    name = "NO2 & Language Levels",
    breaks = c("low_low", "low_high", "high_low", "high_high"),
    labels = c(
      "Low NO2, Low Language Diversity",
      "Low NO2, High Language Diversity",
      "High NO2, Low Language Diversity",
      "High NO2, High Language Diversity"
    )
  ) +
  labs(
    title = "Predicted NO2 and Language Diversity",
    caption = "Data Sources: ACS, Mobile Monitoring, OpenStreetMap",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
    plot.caption = element_text(size = 10, hjust = 1),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "right",
    legend.box = "vertical",
    panel.background = element_rect(fill = "transparent", color = NA)
  )

```

```{r, eval=FALSE}
# Load the dataset
data <- newgrid_king_county

# Clean the data: Ensure all relevant columns are numeric and remove invalid values
data <- data %>%
  mutate(
    predicted_no2 = as.numeric(as.character(predicted_no2)),
    White = as.numeric(as.character(White)),
    Black_AfricanAmerican = as.numeric(as.character(Black_AfricanAmerican)),
    AIAN = as.numeric(as.character(AIAN)),
    Asian = as.numeric(as.character(Asian)),
    Hawaiian_PacificIslander = as.numeric(as.character(Hawaiian_PacificIslander)),
    Other_alone = as.numeric(as.character(Other_alone)),
    X2ormore = as.numeric(as.character(X2ormore)),
    X2ormore_andsomeother = as.numeric(as.character(X2ormore_andsomeother)),
    X3ormore = as.numeric(as.character(X3ormore))
  ) %>%
  filter(
    !is.na(predicted_no2),
    is.finite(predicted_no2),
    !is.na(White),
    !is.na(Black_AfricanAmerican),
    !is.na(AIAN),
    !is.na(Asian),
    !is.na(Hawaiian_PacificIslander),
    !is.na(Other_alone),
    !is.na(X2ormore),
    !is.na(X2ormore_andsomeother),
    !is.na(X3ormore)
  )

# Fit the multiple linear regression model (excluding `_ME` variables)
model <- lm(predicted_no2 ~ White + 
            Black_AfricanAmerican + 
            AIAN + 
            Asian + 
            Hawaiian_PacificIslander + 
            Other_alone + 
            X2ormore + 
            X2ormore_andsomeother + 
            X3ormore,
            data = data)

# Summarize the model
summary(model)

# Extract the coefficients from the model
model_summary <- summary(model)
coefficients_table <- as.data.frame(model_summary$coefficients)

# Rename columns for clarity
colnames(coefficients_table) <- c("Estimate", "Std. Error", "t value", "Pr(>|t|)")

# Add row names as a column for variable names
coefficients_table <- tibble::rownames_to_column(coefficients_table, "Variable")

# Print the table in the console
print(coefficients_table)

# Optional: Use kable for better visualization in RMarkdown or RStudio
library(knitr)
kable(coefficients_table, caption = "Regression Results for Predicted NO2 and Race Variables")


# Optional: Diagnostics for the model
par(mfrow = c(2, 2))  # Arrange plots in a 2x2 grid
plot(model)

###Racial diversity - NOTE: size needs to be explored so maps are less squished

#Text for write up: This map visualizes the racial and ethnic composition of King County, Washington, by census tract, with an OpenStreetMap background for spatial context. Each panel represents the percentage distribution of a specific racial or ethnic group, including American Indian/Alaskan Native, Asian, Black or African American, Hawaiian or Pacific Islander, Hispanic or Latino, and White. The color gradient indicates the concentration, with darker shades representing higher percentages. This visual provides a detailed geographic understanding of demographic patterns across the county. Data sources include ACS and OpenStreetMap.
```

```{r, eval=FALSE}
# For adding OpenStreetMap tiles

# Define the racial/ethnic variables and their readable names
selected_race_vars <- c(
  "Black_AfricanAmerican",
  "White",
  "Asian",
  "AIAN",
  "Hawaiian_PacificIslander",
  "HispanicorLatino"
)

readable_race_names <- c(
  "Black or African American",
  "White",
  "Asian",
  "American Indian/Alaskan Native",
  "Hawaiian or Pacific Islander",
  "Hispanic or Latino"
)

# Ensure CRS is consistent for mapping
newgrid_king_county <- st_transform(newgrid_king_county, crs = 4326)

# Pivot dataset to long format for mapping multiple racial groups
newgrid_long <- newgrid_king_county %>%
  select(geometry, all_of(selected_race_vars)) %>%
  pivot_longer(
    cols = all_of(selected_race_vars),
    names_to = "Race",
    values_to = "Percentage"
  ) %>%
  mutate(
    Race = recode(Race, 
                  "Black_AfricanAmerican" = "Black or African American",
                  "White" = "White",
                  "Asian" = "Asian",
                  "AIAN" = "American Indian/Alaskan Native",
                  "Hawaiian_PacificIslander" = "Hawaiian or Pacific Islander",
                  "HispanicorLatino" = "Hispanic or Latino")
  )

# Create the map with OpenStreetMap as the background
ggplot() +
  annotation_map_tile(type = "osm", zoom = 10) +  # Add OpenStreetMap tiles as the base layer
  geom_sf(
    data = newgrid_long,
    aes(fill = Percentage, geometry = geometry),
    color = NA,
    alpha = 0.8
  ) +
  facet_wrap(~ Race, ncol = 2) +  # Facet by race for multiple maps
  scale_fill_viridis_c(
    name = "Percentage",
    option = "plasma",
    na.value = "grey90"
  ) +
  labs(
    title = "Spatial Distributon of Dominant Race per Census Tract",
    caption = "Data Sources: ACS, OpenStreetMap",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
    plot.caption = element_text(size = 10, hjust = 1),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "right",
    panel.background = element_rect(fill = "transparent", color = NA)
  )

##Racial map choropleth
#Text for write up: 
#This map displays the dominant racial or ethnic group by census tract in Snohomish, Pierce, and King County, Washington, overlaid on an OpenStreetMap basemap for geographic context. Each tract is color-coded to represent the racial or ethnic group with the highest proportion of residents. Groups represented include American Indian/Alaskan Native, Asian, Black or African American, Hawaiian or Pacific Islander, Hispanic or Latino, and White. This visualization highlights the spatial distribution of demographic dominance across the county, providing insights into the region's racial and ethnic diversity. Data sources include the American Community Survey (ACS), Mobile Monitoring, and OpenStreetMap.
```

```{r, eval=FALSE}
# For adding OpenStreetMap tiles

# Ensure CRS consistency for OpenStreetMap compatibility
newgrid_king_county <- st_transform(newgrid_king_county, crs = 4326)

# Normalize racial/ethnic data to 0–1 range
newgrid_normalized <- newgrid_king_county %>%
  mutate(across(all_of(selected_race_vars), ~ rescale(.x, to = c(0, 1))))

# Identify the dominant race in each tract
newgrid_normalized <- newgrid_normalized %>%
  rowwise() %>%
  mutate(
    dominant_race = selected_race_vars[which.max(c_across(all_of(selected_race_vars)))]
  ) %>%
  ungroup() %>%
  mutate(
    dominant_race = recode(dominant_race,
                           "Black_AfricanAmerican" = "Black or African American",
                           "White" = "White",
                           "Asian" = "Asian",
                           "AIAN" = "American Indian/Alaskan Native",
                           "Hawaiian_PacificIslander" = "Hawaiian or Pacific Islander",
                           "HispanicorLatino" = "Hispanic or Latino")
  )

# Assign colors for each dominant race
race_colors <- c(
  "Black or African American" = "#fc9272",   # Light red
  "White" = "#f0f0f0",                       # Light grey
  "Asian" = "#9ecae1",                       # Light blue
  "American Indian/Alaskan Native" = "#a1d99b",  # Light green
  "Hawaiian or Pacific Islander" = "#bcbddc",    # Light purple
  "Hispanic or Latino" = "#fdae6b"           # Light orange
)

# Plot the map using OpenStreetMap tiles
ggplot() +
  annotation_map_tile(type = "osm", zoom = 10) +  # Add OpenStreetMap tiles as the base layer
  geom_sf(
    data = newgrid_normalized,
    aes(fill = dominant_race),
    color = NA,
    alpha = 0.8
  ) +
  scale_fill_manual(
    values = race_colors,
    name = "Prominent Race"
  ) +
  labs(
    title = "Spatial Distribution of Race by Census Tract",
    subtitle = "Dominant racial group per census tract",
    caption = "Data Sources: ACS, OpenStreetMap",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 14, hjust = 0.5),
    plot.caption = element_text(size = 10, hjust = 1),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "right",
    panel.background = element_rect(fill = "transparent", color = NA)
  )

```

```{r, eval=FALSE}
# For adding OpenStreetMap tiles

# Define the language variables and their readable names
selected_language_vars <- c(
  "English_only",
  "Language_other_than_English"
)

readable_language_names <- c(
  "English Only",
  "Language Other Than English"
)

# Ensure CRS is consistent for mapping
newgrid_king_county <- st_transform(newgrid_king_county, crs = 4326)

# Pivot dataset to long format for mapping language groups
newgrid_long_language <- newgrid_king_county %>%
  select(geometry, all_of(selected_language_vars)) %>%
  pivot_longer(
    cols = all_of(selected_language_vars),
    names_to = "Language",
    values_to = "Percentage"
  ) %>%
  mutate(
    Language = recode(Language, 
                      "English_only" = "English Only",
                      "Language_other_than_English" = "Language Other Than English")
  )

# Create the map with OpenStreetMap as the background
ggplot() +
  annotation_map_tile(type = "osm", zoom = 10) +  # Add OpenStreetMap tiles as the base layer
  geom_sf(
    data = newgrid_long_language,
    aes(fill = Percentage, geometry = geometry),
    color = NA,
    alpha = 0.8
  ) +
  facet_wrap(~ Language, ncol = 2) +  # Facet by language group
  scale_fill_viridis_c(
    name = "Percentage",
    option = "plasma",
    na.value = "grey90"
  ) +
  labs(
    title = "Spatial Distribution of Language Diversity by Census Tract",
    caption = "Data Sources: ACS, OpenStreetMap",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
    plot.caption = element_text(size = 10, hjust = 1),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "right",
    panel.background = element_rect(fill = "transparent", color = NA)
  )

##Language diversity
#Text for write up: 
#This map visualizes language use at home by census tract in Snohomish, Pierce, and King County, Washington, with an OpenStreetMap background for geographic context. The two panels represent the percentage distribution of households where English is the primary language ("English Only") and households where other languages are predominantly spoken ("Language Other Than English").

#The left panel highlights areas where English is predominantly spoken, with darker shades indicating higher percentages. The right panel illustrates areas where other languages dominate, showing significant linguistic diversity across the region. The color gradient reflects the proportion of households, from lower percentages in blue to higher percentages in yellow. This map provides insights into the spatial distribution of language use in the county. Data sources include the American Community Survey (ACS) and OpenStreetMap.

```

# Appendix 1: Supplementary results

## Map of Study Area Stop Concentrations Using Stop Data

```{r study.area.stoplevel.map}
#---- Stop data map (put in Appendix) ----
# Add NO2 data with additional map elements
no2_map_stop <- g + 
  geom_sf(data = stop_for_preds %>% filter(variable == "no2"), aes(color = mean_value)) + 
  scale_color_viridis_c() + 
  theme_void() +  
  theme(
    panel.border = element_rect(color = "black", fill = NA, size = 1),
     plot.title = element_text(hjust = 0.5)
  ) + 
  ggspatial::annotation_scale(location = "bl", width_hint = 0.3, unit_category = "imperial") +
  ggspatial::annotation_north_arrow(location = "tr", which_north = "true", style = north_arrow_fancy_orienteering) +  
  labs(col="NO2 (ppb)", title="NO2"
       )

# Add PM2.5 data with additional map elements, restricting scale to 40 and below
pm25_map_stop <- g + 
  geom_sf(data = stop_for_preds %>% filter(variable == "neph_bscat" & mean_value<40), aes(color = mean_value)) + 
  scale_color_viridis_c() +
  theme_void() +  
  theme(
    panel.border = element_rect(color = "black", fill = NA, size = 1),
    plot.title = element_text(hjust = 0.5)
  ) + 
  ggspatial::annotation_scale(location = "bl", width_hint = 0.3, unit_category = "imperial") +
  ggspatial::annotation_north_arrow(location = "tr", which_north = "true", style = north_arrow_fancy_orienteering) +  
  labs(col="PM2.5 (ug/m3)", title="PM2.5"
       )

no2_map_stop + pm25_map_stop + plot_annotation(title = 'Map of Study Area Stop Concentrations',
  #subtitle = 'NO2 (left) and PM2.5 (right)',
  caption = 'Both pollutants are presented on the native scale. \n NO2 is in ppb and PM2.5 is in ug/m3. This map captures 8638 individual\n stops over 304 locations.',
  theme = theme(plot.title = element_text(face = "bold", hjust = 0.5),
                plot.subtitle=element_text(hjust=0.5),
                plot.caption=element_text(hjust=0.5)))
```

## NO2 Variograms and UK Predictions

```{r variogram.plots.no2.appendix}
# ggplot(vgm_no2_annual, aes(x = dist, y=gamma)) + 
#   geom_point(alpha=0.1) + 
#   labs(x="Distance (meters)",
#        title="Variogram Cloud Using Annual Mean Concentration: NO2")
# 
# # Binned Variogram with annual data
# plot(variogram(log_value ~ pop10_s05000 + lu_industrial_p03000 + log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm, annual_for_preds %>% filter(variable == "no2")), main="Binned Variogram with Annual Concentration Data: NO2")
# 
# # Binned Variogram with Point Counts
# plot(variogram(log_value ~ pop10_s05000 + lu_industrial_p03000 + log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm, annual_for_preds %>% filter(variable == "no2")), pl = TRUE, main="Binned Variogram with Annual Concentration Data with Point Counts: NO2")
# 
# # Semi-variogram cloud with smoothed curve
# ggplot(data = vgm_no2_annual, aes(x = dist, y = gamma)) +
#   geom_point(shape = 1, alpha = 0.6) +
#   geom_smooth(se = FALSE, color = "red", linetype = "solid",
#               # making span > 0.75 (default) makes this less wiggly so we can better see the general trend 
#               method = "loess", 
#               span = .8
#               ) +
#   labs(x = "Distance (meters)", 
#        y = "Semi-variance",
#        title = "Semi-variogram Cloud with Smoothed Curve: NO2") +
#   theme_bw() +
#   theme(legend.position = "none")

plot(v_annual, v_annual.fit, main="Empirical Variogram with Fitted Model Overlaid: NO2")
```

```{r grid.UK.preds.logNO2}
# Plot UK predictions 
# pl3 <- plot(ln.no2.kr["var1.pred"], main = "UK Prediction of Log(NO2) on Grid")
# 
# # Plot UK prediction standard errors 
# pl4 <- plot(ln.no2.kr["se"], main = "UK Prediction Error NO2 (Standard Error) on Grid")

# Plot UK predictions
#pl3 <- plot(ln.NO2.block.kr["var1.pred"], main = "UK Prediction of Log(PM2.5) on Census Block Centroids")

# Plot UK prediction standard errors
pl4 <- plot(ln.NO2.block.kr["se"], main = "UK Prediction Error PM2.5\n (Standard Error) on Census Block Centroids")
```

## PM2.5 Variograms and UK Predictions

```{r variogram.plots.pm25.appendix}
#plot(v.uk.pm25, main="Estimated Variogram for log(PM2.5)")

plot(v.uk.pm25, model = m.uk.pm25, main="Estimated Variogram for log(PM2.5) with Fitted Variogram")
```

```{r UK.preds.logPM25}
# Plot UK predictions grid
# pl3 <- plot(ln.PM25.kr.grid["var1.pred"], main = "UK Prediction of Log(PM2.5) on Grid")
# 
# # Plot UK prediction standard errors grid
# pl4 <- plot(ln.PM25.kr.grid["se"], main = "UK Prediction Error PM2.5 (Standard Error) on Grid")

# Plot UK predictions block
#pl3 <- plot(ln.PM25.kr["var1.pred"], main = "UK Prediction of Log(PM2.5) on Census Block Centroids")

# Plot UK prediction standard errors block
pl4 <- plot(ln.PM25.kr["se"], main = "UK Prediction Error PM2.5 (Standard Error)\n on Census Block Centroids")
```

## Univariate regression models

```{r univar. results}

# Formatted table for regression results NO2
no2_univar_res %>%
  select(term, intercept, estimate, std.error, statistic, p.value, conf.low, conf.high) %>% 
  mutate(
    Term = case_when(
      term == "Percent_Below_Poverty" ~ "% Below Poverty",
      term == "p_engl_only" ~ "% English Language Only",
      term == "p_lang_other" ~ "% Other Language",
      term == "p_spanish" ~ "% Spanish Language",
      term == "p_White" ~ "% White",
      term == "p_BlackAA" ~ "% Black or African American",
      term == "p_AIAN" ~ "% American Indian or Alaska Native",
      term == "p_Asian" ~ "% Asian",
      term == "p_HPI" ~ "% Hawaiian or Pacific Islander",
      term == "p_Hispanic" ~ "% Hispanic or Latino",
      TRUE ~ term 
    )
  ) %>%
  select(Term, intercept, estimate, std.error, statistic, p.value, conf.low, conf.high) %>%
  rename(
    Estimate = estimate,
    Intercept = intercept,
    `Std. Error` = std.error,
    `t-value` = statistic,
    `P-value` = p.value,
    `95%CI Low` = conf.low,
    `95%CI High` = conf.high
  ) %>% 
  kable(
    caption = "Table: Univariate Regression for Association Results for NO2 Model. Response variable was log(NO2), and all results have been exponentiated. Each row represents a separate univariate regression model, with the Term as the predictor variable.",
    digits = 3, 
    col.names = c("Term", "Intercept", "Estimate", "Std. Error", "t-value", "P-value", "95% CI Low", "95% CI High")
  ) %>%
  kable_styling(
    full_width = FALSE, 
    bootstrap_options = c("striped", "hover", "condensed")
  )

# Formatted table for regression results
pm_univar_res %>%
  select(term, intercept, estimate, std.error, statistic, p.value, conf.low, conf.high) %>% 
  mutate(
    Term = case_when(
      term == "Percent_Below_Poverty" ~ "% Below Poverty",
      term == "p_engl_only" ~ "% English Language Only",
      term == "p_lang_other" ~ "% Other Language",
      term == "p_spanish" ~ "% Spanish Language",
      term == "p_White" ~ "% White",
      term == "p_BlackAA" ~ "% Black or African American",
      term == "p_AIAN" ~ "% American Indian or Alaska Native",
      term == "p_Asian" ~ "% Asian",
      term == "p_HPI" ~ "% Hawaiian or Pacific Islander",
      term == "p_Hispanic" ~ "% Hispanic or Latino",
      TRUE ~ term 
    )
  ) %>%
  select(Term, intercept, estimate, std.error, statistic, p.value, conf.low, conf.high) %>%
  rename(
    Estimate = estimate,
    Intercept = intercept,
    `Std. Error` = std.error,
    `t-value` = statistic,
    `P-value` = p.value,
    `95%CI Low` = conf.low,
    `95%CI High` = conf.high
  ) %>% 
  kable(
    caption = "Table: Univariate Regression for Association Results for PM2.5 Model. Response variable was log(PM2.5), and all results have been exponentiated. Each row represents a separate univariate regression model, with the Term as the predictor variable.",
    digits = 3, 
    col.names = c("Term", "Intercept", "Estimate", "Std. Error", "t-value", "P-value", "95% CI Low", "95% CI High")
  ) %>%
  kable_styling(
    full_width = FALSE, 
    bootstrap_options = c("striped", "hover", "condensed")
  )
```

# Appendix 2: 

## Session information

```{r session.info}
#-----session information-----

# print R session information
sessionInfo()

```

## Embedded code

```{r code.appendix, ref.label = knitr::all_labels(), echo = TRUE, eval = FALSE, , include=TRUE}
#-----code appendix-----
```

## Functions defined 

```{r functions, eval = TRUE}
#-----functions-----

# Show the names of all functions defined in the .Rmd
# (e.g. loaded in the environment)
lsf.str()

# Show the definitions of all functions loaded into the current environment  
lapply(c(lsf.str()), getAnywhere)
```
