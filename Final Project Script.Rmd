
---
title: "Final Project Script"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: true
toc_depth: 3
date: "2024-11-21"
editor_options: 
  chunk_output_type: console
---

# Set-up and load data
```{r setup}
# Clear workspace of all objects and unload non-base packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
    suppressWarnings(
        lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
               detach, character.only=TRUE, unload=TRUE, force=TRUE)
    )
}

# Load or install 'pacman' for package management
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {
    install.packages("pacman", repos = my_repo)
}

# **SPH server**: need to install rnaturalearthhires like so on the SPH server
if (!require("rnaturalearthhires")) {
    install.packages("rnaturalearthhires", repos = "https://ropensci.r-universe.dev", type = "source")
}

pacman::p_load(
    tidyverse,                 # Data manipulation and visualization
    # takes a while to install on SPH
    biscale,
    broom,
    ggspatial,                 # Geospatial extensions for ggplot.  
    maptiles, # maptiles and tmap libraries can be used instead of or in combination with ggplot + ggspatial. maptiles offers more tile-based map flexibility; ggspatial provides the ability to annotate maps easily; tmap offers both static and interactive maps that we won't review in this course. 
    terra, # alternative mapping with raster files
    
    # need for SPH server?
    prettymapr,
    
    rnaturalearth,             # Land features for map layers (remove water locations)
    rnaturalearthhires,        # High-resolution land features 
    sf,                        # Handling spatial objects (modern replacement for 'sp')
    knitr,                     # Formatting tables with kable()
    gstat,                     # Geostatistical methods (e.g., kriging)
    Hmisc,                     # Data description functions like describe()
    scales,                    # Color scale customization for ggplot
    akima,                     # Bivariate interpolation for irregular data
    downloader,                 # Downloading files over HTTP/HTTPS
    tigris,
    RColorBrewer,
    scales,
    kableExtra, patchwork
)

```

```{r load.data}

annual_data <- read_csv("annual_data_and_predictions.csv")

census_shapefile <- st_read("cb_2019_53_tract_500k.shp")

grid_covariates <- read_csv("dr0311_grid_covariates.csv")

mobile_covariates <- read_csv("dr0311_mobile_covariates.csv")

stop_data <- read_csv("stop_data.csv")

acs_race <- read_csv("ACS19_race.csv")

acs_ethnicity <- read_csv("ACS19_ethnicity.csv")

acs_language <- read_csv("ACS19_language.csv")

acs_median_income <- read_csv("ACS19_median_income.csv")

acs_poverty <- read_csv("ACS19_poverty.csv")

## we do not want this file -- these are predictions? We need the covariates
#unzip("filtered_census_block_predictions.csv.zip")
#census_block_centroids <- read_csv("filtered_census_block_predictions.csv")

census_blocks_covar <- read_csv("wa_block_covar_clean.csv")
```

# Clean data

We selected only NO2 (no2) and PM2.5 (neph_bscat) variables from the stop data and the annual data. We obtained Census shapefile data from the Census Bureau's website for Washington, and only kept those within King, Snohomish, Pierce, and Kitsap counties (*might not need Kitsap but I didn't want to open the map to check*). We linked data using the 6-digit tract code.

Prior to log-transforming covariates, we added 1 to each variable.

```{r data.cleaning}
# annual data
#glimpse(annual_data)
table(annual_data$variable)

#create variable to differentiate between stop site and collocation site
annual_data <- annual_data %>% 
  mutate(SiteType = ifelse(grepl("MS", location), "stop site",
                       ifelse(grepl("MC", location), "collocation site", NA)))

# keep only NO2 and PM, get rid of annual variable since they are all mean_of_win_medians
annual_clean <- annual_data %>% 
  filter(variable == "no2" | variable == "pm2.5_ug_m3") %>%
  filter(SiteType == "stop site") %>% 
  select(-c(annual, SiteType))


# stop data
#create variable to differentiate between stop site and collocation site
stop_data <- stop_data %>% 
  mutate(SiteType = ifelse(grepl("MS", location), "stop site",
                       ifelse(grepl("MC", location), "collocation site", NA)))

stop_clean <- stop_data %>% 
  filter(variable == "no2" | variable == "neph_bscat") %>%
  filter(SiteType == "stop site") %>% 
  select(c(runname, time, location, stop_id, instrument_id, variable, mean_value, median_value))   

# Get location coordinates
stop_coords <- annual_clean %>% select(c("location", "longitude", "latitude")) %>%
  unique()

stop_clean <- stop_clean %>% 
  filter(median_value > 0) %>% # what are our thoughts on this? Thought: should we report how many this got rid of?
  mutate(log_mean = log(mean_value)) %>% 
  mutate(log_median = log(median_value)) %>% 
  filter(log_median > 0) %>% # and on this? 
  left_join(stop_coords)

# Generate NO2 specific dataset
no2_stop_clean <- stop_clean %>%
  filter(variable == "no2")

# Generate PM2.5 specific dataset
pm25_stop_clean <- stop_clean %>% 
  filter(variable == "neph_bscat")


# census data
#glimpse(census_shapefile)
table(census_shapefile$COUNTYFP) # there are 39 counties, we don't need them all 
# only keep King (033), Snohomish (061), Pierce (053), Kitsap (035)
# get rid of LSAD and STATEFP since they are all the same
census_clean <- census_shapefile %>%
  filter(COUNTYFP %in% c("033", "061", "053", "035")) %>%
  select(-c(LSAD,STATEFP)) %>% 
  st_as_sf()

# grid covar
#glimpse(grid_covariates)
# create 6 digit TRACTCE var that matches census shapefile var
# log-transform distance covariates

grid_covar_clean <- grid_covariates %>%
  mutate(TRACTCE = substr(as.character(tract_key), 
            nchar(as.character(tract_key)) - 5, nchar(as.character(tract_key)))) %>%
  select(c(location_id, native_id, tract_key, m_to_a1, m_to_a2, m_to_a3, pop10_s05000, m_to_coast, m_to_l_airp, m_to_comm,lu_industrial_p03000, longitude, latitude)) %>%
  mutate(log_m_to_a1 = log(m_to_a1 + 1), log_m_to_a2=log(m_to_a2 +1),
         log_m_to_a3=log(m_to_a3+1),
         log_m_to_coast=log(m_to_coast+1), log_m_to_l_airp=log(m_to_l_airp+1), 
         log_m_to_comm=log(m_to_comm+1)) %>% 
  select(-c(m_to_a1,m_to_a2,m_to_a3,m_to_coast,m_to_l_airp,m_to_comm))

# mobile covar
#glimpse(mobile_covariates)
# create 6 digit TRACTCE var that matches census shapefile var
mobile_covar_clean <- mobile_covariates %>%
  mutate(TRACTCE = substr(as.character(tract_key), 
            nchar(as.character(tract_key)) - 5, nchar(as.character(tract_key)))) %>%
  select(c(location_id, native_id, tract_key, TRACTCE, m_to_a1, m_to_a2, m_to_a3, pop10_s05000, m_to_coast, m_to_l_airp, m_to_comm,lu_industrial_p03000)) %>%
  mutate(log_m_to_a1 = log(m_to_a1+1), log_m_to_a2=log(m_to_a2+1), log_m_to_a3=log(m_to_a3+1),
         log_m_to_coast=log(m_to_coast+1), log_m_to_l_airp=log(m_to_l_airp+1), 
         log_m_to_comm=log(m_to_comm+1)) %>% 
  select(-c(m_to_a1,m_to_a2,m_to_a3,m_to_coast,m_to_l_airp,m_to_comm))

# create dataset with stop and mobile data combined
stop_and_mobile <- merge(stop_clean, mobile_covar_clean,
                       by.x = "location", by.y = "native_id", 
                       all = FALSE)

# Join with census_clean
stop_mobile_census <- census_clean %>%
  inner_join(stop_and_mobile, by = "TRACTCE") %>% 
  select(c(TRACTCE, location, time, variable, mean_value, median_value, pop10_s05000, lu_industrial_p03000, log_m_to_a1, log_m_to_a2, log_m_to_a3, log_m_to_coast, log_m_to_l_airp, log_m_to_comm, geometry))


# acs race data
#glimpse(acs_race)
# create 6 digit TRACTCE var that matches census shapefile var
acs_race_clean <- acs_race %>%
  mutate(TRACTCE = substr(as.character(GEO_ID), 
            nchar(as.character(GEO_ID)) - 5, nchar(as.character(GEO_ID))))
#colnames(acs_race_clean)
# acs ethnicity data
#glimpse(acs_ethnicity)
# create 6 digit TRACTCE var that matches census shapefile var
acs_ethnicity_clean <- acs_ethnicity %>%
  mutate(TRACTCE = substr(as.character(GEO_ID), 
            nchar(as.character(GEO_ID)) - 5, nchar(as.character(GEO_ID))))
#colnames(acs_ethnicity_clean)
# acs language data
#glimpse(acs_language)
# create 6 digit TRACTCE var that matches census shapefile var
acs_language_clean <- acs_language %>%
  mutate(TRACTCE = substr(as.character(GEO_ID), 
            nchar(as.character(GEO_ID)) - 5, nchar(as.character(GEO_ID))))
#colnames(acs_language_clean)
# acs median income data
#glimpse(acs_median_income)
# create 6 digit TRACTCE var that matches census shapefile var
#---------------------------------------------------------------
#Steph: I don't think we need this anymore since we have made the merge, I'm commenting it out for deletion purposes
#acs_median_income_clean <- acs_median_income %>%
  # mutate(TRACTCE = substr(as.character(GEO_ID), 
  #           nchar(as.character(GEO_ID)) - 5, nchar(as.character(GEO_ID))),
  #        Total_Households_Income = as.numeric(Total_Households_Income),
  #        Total_Households_Income_ME = as.numeric(Total_Households_Income_ME),
  #        White_Households_Income = as.numeric(White_Households_Income),
  #        White_Households_Income_ME = as.numeric(White_Households_Income_ME),
  #        Black_Households_Income = as.numeric(Black_Households_Income),
  #        Black_Households_Income_ME = as.numeric(Black_Households_Income_ME),
  #        AIAN_Households_Income = as.numeric(AIAN_Households_Income),
  #        AIAN_Households_Income_ME = as.numeric(AIAN_Households_Income_ME),
  #        Asian_Households_Income = as.numeric(Asian_Households_Income_ME),
  #        Hawaiian_PI_Households_Income = as.numeric(Hawaiian_PI_Households_Income_ME),
  #        Other_alone_Households_Income = as.numeric(Other_alone_Households_Income),
  #        Other_alone_Households_Income_ME = as.numeric(Other_alone_Households_Income_ME),
  #        HispanicorLatino_Households_Income = as.numeric(HispanicorLatino_Households_Income),
  #        HispanicorLatino_Households_Income_ME=as.numeric(HispanicorLatino_Households_Income_ME)
  #        )
#---------------------------------------------------------------
```

```{r data.cleaning.census.blocks}
###how I got the unique tracts for cleaning the wa blocks
# unique_tracts <- as.data.frame(unique(grid_covar_clean$tract_key))
# write_csv(unique_tracts, "unique_tracts.csv")

# filter census block centroids so that it only contains centroids for the tracts in the grid

census_blocks_covar_clean <- census_blocks_covar %>%
  mutate(log_m_to_a1 = log(m_to_a1+1), log_m_to_a2=log(m_to_a2+1), log_m_to_a3=log(m_to_a3+1),
         log_m_to_coast=log(m_to_coast+1), log_m_to_l_airp=log(m_to_l_airp+1), 
         log_m_to_comm=log(m_to_comm+1)) %>% 
  select(-c(m_to_a1,m_to_a2,m_to_a3,m_to_coast,m_to_l_airp,m_to_comm))

```

#ACS data merge: The goal with this merge is to create 1 CSV for us to work with all relevant ACS data for our project 
```{r merge.acs.files}
#--------------Steph & Jorge edits Nov 30
# merge and clean language, race, ethnicity and poverty datasets
# filter out the water census tracts, cleaned up NAs
acs_combined <- reduce(
  list(acs_language_clean, acs_race_clean, acs_ethnicity_clean, acs_poverty),
  function(x, y) merge(x, y, by = c("GEO_ID", "NAME"), all = TRUE)
) %>% filter(Total_Pop_5yrsplus!=0)
colnames(acs_combined)


#NOTE: Variables dropped from this merge: dropped 2ormore column due to complications with code running and to simplify demographics, dropped non-hispanic or latino since it was redundant and we have a "yes" column for hispanic or latino; also removed total_pop_5yearsplus since it is a variable for children over age 5 and did not seem relevant to our questions
acs_simplified <- acs_combined %>%
  select(
    GEO_ID, NAME, TRACTCE, English_only, Language_other_than_English,
    Spanish, Indo_European, Asian_PI, Other, White, Black_AfricanAmerican,
    AIAN, Asian, Hawaiian_PacificIslander, Other_alone, HispanicorLatino,
    Total_Pop, Total_Pop_ME, Percent_Below_Poverty, Percent_Below_Poverty_ME
  ) %>%
  mutate(
    Percent_Below_Poverty = as.numeric(Percent_Below_Poverty),
    Percent_Below_Poverty_ME = as.numeric(Percent_Below_Poverty_ME),
    tract_key = as.numeric(substr(as.character(GEO_ID), 
                                  nchar(as.character(GEO_ID)) - 10, 
                                  nchar(as.character(GEO_ID))))
  ) %>% #fixing incorrect Total_Pop for Pierce County tract 729.06
  mutate(Total_Pop=case_when(NAME=="Census Tract 729.06, Pierce County, Washington" ~ 1995,
                             .default=Total_Pop))

#-----------------------# Data Dictionary for ACS Combined Dataset # 
# GEO_ID : Unique identifier for the geographic area (e.g., census tract, block group) # NAME : Name of the geographic area (e.g., "Census Tract 1, County, State") # TRACTCE : Census tract code, uniquely identifying the geographic unit within a county # # Household Demographics: # White_Households : Number of households with a White population # Black_Households : Number of households with a Black or African American population # AIAN_Households : Number of households with an American Indian or Alaska Native (AIAN) population # Asian_Households : Number of households with an Asian population # Hawaiian_PI_Households : Number of households with a Native Hawaiian or Pacific Islander (Hawaiian_PI) population # Other_alone_Households : Number of households with people from other racial/ethnic groups (alone) # HispanicorLatino_Households : Number of households with a Hispanic or Latino population #
#-------------------------------------------------------Delete chunk below, was a duplicate join
#merge acs_median_income_clean with acs_simplified
# colnames(acs_median_income_clean)
# acs_combined <- acs_simplified %>%
#   left_join(acs_median_income_clean %>%
#               select(GEO_ID, NAME, TRACTCE,
#                      White_Households, Black_Households, AIAN_Households,
#                      Asian_Households, Hawaiian_PI_Households,
#                      Other_alone_Households, HispanicorLatino_Households,
#                      Total_Households_Income, Total_Households_Income_ME,
#                      White_Households_Income, White_Households_Income_ME,
#                      Black_Households_Income, Black_Households_Income_ME,
#                      Asian_Households_Income, Asian_Households_Income_ME,
#                      HispanicorLatino_Households_Income, HispanicorLatino_Households_Income_ME),
#             by = c("GEO_ID", "NAME", "TRACTCE")) %>%
#   mutate(tract_key=as.numeric(substr(as.character(GEO_ID), 
#             nchar(as.character(GEO_ID)) - 10, nchar(as.character(GEO_ID)))))
# colnames(acs_combined)
```

## Variables to include:
Mercer et al vars + airp: D2A1, A1_50, A23_400, Pop_5000, D2C, Int_3000, D2Comm, D2Airp

What we included from grid and mobile covars: m_to_a1, m_to_a2, m_to_a3, pop_s05000, m_to_coast, m_to_l_airp, m_to_comm,lu_industrial_p03000

ACS vars
```{r merge.stop.and.acs.data}
# create stop data with land use vars and ACS (for descriptive tables)
# merge locations on location var variable (stop_clean) and native_id variable (mobile_covar)
# pre-merge stop_clean dim 17533 x 8, mobile_covar_clean dim 311 x 12
# want to make sure we end up with 17533 rows
# removed extraneous geographic/census vars
stop_w_acs <- left_join(stop_clean, mobile_covar_clean, join_by(location==native_id)) %>%
  left_join(acs_simplified, join_by(tract_key==tract_key)) %>%
  select(-c(GEO_ID))
#dim(stop_w_acs) 
# dimensions are correct (17533 rows)

# created stop data for use in regression for prediction
stop_for_preds <- stop_w_acs %>% select(c(time, location, instrument_id, variable,
                                          log_mean,
                                          mean_value, median_value, pop10_s05000,
                                          lu_industrial_p03000, log_m_to_a1, 
                                          log_m_to_a2, log_m_to_a3, log_m_to_coast,
                                          log_m_to_l_airp, log_m_to_comm, longitude, latitude))

# filter stop_for_preds into two different data sets, one for PM2.5 and one for NO2
pm_for_preds <- stop_for_preds %>% filter(variable == "neph_bscat")

no2_for_preds <- stop_for_preds %>% filter(variable == "no2")

annual_w_acs <- left_join(annual_clean, mobile_covar_clean, join_by(location==native_id)) %>%
  left_join(acs_simplified, join_by(tract_key==tract_key)) %>%
  select(-c(GEO_ID))

# created stop data for use in regression for prediction
annual_for_preds <- annual_w_acs %>% 
  mutate(log_value=log(value)) %>%
  select(c(location, variable, log_value, value, pop10_s05000, lu_industrial_p03000,
           log_m_to_a1, log_m_to_a2, log_m_to_a3, log_m_to_coast, log_m_to_l_airp,
           log_m_to_comm, longitude, latitude))

# create grid data with ACS (for predicting onto and answering Q2/3)
# should end with 5040 rows
 
acs_combined<- acs_combined %>% mutate(tract_key=as.numeric(substr(as.character(GEO_ID), 
            nchar(as.character(GEO_ID)) - 10, nchar(as.character(GEO_ID)))))

grid_w_acs <- left_join(grid_covar_clean, acs_combined, join_by(tract_key==tract_key)) %>%
  select(-c(GEO_ID, TRACTCE))

```


# Descriptive statistics


## Descriptive stats for census data

**Table 1** describes the demographic data for the study area by county. 

TO-DO: Add total row - use code from labs

```{r demographics.table}
# Commenting out this code bc of updated code below that includes a Total row. -kt

# acs_simplified %>% 
#   filter(tract_key %in% unique(stop_w_acs$tract_key)) %>% 
#   mutate(County=case_when(str_detect(NAME, "King") ~ "King County",
#                           str_detect(NAME, "Snohomish") ~ "Snohomish County",
#                           str_detect(NAME, "Pierce") ~ "Pierce County")) %>% group_by(County) %>%
#   summarise("Total Pop."=sum(Total_Pop),
#             White=mean(White/Total_Pop)*100,
#             "Black or African-American"=mean(Black_AfricanAmerican/Total_Pop)*100,
#             "American Indian or Alaska Native"=mean(AIAN/Total_Pop)*100,
#             Asian=mean(Asian/Total_Pop)*100,
#             "Hawaiian or Pacific Islander"=mean(Hawaiian_PacificIslander/Total_Pop)*100,
#             "Hispanic or Latino"=mean(HispanicorLatino/Total_Pop)*100,
#             "Percent Below Poverty"=mean(Percent_Below_Poverty),
#             "SD Percent Below Poverty"=sd(Percent_Below_Poverty),
#             "English only"=mean(English_only/Total_Pop)*100,
#             "Language other than English"=mean(Language_other_than_English/Total_Pop)*100,
#             "Spanish"=mean(Spanish/Total_Pop)*100,
#             "Other language"=mean(Other/Total_Pop)*100) %>% 
#   kable(caption="Table 1: Demographics by County for All Census Tracts included in study. All demographics are presented as percents besides total population.", digits=2) %>% kable_styling()


# County-level summary
county_summary <- acs_simplified %>% 
  filter(tract_key %in% unique(stop_w_acs$tract_key)) %>% 
  mutate(County = case_when(
    str_detect(NAME, "King") ~ "King County",
    str_detect(NAME, "Snohomish") ~ "Snohomish County",
    str_detect(NAME, "Pierce") ~ "Pierce County"
  )) %>% 
  group_by(County) %>%
  summarise(
    `Total Pop. (N)` = sum(Total_Pop),
    White = mean(White / Total_Pop) * 100,
    `Black or African-American` = mean(Black_AfricanAmerican / Total_Pop) * 100,
    `American Indian or Alaska Native` = mean(AIAN / Total_Pop) * 100,
    Asian = mean(Asian / Total_Pop) * 100,
    `Hawaiian or Pacific Islander` = mean(Hawaiian_PacificIslander / Total_Pop) * 100,
    `Hispanic or Latino` = mean(HispanicorLatino / Total_Pop) * 100,
    `Percent Below Poverty` = mean(Percent_Below_Poverty),
    `SD Percent Below Poverty` = sd(Percent_Below_Poverty),
    `English only` = mean(English_only / Total_Pop) * 100,
    `Language other than English` = mean(Language_other_than_English / Total_Pop) * 100,
    Spanish = mean(Spanish / Total_Pop) * 100,
    `Other language` = mean(Other / Total_Pop) * 100
  )

# Total row calculation
total_row <- acs_simplified %>%
  filter(tract_key %in% unique(stop_w_acs$tract_key)) %>%
  summarise(
    County = "Total",
    `Total Pop. (N)` = sum(Total_Pop),
    White = mean(White / Total_Pop) * 100,
    `Black or African-American` = mean(Black_AfricanAmerican / Total_Pop) * 100,
    `American Indian or Alaska Native` = mean(AIAN / Total_Pop) * 100,
    Asian = mean(Asian / Total_Pop) * 100,
    `Hawaiian or Pacific Islander` = mean(Hawaiian_PacificIslander / Total_Pop) * 100,
    `Hispanic or Latino` = mean(HispanicorLatino / Total_Pop) * 100,
    `Percent Below Poverty` = mean(Percent_Below_Poverty),
    `SD Percent Below Poverty` = NA, # SD is not meaningful across aggregated data
    `English only` = mean(English_only / Total_Pop) * 100,
    `Language other than English` = mean(Language_other_than_English / Total_Pop) * 100,
    Spanish = mean(Spanish / Total_Pop) * 100,
    `Other language` = mean(Other / Total_Pop) * 100
  )

# Combine county summaries with the total row
table1 <- bind_rows(county_summary, total_row)

# Generate table
table1 %>% 
  kable(
    caption = "Table 1: Demographics by County for All Census Tracts included in study. All demographics are presented as percents besides total population.",
    digits = 2
  ) %>%
  kable_styling()


# Summarize language usage
# do we still need this code? Aren't these values summarized in Table 1 above? -kt
language_usage <- acs_simplified %>%
  filter(tract_key %in% unique(stop_w_acs$tract_key)) %>%
  summarise(
    English_only = sum(English_only, na.rm = TRUE),
    Language_other_than_English = sum(Language_other_than_English, na.rm = TRUE),
    Spanish = sum(Spanish, na.rm = TRUE),
    Other = sum(Other, na.rm = TRUE)
  ) %>%
  pivot_longer(cols = everything(), names_to = "Language", values_to = "Count") %>%
  mutate(Percentage = Count / sum(Count) * 100)
print(language_usage)# NOTE: here is the summary stats for the languages
```

```{r, eval=FALSE}
# This dot plot is misleading and confusing - what does income of 20 mean? The percentages are confusing and this information would be easier to understand in a table.

# Improved dot plot with ggplot2 color schemes
ggplot(dot_plot_data, aes(x = Value, y = Category, color = Type, shape = Type)) +
  geom_point(size = 4) +
  geom_vline(xintercept = seq(0, 60, 10), linetype = "dotted", color = "gray80") +
  labs(
    title = "Cohort Demographics, Language Usage, and Income\n Below Poverty Level",
    x = "Value (Percentage or Metric Value)",
    y = "Category",
    color = "Type",
    shape = "Type"
  ) +
  scale_color_manual(values = c("#F8766D", "#00BA38", "#619CFF")) + # ggplot2-friendly colors
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )


```

```{r, eval=FALSE}
#---------------------- Exploring pollutant data with language

# Load your dataset
stop_w_acs <- read.csv("stop_w_acs.csv")

# Pivot the racial demographic columns to long format
race_columns <- c(
  "White", "Black_AfricanAmerican", "AIAN", "Asian", 
  "Hawaiian_PacificIslander", "HispanicorLatino", "Other_alone"
)

stop_w_acs_race_long <- stop_w_acs %>%
  pivot_longer(
    cols = all_of(race_columns), # Use the racial demographic columns
    names_to = "Race",
    values_to = "Population"
  )

# Create the boxplot with a color-blind-friendly palette
ggplot(stop_w_acs_race_long, aes(x = Race, y = mean_value, fill = variable)) +
  geom_boxplot(outlier.shape = NA, alpha = 0.8) +
  scale_fill_manual(values = c("#D55E00", "#0072B2", "#009E73", "#F0E442")) + # Color-blind-friendly palette
  theme_minimal() +
  labs(
    title = "Air Pollutant Distribution by Race",
    x = "Race",
    y = "Mean Pollutant Value",
    fill = "Pollutant"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )

## This plot doesn't make much sense - it looks like each stop has every Race category, so when you plot the values, they're all the same. I don't think it makes sense to plot every stop by race, because stops don't have a race. (Cecilia)
```

## Descriptive statistics for stop data

```{r desc.stats.stop.data}
# stop_clean %>% group_by(variable) %>% summarise(N=n(),
#                                                 mean=mean(mean_value),
#                                                 sd=sd(mean_value),
#                                                 max=max(mean_value),
#                                                 min(mean_value),
#                                                 median=median(median_value)) %>%
#   kable(caption="Table 2: Descriptive statistics for NO2 and PM2.5 for Study Area", digits=2) %>% 
#   kable_styling()

stop_clean %>% 
  mutate(variable = case_when(
    variable == "neph_bscat" ~ "PM2.5 (ug/m3)",
    variable == "no2" ~ "NO2 (ppb)",
    TRUE ~ variable
  )) %>%
  group_by(variable) %>% 
  summarise(
    N = n(),
    "Arithmetic Mean" = mean(mean_value),
    "SD" = sd(mean_value),
    "Geometric Mean" = exp(mean(log(mean_value), na.rm = TRUE)),
    "Geometric SD" = exp(sd(log(mean_value), na.rm = TRUE)),
    Max = max(mean_value),
    Min = min(mean_value),
    Median = median(median_value)
  ) %>%
  rename(Pollutant = variable) %>% 
  kable(caption = "Table 2: Descriptive statistics for NO2 and PM2.5 for Study Area", digits = 2) %>% 
  kable_styling()

# I tried incorporating log-transformed values as well, but I think this may be unnecessary if we present the geo mean and SD in the above table. Plus, we can follow up Table 2 with Figure 1 of the distributions to visualize the need for log-transformation. -kt

# table1 <- stop_clean %>% 
#   mutate(
#     variable = case_when(
#       variable == "neph_bscat" ~ "PM2.5 (ug/m3)",
#       variable == "no2" ~ "NO2 (ppb)",
#       TRUE ~ variable
#     ),
#     log_variable = paste0("log(", variable, ")"),
#     log_value = log(mean_value) 
#   ) %>%
#   pivot_longer(
#     cols = c(mean_value, log_value),
#     names_to = "value_type",
#     values_to = "value"
#   ) %>%
#   mutate(
#     variable_type = case_when(
#       value_type == "mean_value" ~ variable,
#       value_type == "log_value" ~ log_variable
#     )
#   ) %>%
#   group_by(variable_type) %>% 
#   summarise(
#     N = n(),
#     Mean = mean(value, na.rm = TRUE),
#     SD = sd(value, na.rm = TRUE),
#     Max = max(value, na.rm = TRUE),
#     Min = min(value, na.rm = TRUE),
#     Median = median(value, na.rm = TRUE)
#   ) %>%
#   rename(Pollutant = variable_type) %>% 
#   kable(caption = "Table 2: Descriptive statistics for NO2, PM2.5, and their log-transformed values", digits = 2) %>% 
#   kable_styling()
# 
# table1

```

## Descriptive statistics for annual data

Also very basic at the moment.
```{r desc.stats.annual.data}

annual_clean %>% 
  mutate(variable = case_when(
    variable == "pm2.5_ug_m3" ~ "PM2.5 (ug/m3)",
    variable == "no2" ~ "NO2 (ppb)",
    TRUE ~ variable
  )) %>%
  group_by(variable) %>% 
  summarise(
  #  N = n(),
    "Arithmetic Mean" = mean(value),
    "SD" = sd(value),
    "Geometric Mean" = exp(mean(log(value), na.rm = TRUE)),
    "Geometric SD" = exp(sd(log(value), na.rm = TRUE)),
    Max = max(value),
    Min = min(value),
    Median = median(value)
  ) %>%
  rename(Pollutant = variable) %>% 
  kable(caption = "Table 3: Descriptive statistics for Annual NO2 and PM2.5 Measurements Across Study Area Monitoring Locations (N = 304)", digits = 2) %>% 
  kable_styling(full_width = FALSE, latex_options = c("hold_position")) %>% 
  add_footnote("Note: The mean of winsorized medians was used to summarize site annual averages.", # pulled wording from data dictionary 
               notation = "none", 
               threeparttable = TRUE)

```

## Summary and visualizations of air pollutant data
Comparing the distribution of the mean values of NO2 and PM2.5 on the native and log scales. Based on these histograms, it looks like we should log transform both variables for subsequent analyses. 
```{r var.distributions}
# Commenting out the plots using the median values, I think we should just present the means -kt

# # NO2 plot on the native scale
# no2_native <- ggplot(data = no2_stop_clean, aes(median_value)) +
#     geom_histogram(colour = "black", 
#                    fill = "white") 
# 
# # NO2 plot on the log scale
# no2_log <- ggplot(data = no2_stop_clean, aes(log_median)) +
#     geom_histogram(colour = "black", 
#                    fill = "white") 
# 
# # PM2.5 plot on the native scale
# pm25_native <- ggplot(data = pm25_stop_clean, aes(median_value)) +
#     geom_histogram(colour = "black", 
#                    fill = "white") 
# 
# # PM2.5 plot on the log scale
# pm25_log <- ggplot(data = pm25_stop_clean, aes(log_median)) +
#     geom_histogram(colour = "black", 
#                    fill = "white") 
# 
# # Add plot type and scale information to the datasets
# no2_native_data <- no2_stop_clean %>%
#   mutate(scale = "Native Scale", pollutant = "NO2", value = median_value)
# 
# no2_log_data <- no2_stop_clean %>%
#   mutate(scale = "Log Scale", pollutant = "NO2", value = log_median)
# 
# pm25_native_data <- pm25_stop_clean %>%
#   mutate(scale = "Native Scale", pollutant = "PM2.5", value = median_value)
# 
# pm25_log_data <- pm25_stop_clean %>%
#   mutate(scale = "Log Scale", pollutant = "PM2.5", value = log_median)
# 
# # Combine all datasets
# combined_data <- bind_rows(no2_native_data, no2_log_data, pm25_native_data, pm25_log_data)
# 
# # Adjusting the scale levels to reorder the columns
# combined_data <- combined_data %>%
#   mutate(scale = factor(scale, levels = c("Native Scale", "Log Scale"))) 
# 
# # Create faceted plot
# faceted_histogram <- ggplot(data = combined_data, aes(value)) +
#   geom_histogram(colour = "black", fill = "white", bins = 30) +
#   facet_grid(pollutant ~ scale, scales = "free_x") +
#   labs(x = "Median Values", y = "Frequency", title="Distribution of Median Values",
#        caption="Distribution of native and log-transformed median values of NO2 (above)\n and PM2.5 (below). Both variables are log-normally distributed.") + 
#   theme_minimal() +
#   theme(plot.title = element_text(hjust = 0.5),
#         plot.caption = element_text(hjust = 0.5)) +
#   theme(panel.grid.minor = element_blank()) 
# 
# # Display the plot
# print(faceted_histogram)

#---- Means in case we want means----
no2_native <- ggplot(data = no2_stop_clean, aes(mean_value)) +
    geom_histogram(colour = "black", 
                   fill = "white") 

# NO2 plot on the log scale
no2_log <- ggplot(data = no2_stop_clean, aes(log_mean)) +
    geom_histogram(colour = "black", 
                   fill = "white") 

# PM2.5 plot on the native scale
pm25_native <- ggplot(data = pm25_stop_clean, aes(mean_value)) +
    geom_histogram(colour = "black", 
                   fill = "white") 

# PM2.5 plot on the log scale
pm25_log <- ggplot(data = pm25_stop_clean, aes(log_mean)) +
    geom_histogram(colour = "black", 
                   fill = "white") 

# Add plot type and scale information to the datasets
no2_native_data <- no2_stop_clean %>%
  mutate(scale = "Native Scale", pollutant = "NO2", value = mean_value)

no2_log_data <- no2_stop_clean %>%
  mutate(scale = "Log Scale", pollutant = "NO2", value = log_mean)

pm25_native_data <- pm25_stop_clean %>%
  mutate(scale = "Native Scale", pollutant = "PM2.5", value = mean_value)

pm25_log_data <- pm25_stop_clean %>%
  mutate(scale = "Log Scale", pollutant = "PM2.5", value = log_mean)

# Combine all datasets
combined_data <- bind_rows(no2_native_data, no2_log_data, pm25_native_data, pm25_log_data)

# Adjusting the scale levels to reorder the columns
combined_data <- combined_data %>%
  mutate(scale = factor(scale, levels = c("Native Scale", "Log Scale"))) 

# Create faceted plot
faceted_histogram <- ggplot(data = combined_data, aes(value)) +
  geom_histogram(colour = "black", fill = "white", bins = 30) +
  facet_grid(pollutant ~ scale, scales = "free_x") +
  labs(x = "Mean Values", y = "Frequency", title="Figure 1. Distribution of Mean Values",
       caption="Distribution of native and log-transformed mean values of NO2 (above)\n and PM2.5 (below). Both variables are log-normally distributed.") + 
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0.5)) +
  theme(panel.grid.minor = element_blank()) 

# Display the plot
print(faceted_histogram)
```


# Prediction model components

## Variograms

These work best for the annual data, so we are using the annual data to make our predictions.

```{r set.up.sf.data}

latlong_proj <- 4326  

lambert_proj <- "+proj=lcc +lat_1=33 +lat_2=45 +lat_0=39 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs"

#transform stop data to Lambert for distances 
stop_for_preds <- st_as_sf(stop_for_preds, coords = c('longitude', 'latitude'), crs = 4326) %>%
  # # convert to a meters CRS
  st_transform(lambert_proj)

annual_for_preds <- st_as_sf(annual_for_preds, coords = c('longitude', 'latitude'), crs = 4326) %>%
  # # convert to a meters CRS
  st_transform(lambert_proj)

# Convert grid to sf
grid_covar_clean <- grid_covar_clean %>% 
  st_as_sf(coords = c('longitude', 'latitude'), crs = latlong_proj)

# Download the land polygon data as an sf multipolygon
# the CRS for this is in lat/long degrees
land <- ne_download(scale = "large", type = "land", category = "physical", returnclass = "sf")

# Crop the land area to the bounding box of the LA grid to reduce processing time
# have to convert the la_grid to the same degrees as LA
land <- suppressWarnings(st_crop(land, st_bbox(grid_covar_clean))) 

# Visualize cropped land area
ggplot(land) + geom_sf()

# Filter la_grid to keep only points that intersect with land
grid_covar_clean <- grid_covar_clean[st_within(grid_covar_clean, land) %>% lengths() > 0,]

# Visualize grid land locations (zoom in)
ggplot(grid_covar_clean) + geom_sf(size=0.001)

```

```{r study.area.map.setup}
# ---- Study Area Map Setup ----
# Define a bounding box (min & max X and Y) with a 10,000m buffer around `grid_covar_clean`
map_bbox <- grid_covar_clean %>%
  # convert from degrees to meters
  st_transform(crs = lambert_proj) %>%
  # add a buffer around the area for visualization purposes
  st_buffer(dist = 10000) %>%
  # convert back to original CRS
  st_transform(crs = latlong_proj) %>%
  # take the min/max X/Y
  st_bbox()

map_bbox

# Base map setup with ggplot2 and ggspatial using OSM tiles
g <- ggplot() +
  ggspatial::annotation_map_tile(type = "osm", zoom = 10, ) +
  labs(title = "Study Area Grid with Map") +
  theme_minimal()

# Plot background map and the LA grid (zoom in)
g + 
  geom_sf(data = grid_covar_clean, size=0.001)

# Add NO2 data with additional map elements
g + 
  geom_sf(data = stop_for_preds %>% filter(variable == "no2"), aes(color = mean_value)) + 
  scale_color_viridis_c() + 
  theme_void() +  
  theme(
    panel.border = element_rect(color = "black", fill = NA, size = 1)
  ) + 
  ggspatial::annotation_scale(location = "bl", width_hint = 0.3, unit_category = "imperial") +
  ggspatial::annotation_north_arrow(location = "tr", which_north = "true", style = north_arrow_fancy_orienteering) +  
  labs(title = "Map of Study Area with\n Annual Average\n NO2 concentrations",
       col="NO2 (ppb)"
       )

# Add PM2.5 data with additional map elements
g + 
  geom_sf(data = stop_for_preds %>% filter(variable == "neph_bscat"), aes(color = mean_value)) + 
  scale_color_viridis_c() + 
  theme_void() +  
  theme(
    panel.border = element_rect(color = "black", fill = NA, size = 1)
  ) + 
  ggspatial::annotation_scale(location = "bl", width_hint = 0.3, unit_category = "imperial") +
  ggspatial::annotation_north_arrow(location = "tr", which_north = "true", style = north_arrow_fancy_orienteering) +  
  labs(title = "Map of Study Area with\n Annual Average\n PM2.5 concentrations",
       col="PM2.5 (ug/m3)"
       )
```

```{r}
# Convert to a meter CRS for distance calculations
stop_for_preds <- st_transform(stop_for_preds, crs = lambert_proj) 

annual_for_preds <- st_transform(annual_for_preds, crs = lambert_proj) 
```

Note: log_mean ~ 1 is Ordinary Kriging -- since we are doing Universal Kriging we need to include our covariates in the formulas.
```{r no2.variograms}
# ---- Empirical Variogram Plots ----

# Variogram Cloud with stop_for_preds --> 24956646 observations
#vgm_no2_stop <- variogram(log_mean ~ pop10_s05000 + lu_industrial_p03000 + log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm, stop_for_preds %>% filter(variable == "no2"), cloud = TRUE)

# Variogram Cloud with annual_for_preds --> 32135
vgm_no2_annual <- variogram(log_value ~ pop10_s05000 + lu_industrial_p03000 + log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm, stop_for_preds, annual_for_preds %>% filter(variable == "no2"), cloud = TRUE)

# RUNS FOREVER --> too many observations due to repeated measures?
# ggplot(vgm_no2_stop, aes(x = dist, y=gamma)) + 
#    geom_point(alpha=0.1) + 
#    labs(x="Distance (meters)")

# pretty
ggplot(vgm_no2_annual, aes(x = dist, y=gamma)) + 
  geom_point(alpha=0.1) + 
  labs(x="Distance (meters)")

# Binned Variogram with stop data --> ugly
#plot(variogram(log_mean ~ pop10_s05000 + lu_industrial_p03000 + log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm, stop_for_preds %>% filter(variable == "no2")))

# Binned Variogram with annual data --> prettier!
plot(variogram(log_value ~ pop10_s05000 + lu_industrial_p03000 + log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm, annual_for_preds %>% filter(variable == "no2")))

# Binned Variogram with Point Counts
plot(variogram(log_value ~ pop10_s05000 + lu_industrial_p03000 + log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm, annual_for_preds %>% filter(variable == "no2")), pl = TRUE)

# Smoothed Cloud Variogram using ggplot2
ggplot(data = vgm_no2_annual, aes(x = dist, y = gamma)) +
  geom_point(shape = 1, alpha = 0.6) +
  geom_smooth(se = FALSE, color = "red", linetype = "solid",
              # making span > 0.75 (default) makes this less wiggly so we can better see the general trend 
              method = "loess", 
              span = .8
              ) +
  labs(x = "Distance (meters)", 
       y = "Semi-variance",
       title = "Semi-variogram Cloud with Smoothed Curve") +
  theme_bw() +
  theme(legend.position = "none")

```

```{r}
# Estimate the empirical variogram
##  By default, the variogram() function limits the maximum lag distance. Increasing the cutoff parameter will allow it to calculate semivariance values over a larger distance, which might help the semivariogram level off if it's naturally reaching a sill
v_stop <- variogram(log_mean ~ pop10_s05000 + lu_industrial_p03000 + log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm, data=stop_for_preds %>% filter(variable == "no2"))
v_annual <- variogram(log_value ~ pop10_s05000 + lu_industrial_p03000 + log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm, data=annual_for_preds %>% filter(variable == "no2"))
# as before:
plot(v_stop)
plot(v_annual)

# Fit a model to the variogram, trying exponential, spherical, and Matern options
v_stop.fit <- fit.variogram(v_stop, vgm(c("Exp", "Sph", "Mat")))
v_annual.fit <- fit.variogram(v_annual, vgm(c("Exp", "Sph", "Mat")))


# Display the selected variogram model and its parameters (sill, range, nugget)
# Note: what model is selected? Spherical for both
v_stop.fit
v_annual.fit

# Plot the empirical variogram with the fitted model overlaid
# consider expanding the x-axis here
plot(v_stop, v_stop.fit)
plot(v_annual, v_annual.fit)

```

```{r fitting variogram for universal kriging no2}
# Estimate the variogram with a covariate predictor
# 100000 is way too high a cutoff
v_stop.uk <- variogram(log_mean ~ pop10_s05000 + lu_industrial_p03000 + log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm, data=(stop_for_preds %>% filter(variable=="no2")))

v_annual.uk <- variogram(log_value ~ log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm + pop10_s05000 + lu_industrial_p03000, data=annual_for_preds %>% filter(variable=="no2"))
plot(v_stop.uk)
plot(v_annual.uk)


# Fit the variogram model with multiple options (Exponential, Spherical, Matern)
# note that this may have convergence issues. If so, you can try selecting one variogram model instead. You can also give it initial values for range, nugget etc. based on looking at the variogram cloud.

# sometimes has convergence issues 
m_stop.uk <- fit.variogram(v_stop.uk, vgm(c("Exp", "Sph", "Mat")))
m_annual.uk <- fit.variogram(v_annual.uk, vgm(c("Exp", "Sph", "Mat")))

# Alternatively, you could fit the variogram with modified initial values. This still has convergence issues.
# m.uk <- fit.variogram(v.uk, vgm("Exp", nugget = 0.01, psill = 0.03, range = 15000))


# Display the selected variogram model parameters
m_stop.uk
m_annual.uk

# Plot the empirical variogram with the fitted model
plot(v_stop.uk, model = m_stop.uk)
plot(v_annual.uk, model = m_annual.uk)

```

## Predict with Universal Kriging 
This code was taken from the Week 6 Lab, and we need to predict on the grid level (for mapping) and the block level (for testing associations, as the grid can underpredict). We can try just mapping with the block level too.

```{r check.for.linearity}
# We could consider checking for linearity for inclusion of additional variables from Mercer et al 2011

# CODE TO EDIT FROM LAB 6: Plot to evaluate linearity between X and X
ggplot(stop_for_preds %>% filter(variable == "neph_bscat"), aes(x = pop10_s05000, y = log_mean)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  geom_smooth() +
  labs(x = "Population within 5000 meters", y = "Log-transformed PM2.5", 
       title = "Relationship between Pop_5000 and log(PM2.5)")

ggplot(stop_for_preds %>% filter(variable == "no2"), aes(x = pop10_s05000, y = log_mean)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  geom_smooth() +
  labs(x = "Population within 5000 meters", y = "Log-transformed NO2", 
       title = "Relationship between Pop_5000 and log(NO2)")

```

```{r universal.kriging.prediction.no2.grid.annual, warning=FALSE}
#transform stop data to Lambert for distances 
grid_for_preds <- st_as_sf(grid_covar_clean, coords = c('longitude', 'latitude'), crs = 4326) %>%
  # # convert to a meters CRS
  st_transform(lambert_proj)

# ---- Universal Kriging Prediction Code from Lab 6 ----

# Fit the universal kriging model for NO2 annual data and predict on the grid 
ln.no2.kr <- krige(log_value ~ log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm + pop10_s05000 + lu_industrial_p03000, 
                 annual_for_preds %>% filter(variable=="no2"), # CRS in meters
                 newdata = grid_for_preds, # CRS in meters
                 model = m_annual.uk)

# Calculate standard errors to assess prediction uncertainty across locations
ln.no2.kr$se <- sqrt(ln.no2.kr$var1.var)  # Standard error is the square root of variance

# Plot UK predictions 
pl3 <- plot(ln.no2.kr["var1.pred"], main = "UK Prediction of Log(NO2)")

# Plot UK prediction standard errors 
pl4 <- plot(ln.no2.kr["se"], main = "UK Prediction Error NO2 (Standard Error)")

```

Tried to let universal kriging on the stop data run for 30 min, but got nothing - let's chalk this up to limited computational power.

```{r universal.kriging.prediction.no2.BLOCK, warning=FALSE}
#transform stop data to Lambert for distances 
blocks_for_preds <- st_as_sf(census_blocks_covar_clean, coords = c('longitude', 'latitude'), crs = 4326) %>%
  # # convert to a meters CRS
  st_transform(lambert_proj)

# ---- Universal Kriging Prediction Code from Lab 6 ----

# Fit the universal kriging model for PM2.5 and predict on the grid
ln.NO2.block.kr <- krige(log_value ~ log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm + pop10_s05000 + lu_industrial_p03000, 
                 annual_for_preds %>% filter(variable=="no2"), # CRS in meters
                 newdata = blocks_for_preds, # CRS in meters
                 model = m_annual.uk)

# Calculate standard errors to assess prediction uncertainty across locations
ln.NO2.block.kr$se <- sqrt(ln.NO2.block.kr$var1.var)  # Standard error is the square root of variance

# Plot UK predictions
pl3 <- plot(ln.NO2.block.kr["var1.pred"], main = "UK Prediction of Log(PM2.5)")

# Plot UK prediction standard errors
pl4 <- plot(ln.NO2.block.kr["se"], main = "UK Prediction Error PM2.5 (Standard Error)")

```

```{r generate.pm25.variogram}
#Generate variograms for pm25

# Estimate the variogram with a covariate predictor
v.uk.pm25 <- variogram(log_value ~ log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm + pop10_s05000 + lu_industrial_p03000, data=annual_for_preds %>% filter(variable=="pm2.5_ug_m3"))
                  #,cutoff=5000)

plot(v.uk.pm25)

# Fit the variogram model with multiple options (Exponential, Spherical, Matern)
# note that this may have convergence issues. If so, you can try selecting one variogram model instead. You can also give it initial values for range, nugget etc. based on looking at the variogram cloud.

# sometimes has convergence issues 
m.uk.pm25 <- fit.variogram(v.uk.pm25, vgm(c("Exp", "Sph", "Mat")))

# Alternatively, you could fit the variogram with modified initial values. This still has convergence issues.
# m.uk <- fit.variogram(v.uk, vgm("Exp", nugget = 0.01, psill = 0.03, range = 15000))

# Display the selected variogram model parameters
m.uk.pm25

# Plot the empirical variogram with the fitted model
plot(v.uk.pm25, model = m.uk.pm25)
```

```{r universal.kriging.prediction.pm25.grid, warning=FALSE}
# ---- Universal Kriging Prediction Code from Lab 6 ----

# Fit the universal kriging model for PM2.5 and predict on the grid
ln.PM25.kr <- krige(log_value ~ log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm + pop10_s05000 + lu_industrial_p03000, 
                 annual_for_preds %>% filter(variable=="pm2.5_ug_m3"), # CRS in meters
                 newdata = grid_for_preds, # CRS in meters
                 model = m.uk.pm25)

# Calculate standard errors to assess prediction uncertainty across locations
ln.PM25.kr$se <- sqrt(ln.PM25.kr$var1.var)  # Standard error is the square root of variance

# Plot UK predictions
pl3 <- plot(ln.PM25.kr["var1.pred"], main = "UK Prediction of Log(PM2.5)")

# Plot UK prediction standard errors
pl4 <- plot(ln.PM25.kr["se"], main = "UK Prediction Error PM2.5 (Standard Error)")

```

```{r universal.kriging.prediction.pm25.BLOCK, warning=FALSE}
# ---- Universal Kriging Prediction Code from Lab 6 ----

# Fit the universal kriging model for PM2.5 and predict on the grid
ln.PM25.kr <- krige(log_value ~ log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm + pop10_s05000 + lu_industrial_p03000, 
                 annual_for_preds %>% filter(variable=="pm2.5_ug_m3"), # CRS in meters
                 newdata = blocks_for_preds, # CRS in meters
                 model = m.uk.pm25)

# Calculate standard errors to assess prediction uncertainty across locations
ln.PM25.kr$se <- sqrt(ln.PM25.kr$var1.var)  # Standard error is the square root of variance

# Plot UK predictions
pl3 <- plot(ln.PM25.kr["var1.pred"], main = "UK Prediction of Log(PM2.5)")

# Plot UK prediction standard errors
pl4 <- plot(ln.PM25.kr["se"], main = "UK Prediction Error PM2.5 (Standard Error)")

```

## Cross-validation of NO2 and PM2.5 universal kriging models

```{r define CV functions}
# ---- Define Cross-Validation Functions ----

# Wrapper function krige.cv2() to retain the projection of the sf object.
# This fixes a known bug in krige.cv() where projection information is lost.
# (Bug reported and fixed on GitHub, but this wrapper may be required for now.)
krige.cv2 <- function(formula, locations, model = NULL, ..., beta = NULL, 
                      nmax = Inf, nmin = 0, maxdist = Inf, 
                      nfold = nrow(locations),  # default is leave-one-out
                      verbose = interactive(), 
                      debug.level = 0) {
  
  # Perform cross-validation and retain projection if it's missing
  krige.cv1 <- krige.cv(formula = formula, locations = locations, model = model, ..., 
                        beta = beta, nmax = nmax, nmin = nmin, maxdist = maxdist, 
                        nfold = nfold, verbose = verbose, debug.level = debug.level)
  
  # Set projection from input data if krige.cv output lacks it
  if (is.na(st_crs(krige.cv1))) {
    st_crs(krige.cv1) <- st_crs(locations)
  }
  return(krige.cv1)
}

# Function to create a bubble plot for kriging residuals
krige.cv.bubble <- function(cv.out, plot_title) {
  ggplot(data = cv.out) +
    geom_sf(aes(size = abs(residual), color = factor(residual > 0)), alpha = 0.5) +
    scale_color_discrete(name = 'Residual > 0', direction = -1) +
    scale_size_continuous(name = '|Residual|') +
    ggtitle(plot_title) +
    theme_bw()
}

# Function to calculate performance metrics: RMSE and R²
krige.cv.stats <- function(krige.cv.output, description) {
  d <- krige.cv.output
  
  # Calculate Mean Squared Error (MSE) and R²
  mean_observed <- mean(d$observed)
  MSE_pred <- mean((d$observed - d$var1.pred)^2)
  MSE_obs <- mean((d$observed - mean_observed)^2)
  
  # Create a summary table with rounded RMSE and MSE-based R²
  tibble(
    Description = description, 
    RMSE = round(sqrt(MSE_pred), 4), 
    MSE_based_R2 = round(max(1 - MSE_pred / MSE_obs, 0), 4)
  )
}

```

```{r cross-validation-no2, warning=FALSE}
# ---- Cross-Validation for log(no2)----

# Perform Universal Kriging (UK) with 5-fold Cross-Validation
cv5uk_no2 <- krige.cv2(log_value ~ log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm + pop10_s05000 + lu_industrial_p03000, 
                 annual_for_preds %>% filter(variable=="no2"), 
                 model = m_annual.uk,
                 nfold = 5)

# Perform Universal Kriging (UK) with Leave-One-Out Cross-Validation (LOOCV)
cvLOOuk_no2 <- krige.cv2(log_value ~ log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm + pop10_s05000 + lu_industrial_p03000, 
                 annual_for_preds %>% filter(variable=="no2"), 
                 model = m_annual.uk
                     )

# Calculate and compare performance statistics across cross-validation methods
# Compile results into a summary table
bind_rows(
  krige.cv.stats(cv5uk_no2, "UK: 5-Fold CV NO2"),
  krige.cv.stats(cvLOOuk_no2, "UK: LOO CV NO2")
) %>% 
  kable(caption = "Summary of Kriging Cross-Validation Results for log(NO2)") %>% kable_styling()

```

```{r cross-validation-pm25, warning=FALSE}
# ---- Cross-Validation for log(PM2.5)----

# Perform Universal Kriging (UK) with 5-fold Cross-Validation
cv5uk_pm25 <- krige.cv2(log_value ~ log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm + pop10_s05000 + lu_industrial_p03000, 
                 annual_for_preds %>% filter(variable=="pm2.5_ug_m3"), 
                 model = m.uk.pm25,
                 nfold = 5)

# Perform Universal Kriging (UK) with Leave-One-Out Cross-Validation (LOOCV)
cvLOOuk_pm25 <- krige.cv2(log_value ~ log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + log_m_to_coast + log_m_to_l_airp + log_m_to_comm + pop10_s05000 + lu_industrial_p03000, 
                 annual_for_preds %>% filter(variable=="pm2.5_ug_m3"), 
                 model = m.uk.pm25
                     )

# Calculate and compare performance statistics across cross-validation methods
# Compile results into a summary table
bind_rows(
  krige.cv.stats(cv5uk_pm25, "UK: 5-Fold CV PM2.5"),
  krige.cv.stats(cvLOOuk_pm25, "UK: LOO CV PM2.5")
) %>% 
  kable(caption = "Summary of Kriging Cross-Validation Results for log(PM2.5)") %>% kable_styling()

```

### Mapping predicted pollutants on native scale
```{r map.native.scale}

# exponentiate log vars
ln.no2.kr$var1.pred.native <- exp(ln.no2.kr$var1.pred)
ln.PM25.kr$var1.pred.native <- exp(ln.PM25.kr$var1.pred)

# re-project back to long-lat proj
blocks_for_preds <- st_transform(blocks_for_preds, latlong_proj)
ln.no2.kr <- st_transform(ln.no2.kr, latlong_proj)
ln.PM25.kr <- st_transform(ln.PM25.kr, latlong_proj)

# check that coordinates are nearly identical
all.equal(st_coordinates(blocks_for_preds), st_coordinates(ln.no2.kr))
all.equal(st_coordinates(blocks_for_preds), st_coordinates(ln.PM25.kr))

# Join grid to predictions; do so by nearest feature to avoid precision merging issues
new_blocks_no2 <- st_join(blocks_for_preds, ln.no2.kr, join = st_nearest_feature) %>% 
  rename(`NO2 (ppb)` = var1.pred.native, 
         `ln(NO2) ln(ppb)`=var1.pred,
         `ln(NO2) var`=var1.var,
         `ln(NO2) se`=se)
new_blocks_pm25 <- st_join(blocks_for_preds, ln.PM25.kr, join = st_nearest_feature) %>% 
  rename(`PM2.5 (ug/m^3)` = var1.pred.native, 
         `ln(PM2.5) ln(ug/m^3)`=var1.pred,
         `ln(PM2.5) var`=var1.var,
         `ln(PM2.5) se`=se)

# Verify the join was successful (no NAs in predictions)
all(!is.na(new_blocks_pm25$`ln(PM2.5) ln(ug/m^3)`)) 
all(!is.na(new_blocks_no2$`ln(NO2) ln(ppb)`)) 
#yay!
```

```{r plot.points}
#NO2 - native scale
ggplot() + ggspatial::annotation_map_tile(type = "osm", zoom = 10, ) + 
  theme_minimal() +
  geom_sf(data = new_blocks_no2, aes(color = `NO2 (ppb)`), alpha = 0.05) +
  # color friendly color scale
  scale_color_viridis_c(option = "plasma") + 
  ggtitle("Map of Seattle Area with UK NO2 Predictions Overlaid as Points") +
  ggspatial::annotation_scale(location = "bl", width_hint = 0.3, unit_category = "imperial") +  # Scale in miles
  ggspatial::annotation_north_arrow(location = "tr", which_north = "true", style = north_arrow_fancy_orienteering)

#PM2.5 - native scale
ggplot() + ggspatial::annotation_map_tile(type = "osm", zoom = 10, ) + 
  theme_minimal() +
  geom_sf(data = new_blocks_pm25, aes(color = `PM2.5 (ug/m^3)`), alpha = 0.05) +
  # color friendly color scale
  scale_color_viridis_c(option = "plasma") + 
  ggtitle("Map of Seattle Area with UK PM2.5 Predictions Overlaid as Points") + ggspatial::annotation_scale(location = "bl", width_hint = 0.3, unit_category = "imperial") +  # Scale in miles
  ggspatial::annotation_north_arrow(location = "tr", which_north = "true", style = north_arrow_fancy_orienteering)
```

```{r merge.newblocks.acs}
# had to average the predictions over each tract based on the predicted blocks
predicted_to_tracts_no2 <- new_blocks_no2 %>% group_by(tract_key) %>%
  summarise("ln(NO2) ln(ppb)"=mean(`ln(NO2) ln(ppb)`),
            "ln(NO2) var" = mean(`ln(NO2) var`), "ln(NO2) se" = mean(`ln(NO2) se`),
            "NO2 (ppb)" = mean(`NO2 (ppb)`))

predicted_to_tracts_pm25 <- new_blocks_pm25 %>% group_by(tract_key) %>%
  summarise("ln(PM2.5) ln(ug/m^3)"=mean(`ln(PM2.5) ln(ug/m^3)`),
            "ln(PM2.5) var" = mean(`ln(PM2.5) var`), "ln(PM2.5) se" = mean(`ln(PM2.5) se`),
            "PM2.5 (ug/m^3)" = mean(`PM2.5 (ug/m^3)`))

full_data_w_preds <- acs_simplified %>% left_join(predicted_to_tracts_no2, join_by(tract_key==tract_key)) %>% left_join(predicted_to_tracts_pm25, join_by(tract_key==tract_key)) %>% select(-c(geometry.x, geometry.y)) %>% 
  filter(!is.na(`ln(NO2) ln(ppb)`) | !is.na(`ln(PM2.5) ln(ug/m^3)`))
```

# Important note about the full data

**IMPORTANT**: FULL DATA containing the predicted NO2, predicted PM2.5, and ACS variables is located in the dataframe called "full_data_w_preds". This is the dataframe we should use to do our association tests. If there are variables in other acs sets you can sub that in in the line of code above as long as they still have "tract_key" -- otherwise you'll need to do a little more.

# Research Question 3: Association between Pollutant Concentrations & Demographics

```{r}
# Regression between annual PM2.5 concentrations and various demographics

# Created new columns for percent of population for each race (same as those included in demographic statistics summary table (except for "Other Langugage"))

full_data_w_preds_perc <- full_data_w_preds %>% 
    mutate(
           p_White = (White / Total_Pop)*100,
           p_BlackAA = (Black_AfricanAmerican/ Total_Pop)*100,
           p_AIAN = (AIAN / Total_Pop)*100,
           p_Asian = (Asian / Total_Pop)*100,
           p_HPI = (Hawaiian_PacificIslander / Total_Pop)*100,
           p_Hispanic = (HispanicorLatino / Total_Pop)*100,
           p_engl_only = (English_only / Total_Pop)*100,
           p_lang_other = (Language_other_than_English / Total_Pop)*100,
           p_spanish = (Spanish / Total_Pop)*100
           )


## I think all of these need to be edited (the predictor and response variables are switched) -kt
#PM2.5 x poverty
pm_poverty <- lm(Percent_Below_Poverty ~ `PM2.5 (ug/m^3)`, 
                 data = full_data_w_preds)
           
# PM2.5 x White
pm_white <- lm(p_White ~ `PM2.5 (ug/m^3)`, data = full_data_w_preds_perc)

# PM2.5 x Black African American
pm_BlackAA <- lm(p_BlackAA ~ `PM2.5 (ug/m^3)`, data = full_data_w_preds_perc)

# PM2.5 x Black African American
pm_AIAN <- lm(p_AIAN ~ `PM2.5 (ug/m^3)`, data = full_data_w_preds_perc)

# PM2.5 x Black African American
pm_Asian <- lm(p_Asian ~ `PM2.5 (ug/m^3)`, data = full_data_w_preds_perc)

# PM2.5 x Black African American
pm_HPI <- lm(p_HPI ~ `PM2.5 (ug/m^3)`, data = full_data_w_preds_perc)

# PM2.5 x Black African American
pm_Hispanic <- lm(p_Hispanic ~ `PM2.5 (ug/m^3)`, data = full_data_w_preds_perc)

# PM2.5 x Black African American
pm_engl_only <- lm(p_engl_only ~ `PM2.5 (ug/m^3)`, data = full_data_w_preds_perc)

pm_lang_other <- lm(p_lang_other ~ `PM2.5 (ug/m^3)`, data = full_data_w_preds_perc)

pm_spanish <- lm(p_spanish ~ `PM2.5 (ug/m^3)`, data = full_data_w_preds_perc)


# Multiple linear regression model for PM2.5
pm_full <- lm(`PM2.5 (ug/m^3)` ~ Percent_Below_Poverty + p_engl_only + p_lang_other + p_spanish + p_White + p_BlackAA + p_AIAN + p_Asian + p_HPI + p_Hispanic, data = full_data_w_preds_perc)

# Extract regression results for PM2.5 model
pm_full_results <- tidy(pm_full)

# Formatted table for regression results
pm_full_results %>%
  select(term, estimate, std.error, statistic, p.value) %>% 
  mutate(
    Term = case_when(
      term == "(Intercept)" ~ "Intercept",
      term == "Percent_Below_Poverty" ~ "% Below Poverty",
      term == "p_engl_only" ~ "% English Language Only",
      term == "p_lang_other" ~ "% Other Language",
      term == "p_spanish" ~ "% Spanish Language",
      term == "p_White" ~ "% White",
      term == "p_BlackAA" ~ "% Black or African American",
      term == "p_AIAN" ~ "% American Indian or Alaska Native",
      term == "p_Asian" ~ "% Asian",
      term == "p_HPI" ~ "% Hawaiian or Pacific Islander",
      term == "p_Hispanic" ~ "% Hispanic or Latino",
      TRUE ~ term 
    )
  ) %>%
  select(Term, estimate, std.error, statistic, p.value) %>%
  rename(
    Estimate = estimate,
    `Std. Error` = std.error,
    `t-value` = statistic,
    `P-value` = p.value
  ) %>% 
  kable(
    caption = "Table: Regression for Association Results for PM2.5 Model",
    digits = 3, 
    col.names = c("Term", "Estimate", "Std. Error", "t-value", "P-value")
  ) %>%
  kable_styling(
    full_width = FALSE, 
    bootstrap_options = c("striped", "hover", "condensed")
  )

```

```{r}
## Again, I think all of these need to be edited (the predictor and response variables are switched) -kt
# Associations between annual NO2 concentrations and various demographics

# NO2 x poverty
no2_poverty <- lm(Percent_Below_Poverty ~ `NO2 (ppb)`, 
                 data = full_data_w_preds)
           
# PM2.5 x White
no2_white <- lm(p_White ~ `NO2 (ppb)`, data = full_data_w_preds_perc)

# PM2.5 x Black African American
no2_BlackAA <- lm(p_BlackAA ~ `NO2 (ppb)`, data = full_data_w_preds_perc)

# PM2.5 x Black African American
no2_AIAN <- lm(p_AIAN ~ `NO2 (ppb)`, data = full_data_w_preds_perc)

# PM2.5 x Black African American
no2_Asian <- lm(p_Asian ~ `NO2 (ppb)`, data = full_data_w_preds_perc)

# PM2.5 x Black African American
no2_HPI <- lm(p_HPI ~ `NO2 (ppb)`, data = full_data_w_preds_perc)

# PM2.5 x Black African American
no2_Hispanic <- lm(p_Hispanic ~ `NO2 (ppb)`, data = full_data_w_preds_perc)

# PM2.5 x Black African American
no2_engl_only <- lm(p_engl_only ~ `NO2 (ppb)`, data = full_data_w_preds_perc)

no2_lang_other <- lm(p_lang_other ~ `NO2 (ppb)`, data = full_data_w_preds_perc)

no2_spanish <- lm(p_spanish ~ `NO2 (ppb)`, data = full_data_w_preds_perc)


# Multiple linear regression model for NO2
no2_full <- lm(`NO2 (ppb)` ~ Percent_Below_Poverty + p_engl_only + p_lang_other + p_spanish + p_White + p_BlackAA + p_AIAN + p_Asian + p_HPI + p_Hispanic, data = full_data_w_preds_perc)

# Extract regression results for NO2 model
no2_full_results <- tidy(no2_full)

# Formatted table for regression results
no2_full_results %>%
  select(term, estimate, std.error, statistic, p.value) %>% 
  mutate(
    Term = case_when(
      term == "(Intercept)" ~ "Intercept",
      term == "Percent_Below_Poverty" ~ "% Below Poverty",
      term == "p_engl_only" ~ "% English Language Only",
      term == "p_lang_other" ~ "% Other Language",
      term == "p_spanish" ~ "% Spanish Language",
      term == "p_White" ~ "% White",
      term == "p_BlackAA" ~ "% Black or African American",
      term == "p_AIAN" ~ "% American Indian or Alaska Native",
      term == "p_Asian" ~ "% Asian",
      term == "p_HPI" ~ "% Hawaiian or Pacific Islander",
      term == "p_Hispanic" ~ "% Hispanic or Latino",
      TRUE ~ term 
    )
  ) %>%
  select(Term, estimate, std.error, statistic, p.value) %>%
  rename(
    Estimate = estimate,
    `Std. Error` = std.error,
    `t-value` = statistic,
    `P-value` = p.value
  ) %>% 
  kable(
    caption = "Table: Regression for Association Results for NO2 Model",
    digits = 3, 
    col.names = c("Term", "Estimate", "Std. Error", "t-value", "P-value")
  ) %>%
  kable_styling(
    full_width = FALSE, 
    bootstrap_options = c("striped", "hover", "condensed")
  )


```

# Choropleth visualization of predicted pollutants with demographic variables

```{r pm25.choropleth.map}
## The below code to merge the full dataset with the census geometries is kinda trash. Would love your expertise in cleaning it up Cecilia :-D

# Ensure `census_clean` is an sf object
census_clean <- st_as_sf(census_clean)

# Select only the `AFFGEOID` and `geometry` columns
census_geometry <- census_clean %>%
  select(AFFGEOID, geometry) %>% 
  rename(GEO_ID = AFFGEOID)

# Drop geometry temporarily
census_geometry_nonspatial <- st_drop_geometry(census_geometry)

# Perform join
full_data_w_preds_perc_spatial <- full_data_w_preds_perc %>%
  left_join(census_geometry_nonspatial, by = "GEO_ID") 

# Reattach geometry to the joined data
full_data_w_preds_perc_spatial <- st_as_sf(
  left_join(census_clean, full_data_w_preds_perc_spatial, by = c("AFFGEOID" = "GEO_ID"))
) %>% 
  filter(!is.na(NAME.y))


# generating plots for significant predictors from regression results

# PM2.5 and % Below Poverty Map

# Classify data for bivariate mapping
pm_poverty_bivariate <- bi_class(
  full_data_w_preds_perc_spatial,
  x = `PM2.5 (ug/m^3)`, 
  y = Percent_Below_Poverty, 
  style = "quantile", 
  dim = 3 
)

# Define the bounding box for the study area
map_bbox <- st_bbox(pm_poverty_bivariate)

# Generate the bivariate map
pm_poverty_map <- ggplot() +
  ggspatial::annotation_map_tile(type = "osm", zoom = 10) +  
  geom_sf(data = pm_poverty_bivariate, aes(fill = bi_class), color = NA) +
  bi_scale_fill(pal = "PurpleGrn", dim = 3, name = NULL) +
  labs(
    title = "% Below Poverty"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(size = 10),
    legend.position = "none"
  )

# Create legend as a matrix of colors
# legend <- bi_legend(
#   pal = "PurpleGrn",
#   dim = 3,
#   xlab = "Higher PM2.5",
#   ylab = "Higher Poverty",
#   size = 12
# )

# Combine the map and the legend
# pm_poverty_map <- pm_poverty_map + inset_element(
#   legend,
#   left = 1.05,
#   bottom = 0.1,
#   right = 1.35,
#   top = 0.4     
# )


# PM2.5 and % English Language Only Map

# Classify data for bivariate mapping
pm_english_bivariate <- bi_class(
  full_data_w_preds_perc_spatial,
  x = `PM2.5 (ug/m^3)`, 
  y = 'p_engl_only', 
  style = "quantile", 
  dim = 3 
)

# Generate the bivariate map
pm_english_map <- ggplot() +
  ggspatial::annotation_map_tile(type = "osm", zoom = 10) +  
  geom_sf(data = pm_english_bivariate, aes(fill = bi_class), color = NA) +
  bi_scale_fill(pal = "PurpleGrn", dim = 3, name = NULL) +
  labs(
    title = "% Only English Language Speaking"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(size = 10),
    legend.position = "none"
  )

# Create legend as a matrix of colors
# legend <- bi_legend(
#   pal = "PurpleGrn",
#   dim = 3,
#   xlab = "Higher PM2.5",
#   ylab = "Higher English Spoken as the Only Language",
#   size = 12
# )
# 
# # Combine the map and the legend
# pm_english_map <- pm_english_map + inset_element(
#   legend,
#   left = 1.05,
#   bottom = 0.1,
#   right = 1.35,
#   top = 0.4     
# )


# PM2.5 and % White Map
# Classify data for bivariate mapping
pm_white_bivariate <- bi_class(
  full_data_w_preds_perc_spatial,
  x = `PM2.5 (ug/m^3)`, 
  y = 'p_White', 
  style = "quantile", 
  dim = 3 
)

# Generate the bivariate map
pm_white_map <- ggplot() +
  ggspatial::annotation_map_tile(type = "osm", zoom = 10) +  
  geom_sf(data = pm_white_bivariate, aes(fill = bi_class), color = NA) +
  bi_scale_fill(pal = "PurpleGrn", dim = 3, name = NULL) +
  labs(
    title = "% White"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(size = 10),
    legend.position = "none"
  )

# Create legend as a matrix of colors
# legend <- bi_legend(
#   pal = "PurpleGrn",
#   dim = 3,
#   xlab = "Higher PM2.5",
#   ylab = "Higher % White",
#   size = 12
# )
# 
# # Combine the map and the legend
# pm_white_map <- pm_white_map + inset_element(
#   legend,
#   left = 1.05,
#   bottom = 0.1,
#   right = 1.35,
#   top = 0.4     
# )


# PM2.5 and % Hispanic or Latino Map
# Classify data for bivariate mapping
pm_hispanic_bivariate <- bi_class(
  full_data_w_preds_perc_spatial,
  x = `PM2.5 (ug/m^3)`, 
  y = 'p_Hispanic', 
  style = "quantile", 
  dim = 3 
)

# Generate the bivariate map
pm_hispanic_map <- ggplot() +
  ggspatial::annotation_map_tile(type = "osm", zoom = 10) +  
  geom_sf(data = pm_hispanic_bivariate, aes(fill = bi_class), color = NA) +
  bi_scale_fill(pal = "PurpleGrn", dim = 3, name = NULL) +
  labs(
    title = "% Hispanic or Latino"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(size = 10),
    legend.position = "none"
  )

# Create legend as a matrix of colors
# legend <- bi_legend(
#   pal = "PurpleGrn",
#   dim = 3,
#   xlab = "Higher PM2.5",
#   ylab = "Higher % Hispanic or Latino",
#   size = 12
# )
# 
# # Combine the map and the legend
# pm_hispanic_map <- pm_hispanic_map + inset_element(
#   legend,
#   left = 1.05,
#   bottom = 0.1,
#   right = 1.35,
#   top = 0.4     
# )


# Combine 4 maps together
# Create a single legend
common_legend <- bi_legend(
  pal = "PurpleGrn",
  dim = 3,
  xlab = "Higher PM2.5",
  ylab = "Higher % of Demographic Variable (e.g., Poverty, White, etc.)",
  size = 8
) +
  theme(
    legend.key.width = unit(0.5, "cm"), 
    legend.key.height = unit(0.5, "cm") 
  )

# Combine the maps without individual legends
combined_pm_choropleth <- (pm_poverty_map + theme(legend.position = "none") |
                  pm_english_map + theme(legend.position = "none")) /
                 (pm_white_map + theme(legend.position = "none") |
                  pm_hispanic_map + theme(legend.position = "none"))


# Add title and legend in the correct layout
combined_pm_choropleth <- (combined_pm_choropleth | wrap_elements(common_legend)) +
  plot_annotation(
    title = "Figure: Relationship between Predicted PM2.5 (ug/m^3)\nand Significant Demographic Variables from Regression Results",
    theme = theme(
      plot.title = element_text(size = 18, face = "bold", hjust = 0.5, margin = margin(b = 20))
    )
  ) +
  plot_layout(widths = c(4, 1))

# Display the final plot
print(combined_pm_choropleth)
```


```{r no2.choropleth.map}
## generating plots for significant predictors from regression results

# NO2 and % Below Poverty Map
# Classify data for bivariate mapping
no2_poverty_bivariate <- bi_class(
  full_data_w_preds_perc_spatial,
  x = `NO2 (ppb)`, 
  y = Percent_Below_Poverty, 
  style = "quantile", 
  dim = 3 
)

# Define the bounding box for the study area
map_bbox <- st_bbox(no2_poverty_bivariate)

# Generate the bivariate map
no2_poverty_map <- ggplot() +
  ggspatial::annotation_map_tile(type = "osm", zoom = 10) +  
  geom_sf(data = no2_poverty_bivariate, aes(fill = bi_class), color = NA) +
  bi_scale_fill(pal = "PurpleGrn", dim = 3, name = NULL) +
  labs(
    title = "% Below Poverty"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(size = 10),
    legend.position = "none"
  )


# NO2 and % English Language Only Map


# Classify data for bivariate mapping
no2_english_bivariate <- bi_class(
  full_data_w_preds_perc_spatial,
  x = `NO2 (ppb)`, 
  y = 'p_engl_only', 
  style = "quantile", 
  dim = 3 
)

# Generate the bivariate map
no2_english_map <- ggplot() +
  ggspatial::annotation_map_tile(type = "osm", zoom = 10) +  
  geom_sf(data = no2_english_bivariate, aes(fill = bi_class), color = NA) +
  bi_scale_fill(pal = "PurpleGrn", dim = 3, name = NULL) +
  labs(
    title = "% Only English Language Speaking"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(size = 10),
    legend.position = "none"
  )


# NO2 and % White Map
# Classify data for bivariate mapping
no2_white_bivariate <- bi_class(
  full_data_w_preds_perc_spatial,
  x = `NO2 (ppb)`, 
  y = 'p_White', 
  style = "quantile", 
  dim = 3 
)

# Generate the bivariate map
no2_white_map <- ggplot() +
  ggspatial::annotation_map_tile(type = "osm", zoom = 10) +  
  geom_sf(data = no2_white_bivariate, aes(fill = bi_class), color = NA) +
  bi_scale_fill(pal = "PurpleGrn", dim = 3, name = NULL) +
  labs(
    title = "% White"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(size = 10),
    legend.position = "none"
  )


# # including this code if we want to be consistent with PM2.5 map, but % Hispanic wasn't a significant predictor in regression model for NO2 - need to note in write-up

# NO2 and % Hispanic or Latino Map
# Classify data for bivariate mapping
no2_hispanic_bivariate <- bi_class(
  full_data_w_preds_perc_spatial,
  x = `NO2 (ppb)`,
  y = 'p_Hispanic',
  style = "quantile",
  dim = 3
)

# Generate the bivariate map
no2_hispanic_map <- ggplot() +
  ggspatial::annotation_map_tile(type = "osm", zoom = 10) +
  geom_sf(data = no2_hispanic_bivariate, aes(fill = bi_class), color = NA) +
  bi_scale_fill(pal = "PurpleGrn", dim = 3, name = NULL) +
  labs(
    title = "% Hispanic or Latino"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(size = 10),
    legend.position = "none"
  )


# Combine 4 maps together
# Create a single legend
common_legend <- bi_legend(
  pal = "PurpleGrn",
  dim = 3,
  xlab = "Higher NO2",
  ylab = "Higher % of Demographic Variable (e.g., Poverty, White, etc.)",
  size = 8
) +
  theme(
    legend.key.width = unit(0.5, "cm"), 
    legend.key.height = unit(0.5, "cm") 
  )

# Combine the maps without individual legends
combined_no2_choropleth <- (no2_poverty_map + theme(legend.position = "none") |
                  no2_english_map + theme(legend.position = "none")) /
                 (no2_white_map + theme(legend.position = "none") |
                  no2_hispanic_map + theme(legend.position = "none"))


# Add title and legend in the correct layout
combined_no2_choropleth <- (combined_no2_choropleth | wrap_elements(common_legend)) +
  plot_annotation(
    title = "Figure: Relationship between Predicted NO2 (ppb)\nand Significant Demographic Variables from Regression Results",
    theme = theme(
      plot.title = element_text(size = 18, face = "bold", hjust = 0.5, margin = margin(b = 20))
    )
  ) +
  plot_layout(widths = c(4, 1))

# Display the final plot
print(combined_no2_choropleth)

```


# Trying to answer questions

Percent below poverty and pollutant concentrations are not linearly related, as evidenced by the below scatter plot.

```{r a useful plot to include, eval=F}
#Box plot of pollutant concentrations by %below FPL...need to fix x axis so its more convincing (Jorge)

## I think to make it more convincing, we should just use a scatterplot, as these are both continuous variables and we're interested in the association (Cecilia)
ggplot(stop_w_acs, aes(x = (Percent_Below_Poverty), y = mean_value, fill = variable)) +
  geom_boxplot() +
  labs(title = "Pollutant Concentrations by Income Level", x = "Percent Below Poverty", y = "Pollutant Concentration") +
  theme_minimal()

## This scatterplot is not compelling at all - I don't think we should really include this. We can say they're not linearly related. (Cecilia)
ggplot(stop_w_acs %>% filter(variable=="neph_bscat"), aes(x = (Percent_Below_Poverty), y = mean_value)) +
  geom_point(shape=1) +
  labs(title = "PM2.5 Concentrations by Income Level", x = "Percent Below Poverty", y = "Pollutant Concentration") +
  theme_minimal()

ggplot(stop_w_acs %>% filter(variable=="no2"), aes(x = (Percent_Below_Poverty), y = mean_value)) +
  geom_point(shape=1) +
  labs(title = "PM2.5 Concentrations by Income Level", x = "Percent Below Poverty", y = "Pollutant Concentration") +
  theme_minimal()

```

```{r}
#NOTE: This needs to be played with/cleaned up but it works
# Clean the data by removing rows with NA or non-finite values in the variables used for modeling

# Edited to take out the is.finite -- why would we care about that? (Cecilia)
stop_w_acs_clean <- stop_w_acs %>%
  filter(
    !is.na(mean_value) & 
    !is.na(Percent_Below_Poverty) & 
    !is.na(White) & 
    !is.na(Language_other_than_English) & 
    !is.na(log_m_to_a1) & 
    !is.na(log_m_to_comm) 
    # is.finite(mean_value) &
    # is.finite(Percent_Below_Poverty) &
    # is.finite(White) &
    # is.finite(Language_other_than_English) &
    # is.finite(log_m_to_a1) &
    # is.finite(log_m_to_comm)
  )

pm25_stop_acs_clean <- stop_w_acs_clean %>% filter(variable == "neph_bscat")

no2_stop_acs_clean <- stop_w_acs_clean %>% filter(variable == "no2")

pm25_annual_data <- annual_data

# View(pm25_stop_acs_clean)
# View(no2_stop_acs_clean)

# Fit the linear regression model -- this does not incorporate universal kriging, so I am commenting it out
# pm25_model <- lm(
#   mean_value ~ Percent_Below_Poverty + White + Language_other_than_English + log_m_to_a1 + log_m_to_comm + log_m_to_l_airp, 
#   data = pm25_stop_acs_clean
# )
# 
# no2_model <- lm(
#   mean_value ~ Percent_Below_Poverty + White + Language_other_than_English + log_m_to_a1 + log_m_to_comm + log_m_to_l_airp,
#   data = no2_stop_acs_clean
# )
# 
# # Display the summary of the model
# summary(pm25_model)
# summary(no2_model)
# 
# # Plot residuals vs. fitted values to check assumptions
# plot(pm25_model, which = 1, main = "Residuals vs Fitted")
# plot(no2_model, which =1, main = "Residuals vs Fitted")
```

```{r}
# Ensure Percent_Below_Poverty in grid_w_acs is numeric
grid_w_acs <- grid_w_acs %>%
  mutate(
    Percent_Below_Poverty = as.numeric(Percent_Below_Poverty),
    White = as.numeric(White),
    Language_other_than_English = as.numeric(Language_other_than_English),
    log_m_to_a1 = as.numeric(log_m_to_a1),
    log_m_to_comm = as.numeric(log_m_to_comm)
  )

# Check for any NA values in the predictors and handle them
grid_w_acs <- grid_w_acs %>%
  filter(
    !is.na(Percent_Below_Poverty) &
    !is.na(White) &
    !is.na(Language_other_than_English) &
    !is.na(log_m_to_a1) &
    !is.na(log_m_to_comm)
  )

## These do not account for spatial structure since they do not have UK, commenting them out (Cecilia)
# Predict PM2.5 concentrations
# grid_w_acs$predicted_pm25 <- predict(pm25_model, newdata = grid_w_acs)

# Display the first few rows of the predictions
# head(grid_w_acs$predicted_pm25)

# Predict PM2.5 concentrations
# rid_w_acs$predicted_no2 <- predict(no2_model, newdata = grid_w_acs)

# Display the first few rows of the predictions
# head(grid_w_acs$predicted_no2)

# Assuming `grid_w_acs` has a column `tract_key`

newgrid_w_acs <- grid_w_acs %>%    mutate(TRACTCE = substr(as.character(tract_key), 
            nchar(as.character(tract_key)) - 5, nchar(as.character(tract_key))))
newgrid_w_acs2 <- left_join(newgrid_w_acs, census_clean, by = "TRACTCE")

```

Note: I likely broke this below because I commented out the incorrect predictions (Cecilia).
```{r, eval=FALSE}
# Define the CRS (EPSG:32148 - Washington State Plane North, NAD83)
lambert_crs <- st_crs(32148)
latlong_proj <- 4326

# Reproject the spatial data to Lambert Conformal Conic
#data_lambert <- st_transform(census_shapefile, crs = lambert_crs)

# Ensure the dataset is an sf object (it likely already is)
newgrid_w_acs2 <- st_as_sf(newgrid_w_acs2)

# Check the CRS (Coordinate Reference System)
st_crs(newgrid_w_acs2)

# Transform to Lambert Conformal Conic projection (if not already)
newgrid_w_acs2_lambert <- st_transform(newgrid_w_acs2, crs = lambert_crs)

# Plot the predicted PM2.5 concentrations
ggplot(newgrid_w_acs2_lambert) +
  geom_sf(aes(fill = predicted_pm25), color = NA) +
  scale_fill_viridis_c(option = "plasma", name = "PM2.5 (µg/m³)") + # Color-blind-friendly palette
  labs(
    title = "Predicted PM2.5 Concentrations by Census Tract",
    subtitle = "Lambert Conformal Conic Projection",
    caption = "Data: ACS and Mobile Monitoring",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    plot.caption = element_text(size = 10, hjust = 0),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )

# Plot the predicted NO2 concentrations
ggplot(newgrid_w_acs2_lambert) +
  geom_sf(aes(fill = predicted_no2), color = NA) +
  scale_fill_viridis_c(option = "plasma", name = "NO2 (ppb)") + # Color-blind-friendly palette
  labs(
    title = "Predicted NO2 Concentrations by Census Tract",
    subtitle = "Lambert Conformal Conic Projection",
    caption = "Data: ACS and Mobile Monitoring",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    plot.caption = element_text(size = 10, hjust = 0),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )


# percent below poverty map
ggplot(newgrid_w_acs2_lambert) +
  geom_sf(aes(fill = Percent_Below_Poverty), color = NA) +
  scale_fill_viridis_c(option = "plasma", name = "Percent") + # Color-blind-friendly palette
  labs(
    title = "Percent Below Poverty by Census Tract",
    subtitle = "Lambert Conformal Conic Projection",
    caption = "Data: ACS and Mobile Monitoring",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    plot.caption = element_text(size = 10, hjust = 0),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )
```

```{r, eval=FALSE}
# Ensure the data is in EPSG:4326 (latitude/longitude)
newgrid_w_acs2_latlong <- st_transform(newgrid_w_acs2, crs = 4326)

# Plot with OpenStreetMap basemap Predicted Pm2.5
ggplot() +
  annotation_map_tile(type = "osm", zoom = 10) +  # OpenStreetMap tiles
  geom_sf(data = newgrid_w_acs2_latlong, aes(fill = predicted_pm25), color = NA, alpha = 0.8) +
  scale_fill_viridis_c(option = "plasma", name = "PM2.5 (µg/m³)") +
  labs(
    title = "Predicted PM2.5 Concentrations",
    subtitle = "Overlayed on OpenStreetMap Basemap",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )


# Plot with OpenStreetMap basemap Predicted NO2
ggplot() +
  annotation_map_tile(type = "osm", zoom = 10) +  # OpenStreetMap tiles
  geom_sf(data = newgrid_w_acs2_latlong, aes(fill = predicted_no2), color = NA, alpha = 0.8) +
  scale_fill_viridis_c(option = "plasma", name = "NO2 (ppb)") +
  labs(
    title = "Predicted NO2 Concentrations",
    subtitle = "Overlayed on OpenStreetMap Basemap",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )

```

```{r, eval=FALSE}
# Plot with transparency PM2.5
ggplot() +
  annotation_map_tile(type = "osm", zoom = 10) +  # OpenStreetMap tiles
  geom_sf(data = newgrid_w_acs2_latlong, aes(fill = predicted_pm25), color = NA, alpha = 0.06) +  # Adjust alpha for transparency
  scale_fill_viridis_c(option = "plasma", name = "PM2.5 (µg/m³)") +  # Color-blind-friendly palette
  labs(
    title = "Predicted PM2.5 Concentrations",
    subtitle = "Overlayed on OpenStreetMap Basemap with Transparency",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )

# Plot with transparency NO2
ggplot() +
  annotation_map_tile(type = "osm", zoom = 10) +  # OpenStreetMap tiles
  geom_sf(data = newgrid_w_acs2_latlong, aes(fill = predicted_no2), color = NA, alpha = 0.06) +  # Adjust alpha for transparency
  scale_fill_viridis_c(option = "plasma", name = "NO2(ppb)") +  # Color-blind-friendly palette
  labs(
    title = "Predicted NO2 Concentrations",
    subtitle = "Overlayed on OpenStreetMap Basemap with Transparency",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )

```

```{r, eval=FALSE}

# Load King County, WA shapefile 
king_county <- counties(state = "WA", cb = TRUE) %>% filter(NAME == "King") 

king_county <- st_transform(king_county, crs = 4326)

# Ensure the data is in EPSG:4326 (latitude/longitude) 
newgrid_w_acs2_latlong <- st_transform(newgrid_w_acs2, crs = 4326) 

# Clip the predicted PM2.5 data to the boundary of King County 
newgrid_king_county <- st_intersection(newgrid_w_acs2_latlong, king_county) 

# Plot with OpenStreetMap basemap, focusing on King County, WA for PM2.5

ggplot() + annotation_map_tile(type = "osm", zoom = 11) + # Adjust zoom level for King County 
  geom_sf(data = newgrid_king_county, aes(fill = predicted_pm25), color = NA, alpha = 0.8) + scale_fill_viridis_c(option = "plasma", name = "PM2.5 (µg/m³)") + labs( title = "Predicted PM2.5 Concentrations in Greater King County, WA",
 subtitle = "Overlayed on OpenStreetMap Basemap", x = "Longitude", y = "Latitude" ) + theme_minimal() + theme( plot.title = element_text(size = 16, face = "bold", hjust = 0.5), plot.subtitle = element_text(size = 12, hjust = 0.5), legend.title = element_text(size = 12), legend.text = element_text(size = 10) ) 

# Plot with transparency for better clarity 
ggplot() + annotation_map_tile(type = "osm", zoom = 11) + # Adjust zoom level for King County 
  geom_sf(data = newgrid_king_county, aes(fill = predicted_pm25), color = NA, alpha = 0.06) + # Adjust alpha for transparency 
  scale_fill_viridis_c(option = "plasma", name = "PM2.5 (µg/m³)") + # Color-blind-friendly palette 
  labs( title = "Predicted PM2.5 Concentrations in Greater King County, WA", subtitle = "Overlayed on OpenStreetMap Basemap with Transparency", x = "Longitude", y = "Latitude" ) + theme_minimal() +               theme( plot.title = element_text(size = 16, face = "bold", hjust = 0.5), plot.subtitle = element_text(size = 12, hjust = 0.5), legend.title = element_text(size = 12), legend.text = element_text(size = 10) )        

# Plot with OpenStreetMap basemap, focusing on King County, WA for PM2.5

ggplot() + annotation_map_tile(type = "osm", zoom = 11) + # Adjust zoom level for King County 
  geom_sf(data = newgrid_king_county, aes(fill = predicted_no2), color = NA, alpha = 0.8) + scale_fill_viridis_c(option = "plasma", name = "NO2 (ppb)") + labs( title = "Predicted NO2 Concentrations in Greater King County, WA",
 subtitle = "Overlayed on OpenStreetMap Basemap", x = "Longitude", y = "Latitude" ) + theme_minimal() + theme( plot.title = element_text(size = 16, face = "bold", hjust = 0.5), plot.subtitle = element_text(size = 12, hjust = 0.5), legend.title = element_text(size = 12), legend.text = element_text(size = 10) ) 

# Plot with transparency for better clarity 
ggplot() + annotation_map_tile(type = "osm", zoom = 11) + # Adjust zoom level for King County 
  geom_sf(data = newgrid_king_county, aes(fill = predicted_no2), color = NA, alpha = 0.06) + # Adjust alpha for transparency 
  scale_fill_viridis_c(option = "plasma", name = "NO2(ppb)") + # Color-blind-friendly palette 
  labs( title = "Predicted PM2.5 Concentrations in Greater King County, WA", subtitle = "Overlayed on OpenStreetMap Basemap with Transparency", x = "Longitude", y = "Latitude" ) + theme_minimal() +               theme( plot.title = element_text(size = 16, face = "bold", hjust = 0.5), plot.subtitle = element_text(size = 12, hjust = 0.5), legend.title = element_text(size = 12), legend.text = element_text(size = 10) )                  
```
#### Stephanie and Jorge's additions
#Poverty and PM2.5 map
```{r, include=F, eval=F}
# Normalize PM2.5 and Poverty levels to a 0–1 range
newgrid_king_county <- newgrid_king_county %>%
  mutate(
    pm25_scaled = scales::rescale(predicted_pm25, to = c(0, 1)),
    poverty_scaled = scales::rescale(Percent_Below_Poverty, to = c(0, 1)),
    bivar_index = (pm25_scaled + poverty_scaled) / 2  # Combine into a single index
  )

# Define a bivariate color palette (e.g., high PM2.5 = red, high poverty = blue)
bivariate_colors <- c(
  "low_low" = "#e8e8e8",  # Low PM2.5, Low Poverty
  "low_high" = "#89a1c8",  # Low PM2.5, High Poverty
  "high_low" = "#c48491",  # High PM2.5, Low Poverty
  "high_high" = "#ad6d4e"  # High PM2.5, High Poverty
)

# Assign bivariate categories
newgrid_king_county <- newgrid_king_county %>%
  mutate(
    bivar_category = case_when(
      pm25_scaled <= 0.5 & poverty_scaled <= 0.5 ~ "low_low",
      pm25_scaled <= 0.5 & poverty_scaled > 0.5 ~ "low_high",
      pm25_scaled > 0.5 & poverty_scaled <= 0.5 ~ "high_low",
      pm25_scaled > 0.5 & poverty_scaled > 0.5 ~ "high_high"
    )
  )

# Plot the bivariate map
ggplot() +
  geom_sf(data = newgrid_king_county, aes(fill = bivar_category), color = NA, alpha = 0.8) +
  scale_fill_manual(
    values = bivariate_colors,
    name = "Bivariate\nLegend",
    labels = c("Low PM2.5, Low Poverty", "Low PM2.5, High Poverty",
               "High PM2.5, Low Poverty", "High PM2.5, High Poverty")
  ) +
  labs(
    title = "Bivariate Map of PM2.5 and Poverty Levels in King County, WA",
    subtitle = "High PM2.5 and High Poverty areas are highlighted",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 14, hjust = 0.5),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )

```

#improved map: DO NOT DELETE THIS MAP
```{r, eval=F}
#Define the correct bivariate color palette
bivariate_colors_corrected <- c(
  "low_low" = "#f0f0f0",  # Low PM2.5, Low Poverty (light grey, almost transparent)
  "low_high" = "#9ecae1",  # Low PM2.5, High Poverty (light blue)
  "high_low" = "#fc9272",  # High PM2.5, Low Poverty (light red)
  "high_high" = "#67000d"  # High PM2.5, High Poverty (dark red)
)

# Filter and ensure CRS is consistent
newgrid_king_county_filtered <- st_transform(newgrid_king_county, crs = 4326)

# Plot with the corrected color scheme and appropriate transparency
ggplot() +
  annotation_map_tile(type = "osm", zoom = 10) +  # OpenStreetMap basemap
  geom_sf(data = newgrid_king_county_filtered, aes(fill = bivar_category), color = NA, alpha = 0.8) +  # Increased transparency for subtle areas
  scale_fill_manual(
    values = bivariate_colors_corrected,
    name = "PM2.5 & Poverty Levels",
    labels = c(
      "High PM2.5, High Poverty",
      "High PM2.5, Low Poverty",
      "Low PM2.5, High Poverty",
      "Low PM2.5, Low Poverty")
  ) +
  labs(
    title = "PM2.5 and Poverty Levels in King County, WA",
    subtitle = "High PM2.5 and High Poverty areas are highlighted in dark red",
    caption = "Data Sources: ACS, Mobile Monitoring, OpenStreetMap",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 14, hjust = 0.5),
    plot.caption = element_text(size = 10, hjust = 1),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "right",  # Legend on the right-hand side
    legend.box = "vertical",
    panel.background = element_rect(fill = "transparent", color = NA)
  )

```
#Linear regression Predicted NO2 and poverty
RESULTS:
The relationship between predicted nitrogen dioxide (NO2) levels and the percentage of the population below the poverty line was examined using linear regression. The model revealed a significant positive association between poverty and NO2 levels (Estimate = 0.143, p < 0.001), indicating that areas with higher poverty rates are associated with higher predicted NO2 concentrations. The model explained approximately 17.6% of the variance in predicted NO2 levels (Adjusted R-squared = 0.176). The intercept suggests a baseline NO2 level of 7.39 in areas with no population below the poverty line. These findings highlight a potential environmental justice concern, with higher NO2 levels disproportionately affecting lower-income communities.
```{r, eval=FALSE}
colnames(newgrid_king_county)

# Load the dataset
data <- newgrid_king_county

# Clean the data: Ensure columns are numeric and remove invalid values
data <- data %>%
  mutate(
    predicted_no2 = as.numeric(as.character(predicted_no2)),
    Percent_Below_Poverty = as.numeric(as.character(Percent_Below_Poverty))  # Replace with the actual column name for below poverty
  ) %>%
  filter(
    !is.na(predicted_no2),
    !is.na(Percent_Below_Poverty),
    is.finite(predicted_no2),
    is.finite(Percent_Below_Poverty)
  )

# Fit the linear regression model
model_poverty <- lm(predicted_no2 ~ Percent_Below_Poverty, data = data)

# Summarize the model
summary(model_poverty)

# Optional: Plot the relationship
ggplot(data, aes(x = Percent_Below_Poverty, y = predicted_no2)) +
  geom_point() +
  geom_smooth(method = "lm", color = "blue") +
  labs(
    title = "Linear Regression: Predicted NO2 vs Below Poverty",
    x = "Percent Below Poverty",
    y = "Predicted NO2"
  )

```

#Map of Predicted NO2 and poverty
```{r, eval=FALSE}
# Normalize NO2 and Poverty levels to a 0–1 range
newgrid_king_county <- newgrid_king_county %>%
  mutate(
    no2_scaled = rescale(predicted_no2, to = c(0, 1)),  # Assuming `predicted_no2` is the column for NO2
    poverty_scaled = rescale(Percent_Below_Poverty, to = c(0, 1)),
    bivar_category = case_when(
      no2_scaled <= 0.5 & poverty_scaled <= 0.5 ~ "low_low",
      no2_scaled <= 0.5 & poverty_scaled > 0.5 ~ "low_high",
      no2_scaled > 0.5 & poverty_scaled <= 0.5 ~ "high_low",
      no2_scaled > 0.5 & poverty_scaled > 0.5 ~ "high_high"
    )
  )

# Define a bivariate color palette
bivariate_colors_no2 <- c(
  "low_low" = "#f0f0f0",  # Low NO2, Low Poverty (light grey)
  "low_high" = "#9ecae1",  # Low NO2, High Poverty (light blue)
  "high_low" = "#fc9272",  # High NO2, Low Poverty (light red)
  "high_high" = "#67000d"  # High NO2, High Poverty (dark red)
)

# Ensure CRS is consistent for mapping
newgrid_king_county_filtered <- st_transform(newgrid_king_county, crs = 4326)

# Plot the NO2 and Poverty map
ggplot() +
  annotation_map_tile(type = "osm", zoom = 10) +  # OpenStreetMap basemap
  geom_sf(
    data = newgrid_king_county_filtered,
    aes(fill = bivar_category),
    color = NA,
    alpha = 0.8
  ) +
  scale_fill_manual(
    values = bivariate_colors_no2,
    name = "NO2 & Poverty Levels",
    breaks = c("low_low", "low_high", "high_low", "high_high"),  # Ensures the order matches the color definitions
    labels = c(
      "Low NO2, Low Poverty",   # Corresponds to "low_low"
      "Low NO2, High Poverty",  # Corresponds to "low_high"
      "High NO2, Low Poverty",  # Corresponds to "high_low"
      "High NO2, High Poverty"  # Corresponds to "high_high"
    )
  ) +
  labs(
    title = "NO2 and Poverty Levels in King County, WA",
    subtitle = "High NO2 and High Poverty areas are highlighted in dark red",
    caption = "Data Sources: ACS, Mobile Monitoring, OpenStreetMap",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 14, hjust = 0.5),
    plot.caption = element_text(size = 10, hjust = 1),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "right",  # Legend on the right-hand side
    legend.box = "vertical",
    panel.background = element_rect(fill = "transparent", color = NA)
  )

```
#Linear regression of predicted NO2 and language diversity
```{r, eval=FALSE}

# Load the dataset
data <- newgrid_king_county

# Clean the data: Ensure columns are numeric and remove invalid values
data <- data %>%
  mutate(
    predicted_no2 = as.numeric(as.character(predicted_no2)),
    Language_other_than_English = as.numeric(as.character(Language_other_than_English))
  ) %>%
  filter(
    !is.na(predicted_no2), 
    !is.na(Language_other_than_English),
    is.finite(predicted_no2), 
    is.finite(Language_other_than_English)
  )

# Fit the linear regression model
model <- lm(predicted_no2 ~ Language_other_than_English, data = data)

# Summarize the model
summary(model)

# Optional: Plot the relationship
ggplot(data, aes(x = Language_other_than_English, y = predicted_no2)) +
  geom_point() +
  geom_smooth(method = "lm", color = "blue") +
  labs(
    title = "Linear Regression: Predicted NO2 vs Language Other Than English",
    x = "Language Other Than English",
    y = "Predicted NO2"
  )

```

#Map of Predicted NO2 and language diversity

This map visualizes the relationship between predicted nitrogen dioxide (NO₂) levels and language diversity in a specific geographic area, likely in Washington State, as it includes landmarks like Seattle and Tacoma. It is categorized into four distinct levels of NO₂ and language diversity

```{r, eval=FALSE}
# Ensure columns are numeric and remove invalid values
newgrid_king_county <- newgrid_king_county %>%
  mutate(
    predicted_no2 = as.numeric(as.character(predicted_no2)),
    Language_other_than_English = as.numeric(as.character(Language_other_than_English))
  ) %>%
  filter(
    !is.na(predicted_no2), 
    !is.na(Language_other_than_English),
    is.finite(predicted_no2), 
    is.finite(Language_other_than_English)
  ) %>%
  mutate(
    no2_scaled = rescale(predicted_no2, to = c(0, 1)),
    language_scaled = rescale(Language_other_than_English, to = c(0, 1)),
    bivar_category = case_when(
      no2_scaled <= 0.5 & language_scaled <= 0.5 ~ "low_low",
      no2_scaled <= 0.5 & language_scaled > 0.5 ~ "low_high",
      no2_scaled > 0.5 & language_scaled <= 0.5 ~ "high_low",
      no2_scaled > 0.5 & language_scaled > 0.5 ~ "high_high"
    )
  )

# Ensure it's an sf object and transform CRS
if (!inherits(newgrid_king_county, "sf")) {
  newgrid_king_county <- st_as_sf(newgrid_king_county, coords = c("longitude", "latitude"), crs = 4326)
}
newgrid_king_county_filtered <- st_transform(newgrid_king_county, crs = 4326)

# Define the bivariate color palette
bivariate_colors_no2 <- c(
  "low_low" = "#f0f0f0", 
  "low_high" = "#9ecae1", 
  "high_low" = "#fc9272", 
  "high_high" = "#67000d"
)

# Plot the map
ggplot() +
  annotation_map_tile(type = "osm", zoom = 10) +
  geom_sf(
    data = newgrid_king_county_filtered,
    aes(fill = bivar_category),
    color = NA,
    alpha = 0.8
  ) +
  scale_fill_manual(
    values = bivariate_colors_no2,
    name = "NO2 & Language Levels",
    breaks = c("low_low", "low_high", "high_low", "high_high"),
    labels = c(
      "Low NO2, Low Language Diversity",
      "Low NO2, High Language Diversity",
      "High NO2, Low Language Diversity",
      "High NO2, High Language Diversity"
    )
  ) +
  labs(
    title = "Predicted NO2 and Language Diversity",
    caption = "Data Sources: ACS, Mobile Monitoring, OpenStreetMap",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
    plot.caption = element_text(size = 10, hjust = 1),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "right",
    legend.box = "vertical",
    panel.background = element_rect(fill = "transparent", color = NA)
  )

```

#MLR racial diversity and predicted NO2

Narrative for write up: 
A multiple linear regression model examined the relationship between predicted nitrogen dioxide (NO2) levels and race-related variables. Areas with higher proportions of Black/African American and "Two or More Races" residents were associated with increased NO2 levels, while areas with higher proportions of White, American Indian and Alaska Native (AIAN), and Asian residents had lower predicted NO2 levels. The category "Hawaiian/Pacific Islander" showed no significant association with NO2 levels. The findings highlight disparities in environmental exposures across racial groups, with potential implications for public health and environmental justice. Targeted interventions are needed to address elevated NO2 levels in disproportionately affected communities.
```{r, eval=FALSE}
# Load the dataset
data <- newgrid_king_county

# Clean the data: Ensure all relevant columns are numeric and remove invalid values
data <- data %>%
  mutate(
    predicted_no2 = as.numeric(as.character(predicted_no2)),
    White = as.numeric(as.character(White)),
    Black_AfricanAmerican = as.numeric(as.character(Black_AfricanAmerican)),
    AIAN = as.numeric(as.character(AIAN)),
    Asian = as.numeric(as.character(Asian)),
    Hawaiian_PacificIslander = as.numeric(as.character(Hawaiian_PacificIslander)),
    Other_alone = as.numeric(as.character(Other_alone)),
    X2ormore = as.numeric(as.character(X2ormore)),
    X2ormore_andsomeother = as.numeric(as.character(X2ormore_andsomeother)),
    X3ormore = as.numeric(as.character(X3ormore))
  ) %>%
  filter(
    !is.na(predicted_no2),
    is.finite(predicted_no2),
    !is.na(White),
    !is.na(Black_AfricanAmerican),
    !is.na(AIAN),
    !is.na(Asian),
    !is.na(Hawaiian_PacificIslander),
    !is.na(Other_alone),
    !is.na(X2ormore),
    !is.na(X2ormore_andsomeother),
    !is.na(X3ormore)
  )

# Fit the multiple linear regression model (excluding `_ME` variables)
model <- lm(predicted_no2 ~ White + 
            Black_AfricanAmerican + 
            AIAN + 
            Asian + 
            Hawaiian_PacificIslander + 
            Other_alone + 
            X2ormore + 
            X2ormore_andsomeother + 
            X3ormore,
            data = data)

# Summarize the model
summary(model)

# Extract the coefficients from the model
model_summary <- summary(model)
coefficients_table <- as.data.frame(model_summary$coefficients)

# Rename columns for clarity
colnames(coefficients_table) <- c("Estimate", "Std. Error", "t value", "Pr(>|t|)")

# Add row names as a column for variable names
coefficients_table <- tibble::rownames_to_column(coefficients_table, "Variable")

# Print the table in the console
print(coefficients_table)

# Optional: Use kable for better visualization in RMarkdown or RStudio
library(knitr)
kable(coefficients_table, caption = "Regression Results for Predicted NO2 and Race Variables")


# Optional: Diagnostics for the model
par(mfrow = c(2, 2))  # Arrange plots in a 2x2 grid
plot(model)

```

#Racial diversity - NOTE: size needs to be explored so maps are less squished

Text for write up: This map visualizes the racial and ethnic composition of King County, Washington, by census tract, with an OpenStreetMap background for spatial context. Each panel represents the percentage distribution of a specific racial or ethnic group, including American Indian/Alaskan Native, Asian, Black or African American, Hawaiian or Pacific Islander, Hispanic or Latino, and White. The color gradient indicates the concentration, with darker shades representing higher percentages. This visual provides a detailed geographic understanding of demographic patterns across the county. Data sources include ACS and OpenStreetMap.
```{r, eval=FALSE}
# For adding OpenStreetMap tiles

# Define the racial/ethnic variables and their readable names
selected_race_vars <- c(
  "Black_AfricanAmerican",
  "White",
  "Asian",
  "AIAN",
  "Hawaiian_PacificIslander",
  "HispanicorLatino"
)

readable_race_names <- c(
  "Black or African American",
  "White",
  "Asian",
  "American Indian/Alaskan Native",
  "Hawaiian or Pacific Islander",
  "Hispanic or Latino"
)

# Ensure CRS is consistent for mapping
newgrid_king_county <- st_transform(newgrid_king_county, crs = 4326)

# Pivot dataset to long format for mapping multiple racial groups
newgrid_long <- newgrid_king_county %>%
  select(geometry, all_of(selected_race_vars)) %>%
  pivot_longer(
    cols = all_of(selected_race_vars),
    names_to = "Race",
    values_to = "Percentage"
  ) %>%
  mutate(
    Race = recode(Race, 
                  "Black_AfricanAmerican" = "Black or African American",
                  "White" = "White",
                  "Asian" = "Asian",
                  "AIAN" = "American Indian/Alaskan Native",
                  "Hawaiian_PacificIslander" = "Hawaiian or Pacific Islander",
                  "HispanicorLatino" = "Hispanic or Latino")
  )

# Create the map with OpenStreetMap as the background
ggplot() +
  annotation_map_tile(type = "osm", zoom = 10) +  # Add OpenStreetMap tiles as the base layer
  geom_sf(
    data = newgrid_long,
    aes(fill = Percentage, geometry = geometry),
    color = NA,
    alpha = 0.8
  ) +
  facet_wrap(~ Race, ncol = 2) +  # Facet by race for multiple maps
  scale_fill_viridis_c(
    name = "Percentage",
    option = "plasma",
    na.value = "grey90"
  ) +
  labs(
    title = "Spatial Distributon of Dominant Race per Census Tract",
    caption = "Data Sources: ACS, OpenStreetMap",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
    plot.caption = element_text(size = 10, hjust = 1),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "right",
    panel.background = element_rect(fill = "transparent", color = NA)
  )
```

#Racial map choropleth
Text for write up: 
This map displays the dominant racial or ethnic group by census tract in Snohomish, Pierce, and King County, Washington, overlaid on an OpenStreetMap basemap for geographic context. Each tract is color-coded to represent the racial or ethnic group with the highest proportion of residents. Groups represented include American Indian/Alaskan Native, Asian, Black or African American, Hawaiian or Pacific Islander, Hispanic or Latino, and White. This visualization highlights the spatial distribution of demographic dominance across the county, providing insights into the region's racial and ethnic diversity. Data sources include the American Community Survey (ACS), Mobile Monitoring, and OpenStreetMap.
```{r, eval=FALSE}
# For adding OpenStreetMap tiles

# Ensure CRS consistency for OpenStreetMap compatibility
newgrid_king_county <- st_transform(newgrid_king_county, crs = 4326)

# Normalize racial/ethnic data to 0–1 range
newgrid_normalized <- newgrid_king_county %>%
  mutate(across(all_of(selected_race_vars), ~ rescale(.x, to = c(0, 1))))

# Identify the dominant race in each tract
newgrid_normalized <- newgrid_normalized %>%
  rowwise() %>%
  mutate(
    dominant_race = selected_race_vars[which.max(c_across(all_of(selected_race_vars)))]
  ) %>%
  ungroup() %>%
  mutate(
    dominant_race = recode(dominant_race,
                           "Black_AfricanAmerican" = "Black or African American",
                           "White" = "White",
                           "Asian" = "Asian",
                           "AIAN" = "American Indian/Alaskan Native",
                           "Hawaiian_PacificIslander" = "Hawaiian or Pacific Islander",
                           "HispanicorLatino" = "Hispanic or Latino")
  )

# Assign colors for each dominant race
race_colors <- c(
  "Black or African American" = "#fc9272",   # Light red
  "White" = "#f0f0f0",                       # Light grey
  "Asian" = "#9ecae1",                       # Light blue
  "American Indian/Alaskan Native" = "#a1d99b",  # Light green
  "Hawaiian or Pacific Islander" = "#bcbddc",    # Light purple
  "Hispanic or Latino" = "#fdae6b"           # Light orange
)

# Plot the map using OpenStreetMap tiles
ggplot() +
  annotation_map_tile(type = "osm", zoom = 10) +  # Add OpenStreetMap tiles as the base layer
  geom_sf(
    data = newgrid_normalized,
    aes(fill = dominant_race),
    color = NA,
    alpha = 0.8
  ) +
  scale_fill_manual(
    values = race_colors,
    name = "Prominent Race"
  ) +
  labs(
    title = "Spatial Distribution of Race by Census Tract",
    subtitle = "Dominant racial group per census tract",
    caption = "Data Sources: ACS, OpenStreetMap",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 14, hjust = 0.5),
    plot.caption = element_text(size = 10, hjust = 1),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "right",
    panel.background = element_rect(fill = "transparent", color = NA)
  )


```
#Language diversity
Text for write up: 
This map visualizes language use at home by census tract in Snohomish, Pierce, and King County, Washington, with an OpenStreetMap background for geographic context. The two panels represent the percentage distribution of households where English is the primary language ("English Only") and households where other languages are predominantly spoken ("Language Other Than English").

The left panel highlights areas where English is predominantly spoken, with darker shades indicating higher percentages. The right panel illustrates areas where other languages dominate, showing significant linguistic diversity across the region. The color gradient reflects the proportion of households, from lower percentages in blue to higher percentages in yellow. This map provides insights into the spatial distribution of language use in the county. Data sources include the American Community Survey (ACS) and OpenStreetMap.

```{r, eval=FALSE}
# For adding OpenStreetMap tiles

# Define the language variables and their readable names
selected_language_vars <- c(
  "English_only",
  "Language_other_than_English"
)

readable_language_names <- c(
  "English Only",
  "Language Other Than English"
)

# Ensure CRS is consistent for mapping
newgrid_king_county <- st_transform(newgrid_king_county, crs = 4326)

# Pivot dataset to long format for mapping language groups
newgrid_long_language <- newgrid_king_county %>%
  select(geometry, all_of(selected_language_vars)) %>%
  pivot_longer(
    cols = all_of(selected_language_vars),
    names_to = "Language",
    values_to = "Percentage"
  ) %>%
  mutate(
    Language = recode(Language, 
                      "English_only" = "English Only",
                      "Language_other_than_English" = "Language Other Than English")
  )

# Create the map with OpenStreetMap as the background
ggplot() +
  annotation_map_tile(type = "osm", zoom = 10) +  # Add OpenStreetMap tiles as the base layer
  geom_sf(
    data = newgrid_long_language,
    aes(fill = Percentage, geometry = geometry),
    color = NA,
    alpha = 0.8
  ) +
  facet_wrap(~ Language, ncol = 2) +  # Facet by language group
  scale_fill_viridis_c(
    name = "Percentage",
    option = "plasma",
    na.value = "grey90"
  ) +
  labs(
    title = "Spatial Distribution of Language Diversity by Census Tract",
    caption = "Data Sources: ACS, OpenStreetMap",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
    plot.caption = element_text(size = 10, hjust = 1),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "right",
    panel.background = element_rect(fill = "transparent", color = NA)
  )

```



#Cross-Validation -- this is currently incorrect since it was using the wrong predictions.
```{r, eval=FALSE}
# Ensure the data has a unique ID column
no2_data <- stop_for_preds %>%
  filter(variable == "no2") %>%
  mutate(ID = row_number())  # Add a unique ID for each row

# Define Functions
get_MSE <- function(obs, pred) {
  obs_avg <- mean(obs)
  MSE_obs <- mean((obs - obs_avg)^2)
  MSE_pred <- mean((obs - pred)^2)
  c(RMSE = sqrt(MSE_pred), MSE_based_R2 = max(1 - MSE_pred / MSE_obs, 0))
}

do_CV <- function(data, id = "id", group = "group", formula) {
  lapply(unique(data[[group]]), function(this_group) {
    CV_lm <- lm(formula, data = data[data[[group]] != this_group, ])
    data[data[[group]] == this_group, ] %>%
      mutate(cvpreds = predict(CV_lm, newdata = .) %>% unname())
  }) %>% bind_rows() %>% arrange(.data[[id]])
}

# Define formula for the model
frml <- as.formula(log_mean ~ log_m_to_a1 + log_m_to_a2 + log_m_to_a3 + pop10_s05000)

# Create Random Cross-Validation Groups
set.seed(123)
no2_data <- no2_data %>%
  mutate(CV_grp = sample(rep(1:10, length.out = n())))

# Perform Cross-Validation
cv_results <- do_CV(data = no2_data, id = "ID", group = "CV_grp", formula = frml)

# Calculate RMSE and R2
metrics <- get_MSE(no2_data$log_mean, cv_results$cvpreds)

# Print Metrics
print(metrics)

```